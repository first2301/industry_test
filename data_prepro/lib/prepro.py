import polars as pl
from PIL import Image
import psutil
import time
import os
import subprocess
import logging
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import signal
import functools


def timeout_handler(signum, frame):
    """íƒ€ì„ì•„ì›ƒ í•¸ë“¤ëŸ¬"""
    raise TimeoutError("íŒŒì¼ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼")


def timeout(seconds):
    """íƒ€ì„ì•„ì›ƒ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Windowsì—ì„œëŠ” signal.SIGALRMì´ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš©
            if os.name == 'nt':  # Windows
                import threading
                result = [None]
                exception = [None]
                
                def target():
                    try:
                        result[0] = func(*args, **kwargs)
                    except Exception as e:
                        exception[0] = e
                
                thread = threading.Thread(target=target)
                thread.daemon = True
                thread.start()
                thread.join(seconds)
                
                if thread.is_alive():
                    # ìŠ¤ë ˆë“œê°€ ì—¬ì „íˆ ì‹¤í–‰ ì¤‘ì´ë©´ íƒ€ì„ì•„ì›ƒ
                    raise TimeoutError(f"í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ ({seconds}ì´ˆ)")
                
                if exception[0]:
                    raise exception[0]
                
                return result[0]
            else:  # Unix/Linux
                old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(seconds)
                try:
                    result = func(*args, **kwargs)
                finally:
                    signal.alarm(0)
                    signal.signal(signal.SIGALRM, old_handler)
                return result
        return wrapper
    return decorator


class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ì™€ ê´€ë ¨ëœ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    @staticmethod
    def format_bytes(size: int) -> str:
        """byteë¥¼ KB, MB, GB, TB ë“±ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜"""
        volum = 1024
        n = 0
        volum_labels = {0: 'B', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}
        original_size = size
        while original_size >= volum and n < len(volum_labels) - 1:
            original_size /= volum
            n += 1
        return f"{original_size:.1f} {volum_labels[n]}"
    
    @staticmethod
    def get_directory_info(directory: str, summary_only: bool = False) -> Optional[str]:
        """ë””ë ‰í† ë¦¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ê°„ë‹¨í•œ ìš”ì•½ë§Œ)"""
        try:
            path = Path(directory)
            if not path.exists():
                return f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}"
            
            if summary_only:
                # ë¹ ë¥¸ ìš”ì•½(í´ë”/íŒŒì¼ ê°œìˆ˜ë§Œ)
                file_count = 0
                dir_count = 0
                for root, dirs, files in os.walk(path):
                    dir_count += len(dirs)
                    file_count += len(files)
                return f"[ìš”ì•½] í´ë” {dir_count}ê°œ, íŒŒì¼ {file_count}ê°œ"
            else:
                # ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡° (ê°œì„ ëœ í˜•íƒœ)
                result = f"ë””ë ‰í† ë¦¬ êµ¬ì¡°: {directory}\n"
                result += "=" * 80 + "\n"
                
                # ìµœëŒ€ ê¹Šì´ ì œí•œ (ë„ˆë¬´ ê¹Šì€ êµ¬ì¡° ë°©ì§€)
                max_depth = 5
                max_files_per_dir = 10
                
                for root, dirs, files in os.walk(path):
                    # ê¹Šì´ ê³„ì‚°
                    depth = root.replace(str(path), '').count(os.sep)
                    if depth > max_depth:
                        continue
                    
                    # ë“¤ì—¬ì“°ê¸°
                    indent = '  ' * depth
                    dir_name = os.path.basename(root) if root != str(path) else os.path.basename(path)
                    
                    # ë””ë ‰í† ë¦¬ í‘œì‹œ
                    if depth == 0:
                        result += f"{indent}ğŸ“ {dir_name}/\n"
                    else:
                        result += f"{indent}ğŸ“ {dir_name}/\n"
                    
                    # íŒŒì¼ í‘œì‹œ (ì²˜ìŒ 10ê°œë§Œ)
                    if files:
                        for i, file in enumerate(files[:max_files_per_dir]):
                            file_size = ""
                            try:
                                file_path = os.path.join(root, file)
                                if os.path.exists(file_path):
                                    size = os.path.getsize(file_path)
                                    if size > 1024 * 1024:  # 1MB ì´ìƒ
                                        file_size = f" ({DataProcessor.format_bytes(size)})"
                            except:
                                pass
                            result += f"{indent}  ğŸ“„ {file}{file_size}\n"
                        
                        if len(files) > max_files_per_dir:
                            result += f"{indent}  ... ({len(files) - max_files_per_dir}ê°œ ë”)\n"
                    
                    # í•˜ìœ„ ë””ë ‰í† ë¦¬ ìˆ˜ í‘œì‹œ
                    if dirs:
                        result += f"{indent}  ğŸ“‚ í•˜ìœ„ ë””ë ‰í† ë¦¬: {len(dirs)}ê°œ\n"
                
                result += "=" * 80 + "\n"
                return result
        except Exception as e:
            logging.error(f"ë””ë ‰í† ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def split_file_list(file_paths: List[Path], batch_size: int) -> List[List[Path]]:
        """íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        for i in range(0, len(file_paths), batch_size):
            batch = file_paths[i:i + batch_size]
            batches.append(batch)
        return batches
    
    @staticmethod
    def process_batch(batch: List[Path], batch_num: int, total_batches: int, 
                     max_workers: int = 4) -> Tuple[pd.DataFrame, str]:
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬ (ë¡œê¹… ìµœì í™”)"""
        batch_start_time = time.time()
        
        # ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë°°ì¹˜ ì •ë³´ ì „ë‹¬)
        metadata_list = DataProcessor.extract_file_metadata(
            batch, max_workers, batch_num, total_batches
        )
        
        # DataFrame ìƒì„±
        df = pl.DataFrame(metadata_list)
        pandas_df = df.to_pandas()
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_end_time = time.time()
        batch_elapsed = batch_end_time - batch_start_time
        
        # ë°°ì¹˜ ì •ë³´ ìƒì„±
        batch_size_bytes = sum(pandas_df['file_size'])
        batch_info = (f"ë°°ì¹˜ {batch_num}/{total_batches}: {len(batch)}ê°œ íŒŒì¼, "
                     f"{DataProcessor.format_bytes(batch_size_bytes)}, "
                     f"ì²˜ë¦¬ì‹œê°„: {batch_elapsed:.1f}ì´ˆ")
        
        return pandas_df, batch_info
    
    @staticmethod
    @timeout(30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    def _extract_single_image_resolution(file_path: Path) -> Tuple[int, int]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ í•´ìƒë„ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            with Image.open(file_path) as img:
                return img.size
        except Exception as e:
            logging.warning(f"ì´ë¯¸ì§€ í•´ìƒë„ ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
            return (0, 0)
    
    @staticmethod
    def extract_file_metadata(file_paths: List[Path], max_workers: int = 4, 
                             batch_num: int = 0, total_batches: int = 0) -> List[Dict[str, Any]]:
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ - ê¸°ë³¸ ì •ë³´ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
        metadata_list = []
        total_files = len(file_paths)
        
        # ë°°ì¹˜ ë‚´ë¶€ì—ì„œëŠ” ë¡œê¹…í•˜ì§€ ì•ŠìŒ (ì „ì²´ ì§„í–‰ë¥ ë§Œ í‘œì‹œ)
        for i, path in enumerate(file_paths):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                metadata = {
                    'full_path': str(path),
                    'file_id': path.stem,
                    'folder_name': path.parent.name,
                    'file_size': path.stat().st_size if path.exists() else 0,
                    'file_type': path.suffix.lower(),
                }
                
                metadata_list.append(metadata)
            except Exception as e:
                logging.warning(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {path}: {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê¸°ë³¸ ì •ë³´ëŠ” í¬í•¨
                metadata = {
                    'full_path': str(path),
                    'file_id': path.stem,
                    'folder_name': path.parent.name,
                    'file_size': 0,
                }
                metadata_list.append(metadata)
        
        return metadata_list
    
    @staticmethod
    def extract_image_resolutions(file_paths: List[Path], max_workers: int = 4,
                                 batch_num: int = 0, total_batches: int = 0) -> List[Tuple[int, int]]:
        """ì´ë¯¸ì§€ í•´ìƒë„ ì¶”ì¶œ (width, height) - ë³‘ë ¬ ì²˜ë¦¬ ì ìš© (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
        resolutions = []
        total_files = len(file_paths)
        
        # íŒŒì¼ ìˆ˜ê°€ ì ìœ¼ë©´ ìˆœì°¨ ì²˜ë¦¬
        if len(file_paths) < 10:
            for i, path in enumerate(file_paths):
                resolutions.append(DataProcessor._extract_single_image_resolution(path))
        else:
            # ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_path = {executor.submit(DataProcessor._extract_single_image_resolution, path): path 
                                for path in file_paths}
                
                for i, future in enumerate(as_completed(future_to_path)):
                    resolutions.append(future.result())
        
        return resolutions
    
    @staticmethod
    def make_polars_dataframe(file_paths: List[Path], max_workers: int = 4) -> pl.DataFrame:
        """polars DataFrame ìƒì„± - ê¸°ë³¸ ì •ë³´ + ì´ë¯¸ì§€ í•´ìƒë„"""
        logging.info(f"ì´ {len(file_paths)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata_list = DataProcessor.extract_file_metadata(file_paths, max_workers)
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pl.DataFrame(metadata_list)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        df = df.select(['full_path', 'file_id', 'folder_name', 'file_size', 'image_width', 'image_height'])
        
        logging.info(f"DataFrame ìƒì„± ì™„ë£Œ: {len(df)} í–‰, {len(df.columns)} ì—´")
        return df
    
    @staticmethod
    def make_pandas_dataframe(file_paths: List[Path], max_workers: int = 4) -> pd.DataFrame:
        """pandas DataFrame ìƒì„±"""
        df = DataProcessor.make_polars_dataframe(file_paths, max_workers)
        return df.to_pandas()
    
    @staticmethod
    def log_system_resources() -> str:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        try:
            memory_info = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # Windowsì—ì„œëŠ” ë£¨íŠ¸ ë””ìŠ¤í¬ ëŒ€ì‹  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì‚¬ìš©
            current_dir = Path.cwd()
            disk_usage = psutil.disk_usage(str(current_dir))
            
            # G ë“œë¼ì´ë¸Œ ì •ë³´ ì¶”ê°€
            g_drive_info = ""
            try:
                g_disk_usage = psutil.disk_usage('G:\\')
                g_drive_info = f"""
            G ë“œë¼ì´ë¸Œ ì •ë³´:
            --------------------------------------
            G ë“œë¼ì´ë¸Œ ì „ì²´ ìš©ëŸ‰: {DataProcessor.format_bytes(g_disk_usage.total)} 
            G ë“œë¼ì´ë¸Œ ì‚¬ìš©ëœ ìš©ëŸ‰: {DataProcessor.format_bytes(g_disk_usage.used)} 
            G ë“œë¼ì´ë¸Œ ì‚¬ìš© í¼ì„¼íŠ¸: {g_disk_usage.percent}%
            G ë“œë¼ì´ë¸Œ ì—¬ìœ  ìš©ëŸ‰: {DataProcessor.format_bytes(g_disk_usage.free)}
            --------------------------------------"""
            except Exception as e:
                g_drive_info = f"""
            G ë“œë¼ì´ë¸Œ ì •ë³´:
            --------------------------------------
            G ë“œë¼ì´ë¸Œ ì ‘ê·¼ ë¶ˆê°€: {e}
            --------------------------------------"""
            
            current_time = time.strftime('%Y-%m-%d %H:%M:%S') # í˜„ì¬ ì‹œê°„ ê¸°ë¡
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ë¥¼ í‘œ í˜•íƒœë¡œ ìƒì„±
            system_info = f"""
            ì‚¬ìš© ì¤‘ì¸ ìì› í™•ì¸:
            --------------------------------------
            ì „ì²´ ë©”ëª¨ë¦¬: {DataProcessor.format_bytes(memory_info.total)} 
            ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {DataProcessor.format_bytes(memory_info.available)} 
            ì‚¬ìš©ëœ ë©”ëª¨ë¦¬: {DataProcessor.format_bytes(memory_info.used)} 
            ë©”ëª¨ë¦¬ ì‚¬ìš© í¼ì„¼íŠ¸: {memory_info.percent}%
            CPU ì‚¬ìš© í¼ì„¼íŠ¸: {cpu_percent}%
            í˜„ì¬ ë””ìŠ¤í¬ ìš©ëŸ‰: {DataProcessor.format_bytes(disk_usage.total)} 
            í˜„ì¬ ì‚¬ìš©ëœ ë””ìŠ¤í¬ ìš©ëŸ‰: {DataProcessor.format_bytes(disk_usage.used)} 
            í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš© í¼ì„¼íŠ¸: {disk_usage.percent}%{g_drive_info}
            """
            return system_info
        except Exception as e:
            logging.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return "ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨"
    
    @staticmethod
    def setup_logging(log_file: str = './log/prepro.log', level: int = logging.INFO) -> None:
        """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
        try:
            os.makedirs(os.path.dirname(log_file), exist_ok=True)  # log ë””ë ‰í† ë¦¬ ìƒì„±
            
            # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
            for handler in logging.getLogger().handlers[:]:
                logging.getLogger().removeHandler(handler)
            
            # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(level)
            file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            
            # ì½˜ì†” í•¸ë“¤ëŸ¬ ì„¤ì • (ì§„í–‰ë¥ ì€ INFO ë ˆë²¨ ì´ìƒë§Œ)
            console_handler = logging.StreamHandler()
            console_handler.setLevel(level)
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            
            # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
            root_logger = logging.getLogger()
            root_logger.setLevel(logging.DEBUG)  # ëª¨ë“  ë ˆë²¨ í—ˆìš©
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
            
        except Exception as e:
            print(f"ë¡œê¹… ì„¤ì • ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def log_progress(message: str, level: int = logging.INFO, force_console: bool = False):
        """ì§„í–‰ë¥  ë¡œê¹… (ì½˜ì†” ì¶œë ¥ ì œì–´)"""
        if force_console:
            # ê°•ì œë¡œ ì½˜ì†”ì— ì¶œë ¥ (flush=Trueë¡œ ì¦‰ì‹œ ì¶œë ¥)
            print(f"[ì§„í–‰ë¥ ] {message}", flush=True)
        else:
            logging.log(level, message)
    
    @staticmethod
    def generate_file_info(pandas_df: pd.DataFrame) -> str:
        """íŒŒì¼ ì •ë³´ ìš”ì•½ ìƒì„±"""
        try:
            file_size = sum(pandas_df['file_size'])
            
            file_info = f"""
            ë°ì´í„° ì²˜ë¦¬ ì •ë³´:
            --------------------------------------
            ì „ì²´ íŒŒì¼ ìˆ˜: {len(pandas_df)}
            ì „ì²´ íŒŒì¼ ìš©ëŸ‰: {DataProcessor.format_bytes(file_size)}
            --------------------------------------
            """
            return file_info
        except Exception as e:
            logging.error(f"íŒŒì¼ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return "íŒŒì¼ ì •ë³´ ìƒì„± ì‹¤íŒ¨"
    
    @staticmethod
    def save_to_database(pandas_df: pd.DataFrame, db_path: str = './database/database.db') -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì €ì¥"""
        import sqlite3
        
        try:
            os.makedirs(os.path.dirname(db_path), exist_ok=True)  # database ë””ë ‰í† ë¦¬ ìƒì„±
            conn = sqlite3.connect(db_path)
            
            # ë°ì´í„° ë² ì´ìŠ¤ì— ë°ì´í„° ì €ì¥
            pandas_df.to_sql('TB_meta_info', conn, if_exists='replace', index=False)
            
            conn.close()
            logging.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {db_path}")
            return db_path
        except Exception as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    @staticmethod
    def append_to_database(pandas_df: pd.DataFrame, db_path: str = './database/database.db', 
                          table_name: str = 'TB_meta_info') -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì¶”ê°€ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)"""
        import sqlite3
        
        try:
            os.makedirs(os.path.dirname(db_path), exist_ok=True)  # database ë””ë ‰í† ë¦¬ ìƒì„±
            conn = sqlite3.connect(db_path)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì¶”ê°€
            pandas_df.to_sql(table_name, conn, if_exists='append', index=False)
            
            conn.close()
            # ë¡œê·¸ ì œê±° - ë„ˆë¬´ ë§ì€ ë¡œê·¸ ì¶œë ¥ ë°©ì§€
            return db_path
        except Exception as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    @staticmethod
    def validate_data_path(data_path: str) -> bool:
        """ë°ì´í„° ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬"""
        path = Path(data_path)
        if not path.exists():
            logging.error(f"ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            return False
        if not path.is_dir():
            logging.error(f"ë°ì´í„° ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {data_path}")
            return False
        return True
    
    @staticmethod
    def process_data_pipeline(data_path: str, output_db_path: str = './database/database.db', 
                            max_workers: int = 4, batch_size: int = 1000) -> Tuple[pd.DataFrame, str, str]:
        """ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›, ë¡œê¹… ìµœì í™”)"""
        pipeline_start_time = time.time()
        
        try:
            # ë°ì´í„° ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬
            if not DataProcessor.validate_data_path(data_path):
                raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„° ê²½ë¡œ: {data_path}")
            
            logging.info(f"ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {data_path}")
            
            # ë°ì´í„° ë¡œë”©
            all_path = Path(data_path).rglob('*')
            file_only = [i for i in all_path if i.is_file()]
            
            if not file_only:
                logging.warning(f"ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {data_path}")
                return pd.DataFrame(), "ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤", output_db_path
            
            logging.info(f"ì´ {len(file_only)}ê°œ íŒŒì¼ ë°œê²¬")
            
            # ë°°ì¹˜ í¬ê¸° ê²°ì • (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤)
            if batch_size <= 0:
                batch_size = 1000  # ê¸°ë³¸ê°’
            
            # ë°°ì¹˜ë¡œ ë¶„í• 
            batches = DataProcessor.split_file_list(file_only, batch_size)
            total_batches = len(batches)
            logging.info(f"ì´ {total_batches}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (ë°°ì¹˜ í¬ê¸°: {batch_size})")
            
            # ì§„í–‰ë¥  ë¡œê¹… ê°„ê²© ê²°ì • (ë°°ì¹˜ ìˆ˜ì— ë”°ë¼ ì ì‘ì  ì¡°ì •)
            if total_batches <= 100:
                progress_interval = 1  # 100ê°œ ì´í•˜ë©´ ë§¤ ë°°ì¹˜ë§ˆë‹¤
            elif total_batches <= 1000:
                progress_interval = 5  # 1000ê°œ ì´í•˜ë©´ 5ë°°ì¹˜ë§ˆë‹¤
            elif total_batches <= 5000:
                progress_interval = 25  # 5000ê°œ ì´í•˜ë©´ 25ë°°ì¹˜ë§ˆë‹¤
            else:
                progress_interval = 50  # 5000ê°œ ì´ìƒì´ë©´ 50ë°°ì¹˜ë§ˆë‹¤
            
            # ì§„í–‰ë¥  ë¡œê¹…ì„ ìœ„í•œ ë³€ìˆ˜
            last_logged_batch = 0
            
            # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            if batches:
                first_batch_df, first_batch_info = DataProcessor.process_batch(
                    batches[0], 1, total_batches, max_workers
                )
                DataProcessor.save_to_database(first_batch_df, output_db_path)
                
                # ì²« ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ ë¡œê·¸
                current_progress = (1 / total_batches) * 100
                logging.info(f"ì²« ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ, ì§„í–‰ë¥  ë¡œê·¸ í˜¸ì¶œ ì˜ˆì •: {current_progress:.1f}%")
                DataProcessor.log_progress(f"ì „ì²´ ì§„í–‰ë¥ : {current_progress:.1f}% (1/{total_batches} ë°°ì¹˜)", force_console=True)
                last_logged_batch = 1
                
                # ë‚˜ë¨¸ì§€ ë°°ì¹˜ë“¤ ì²˜ë¦¬
                all_dfs = [first_batch_df]
                batch_infos = [first_batch_info]
                
                # ì „ì²´ ì§„í–‰ë¥  ë¡œê¹… (ì ì‘ì  ê°„ê²©)
                for i, batch in enumerate(batches[1:], 2):
                    batch_df, batch_info = DataProcessor.process_batch(
                        batch, i, total_batches, max_workers
                    )
                    DataProcessor.append_to_database(batch_df, output_db_path)
                    all_dfs.append(batch_df)
                    batch_infos.append(batch_info)
                    
                    # ì§„í–‰ë¥  ë¡œê¹… (ì ì‘ì  ê°„ê²©)
                    if i >= last_logged_batch + progress_interval:
                        current_progress = (i / total_batches) * 100
                        logging.info(f"ì§„í–‰ë¥  ë¡œê·¸ í˜¸ì¶œ: {current_progress:.1f}% ({i}/{total_batches})")
                        DataProcessor.log_progress(f"ì „ì²´ ì§„í–‰ë¥ : {current_progress:.1f}% ({i}/{total_batches} ë°°ì¹˜)", force_console=True)
                        last_logged_batch = i
                
                # ë§ˆì§€ë§‰ ë°°ì¹˜ ì™„ë£Œ ë¡œê·¸
                if last_logged_batch < total_batches:
                    final_progress = (total_batches / total_batches) * 100
                    DataProcessor.log_progress(f"ì „ì²´ ì§„í–‰ë¥ : {final_progress:.1f}% ({total_batches}/{total_batches} ë°°ì¹˜)", force_console=True)
                
                # ì „ì²´ DataFrame ê²°í•©
                pandas_df = pd.concat(all_dfs, ignore_index=True)
                
                # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
                pipeline_end_time = time.time()
                # total_elapsed = pipeline_end_time - pipeline_start_time
                
                # íŒŒì¼ ì •ë³´ ìƒì„±
                file_info = DataProcessor.generate_file_info(pandas_df)
                # file_info += f"\nì „ì²´ ì²˜ë¦¬ ì‹œê°„: {total_elapsed:.1f}ì´ˆ"
                
                # ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½ ì •ë³´ë§Œ í‘œì‹œ
                # total_files_processed = sum(len(batch) for batch in batches)
                # total_size_processed = sum(pandas_df['file_size'])
                # avg_batch_time = total_elapsed / total_batches
                
                # file_info += f"\n\në°°ì¹˜ ì²˜ë¦¬ ìš”ì•½:"
                # file_info += f"\n- ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ"
                # file_info += f"\n- ì´ íŒŒì¼ ìˆ˜: {total_files_processed:,}ê°œ"
                # file_info += f"\n- ì´ ìš©ëŸ‰: {DataProcessor.format_bytes(total_size_processed)}"
                # file_info += f"\n- í‰ê·  ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„: {avg_batch_time:.1f}ì´ˆ"
                # file_info += f"\n- ì²˜ë¦¬ ì†ë„: {total_files_processed/total_elapsed:.0f} íŒŒì¼/ì´ˆ"
                
                # logging.info(f"ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì´ {total_elapsed:.1f}ì´ˆ)")
                return pandas_df, file_info, output_db_path
            else:
                return pd.DataFrame(), "ì²˜ë¦¬í•  ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤", output_db_path
            
        except Exception as e:
            logging.error(f"ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            raise