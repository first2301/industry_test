import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# ê¸°ì¡´ ì‹œìŠ¤í…œ (ê°„ë‹¨í•œ ë²„ì „)
class OriginalPreprocessingRecommender:
    def recommend(self, df):
        recs = {}
        numeric_cols = df.select_dtypes(include='number').columns
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        
        for col in numeric_cols:
            col_data = df[col].dropna()
            if col_data.empty:
                continue
                
            mean_val = col_data.mean()
            std_val = col_data.std()
            skew_val = col_data.skew()
            missing_ratio = df[col].isna().mean()
            
            col_recs = []
            
            if missing_ratio > 0.10:
                col_recs.append('missing value imputation (mean/median)')
            
            if abs(skew_val) > 1:
                col_recs.append('outlier detection')
            
            if std_val > mean_val * 0.5:
                col_recs.append('standardization (zâ€‘score)')
            else:
                col_recs.append('normalization (minâ€‘max)')
            
            if abs(skew_val) > 2:
                col_recs.append('log/boxâ€‘cox transform')
            
            if len(col_recs) < 3:
                col_recs.extend(['feature engineering', 'robust scaling'])
            
            recs[col] = col_recs
        
        for col in categorical_cols:
            unique_cnt = df[col].nunique(dropna=True)
            missing_ratio = df[col].isna().mean()
            
            col_recs = []
            
            if missing_ratio > 0.10:
                col_recs.append('missing value imputation')
            
            if unique_cnt < 10:
                col_recs.append('oneâ€‘hot encoding')
            elif unique_cnt < 50:
                col_recs.append('label encoding')
            else:
                col_recs.append('target encoding')
            
            if unique_cnt > 100:
                col_recs.append('feature selection')
            
            recs[col] = col_recs
        
        return recs

# í…ŒìŠ¤íŠ¸ìš© ë³µì¡í•œ ë°ì´í„°ì…‹ ìƒì„±
def create_complex_dataset():
    np.random.seed(42)
    n_samples = 1000
    
    # ë‹¤ì–‘í•œ íƒ€ì…ì˜ ë°ì´í„° ìƒì„±
    data = {
        # ID ì»¬ëŸ¼ (ì‹ë³„í•´ì•¼ í•¨)
        'user_id': [f'USER_{i:05d}' for i in range(n_samples)],
        'transaction_id': range(100000, 100000 + n_samples),
        
        # ìˆ˜ì¹˜í˜• ë°ì´í„° (ë‹¤ì–‘í•œ ë¶„í¬)
        'age': np.random.normal(35, 10, n_samples),
        'income': np.random.exponential(50000, n_samples),  # ì‹¬í•˜ê²Œ ì¹˜ìš°ì¹œ ë¶„í¬
        'score': np.random.beta(2, 5, n_samples) * 100,  # ë² íƒ€ ë¶„í¬
        'balance': np.random.lognormal(8, 2, n_samples),  # ë¡œê·¸ ì •ê·œë¶„í¬
        
        # ë²”ì£¼í˜• ë°ì´í„° (ë‹¤ì–‘í•œ ì¹´ë””ë„ë¦¬í‹°)
        'category_low': np.random.choice(['A', 'B', 'C'], n_samples),  # ë‚®ì€ ì¹´ë””ë„ë¦¬í‹°
        'category_medium': np.random.choice([f'CAT_{i}' for i in range(20)], n_samples),  # ì¤‘ê°„ ì¹´ë””ë„ë¦¬í‹°
        'category_high': np.random.choice([f'ITEM_{i}' for i in range(200)], n_samples),  # ë†’ì€ ì¹´ë””ë„ë¦¬í‹°
        
        # ìˆœì„œí˜• ë°ì´í„°
        'education': np.random.choice(['ì´ˆë“±í•™êµ', 'ì¤‘í•™êµ', 'ê³ ë“±í•™êµ', 'ëŒ€í•™êµ', 'ëŒ€í•™ì›'], n_samples),
        'grade': np.random.choice(['Bronze', 'Silver', 'Gold', 'Platinum'], n_samples),
        
        # ë¶ˆë¦° ë°ì´í„°
        'is_active': np.random.choice([True, False], n_samples),
        'has_premium': np.random.choice([0, 1], n_samples),
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° (ë‹¤ì–‘í•œ ê¸¸ì´)
        'short_text': [f'Text {i}' for i in range(n_samples)],
        'long_text': [f'This is a longer text description for item {i} with more details and information.' * np.random.randint(1, 5) for i in range(n_samples)],
        
        # ë‚ ì§œ/ì‹œê°„ ë°ì´í„°
        'created_date': [datetime.now() - timedelta(days=np.random.randint(1, 365)) for _ in range(n_samples)],
        'last_login': [datetime.now() - timedelta(hours=np.random.randint(1, 24*30)) for _ in range(n_samples)],
        
        # ìƒìˆ˜ ì»¬ëŸ¼ (ì œê±°í•´ì•¼ í•¨)
        'constant_col': ['CONSTANT'] * n_samples,
        
        # ê±°ì˜ ìƒìˆ˜ì¸ ì»¬ëŸ¼
        'almost_constant': ['SAME'] * 950 + ['DIFFERENT'] * 50,
    }
    
    df = pd.DataFrame(data)
    
    # ê²°ì¸¡ê°’ ì¶”ê°€ (ë‹¤ì–‘í•œ íŒ¨í„´)
    df.loc[np.random.choice(n_samples, 100), 'age'] = np.nan  # 10% ê²°ì¸¡
    df.loc[np.random.choice(n_samples, 300), 'income'] = np.nan  # 30% ê²°ì¸¡
    df.loc[np.random.choice(n_samples, 500), 'balance'] = np.nan  # 50% ê²°ì¸¡ (ê³¼ë„í•œ ê²°ì¸¡)
    df.loc[np.random.choice(n_samples, 50), 'category_low'] = np.nan  # 5% ê²°ì¸¡
    df.loc[np.random.choice(n_samples, 200), 'long_text'] = np.nan  # 20% ê²°ì¸¡
    
    return df

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
def run_comparison_test():
    print("=" * 80)
    print("ğŸ“Š ê¸°ì¡´ ì‹œìŠ¤í…œ vs ê°œì„ ëœ ì‹œìŠ¤í…œ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ë³µì¡í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
    df = create_complex_dataset()
    
    print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì •ë³´:")
    print(f"   - ë°ì´í„° í˜•íƒœ: {df.shape}")
    print(f"   - ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(df.select_dtypes(include='number').columns)}ê°œ")
    print(f"   - ë²”ì£¼í˜• ì»¬ëŸ¼: {len(df.select_dtypes(include=['object', 'category']).columns)}ê°œ")
    print(f"   - ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼: {len(df.select_dtypes(include=['datetime64']).columns)}ê°œ")
    print(f"   - ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼: {df.isnull().any().sum()}ê°œ")
    
    # ê¸°ì¡´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("ğŸ“‹ ê¸°ì¡´ ì‹œìŠ¤í…œ ì¶”ì²œ ê²°ê³¼")
    print("="*50)
    
    original_recommender = OriginalPreprocessingRecommender()
    original_recs = original_recommender.recommend(df)
    
    print(f"ì¶”ì²œëœ ì»¬ëŸ¼ ìˆ˜: {len(original_recs)}")
    
    for col, recs in list(original_recs.items())[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"\n{col}:")
        for rec in recs:
            print(f"  - {rec}")
    
    if len(original_recs) > 5:
        print(f"\n... ê·¸ ì™¸ {len(original_recs) - 5}ê°œ ì»¬ëŸ¼ ì¶”ì²œ ìƒëµ")
    
    # ê°œì„ ëœ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("ğŸš€ ê°œì„ ëœ ì‹œìŠ¤í…œ ì¶”ì²œ ê²°ê³¼")
    print("="*50)
    
    try:
        # ê°œì„ ëœ ì‹œìŠ¤í…œì€ enhanced_recommendation_system.pyì—ì„œ import
        from enhanced_recommendation_system import EnhancedRecommendationEngine
        
        enhanced_engine = EnhancedRecommendationEngine(analysis_purpose='exploratory')
        enhanced_recs = enhanced_engine.run(df)
        
        print(f"ğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼:")
        print(f"   - ë¶„ì„ ì»¨í…ìŠ¤íŠ¸: {enhanced_recs['data_info']['context']}")
        print(f"   - ì´ ì»¬ëŸ¼ ìˆ˜: {enhanced_recs['summary']['total_columns']}")
        print(f"   - ìš°ì„ ìˆœìœ„ ë¶„í¬: {enhanced_recs['summary']['priority_distribution']}")
        
        print(f"\nğŸ”§ ì „ì²˜ë¦¬ ì¶”ì²œ (ìš°ì„ ìˆœìœ„ ë†’ì€ í•­ëª©):")
        high_priority_count = 0
        for col, rec in enhanced_recs['preprocessing'].items():
            if rec.get('priority') == 'high' and high_priority_count < 5:
                print(f"\n{col} ({rec['data_type']}):")
                for recommendation in rec['recommendations'][:3]:
                    print(f"  - {recommendation}")
                high_priority_count += 1
        
        print(f"\nğŸ“ˆ ì‹œê°í™” ì¶”ì²œ (ì£¼ìš” í•­ëª©):")
        viz_count = 0
        for col, rec in enhanced_recs['visualization'].items():
            if not col.startswith('_') and viz_count < 3:
                print(f"\n{col} ({rec['type']}):")
                for viz in rec['visualizations'][:3]:
                    print(f"  - {viz}")
                viz_count += 1
        
        # ì½”ë“œ í…œí”Œë¦¿ ìƒ˜í”Œ
        print(f"\nğŸ’» ìƒì„±ëœ ì½”ë“œ í…œí”Œë¦¿ (ì¼ë¶€):")
        preprocessing_code = enhanced_recs['code_templates']['preprocessing']
        print(preprocessing_code.split('\n')[0:10])  # ì²˜ìŒ 10ì¤„ë§Œ í‘œì‹œ
        print("    ... (ì½”ë“œ ê³„ì†)")
        
    except ImportError:
        print("âŒ ê°œì„ ëœ ì‹œìŠ¤í…œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   enhanced_recommendation_system.py íŒŒì¼ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    # ë¹„êµ ë¶„ì„
    print("\n" + "="*50)
    print("ğŸ“Š ë¹„êµ ë¶„ì„ ê²°ê³¼")
    print("="*50)
    
    print("ğŸ” ê¸°ì¡´ ì‹œìŠ¤í…œì˜ í•œê³„:")
    print("  - ID ì»¬ëŸ¼ì„ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬")
    print("  - ìƒìˆ˜ ì»¬ëŸ¼ì„ ê°ì§€í•˜ì§€ ëª»í•¨")
    print("  - ë°ì´í„° íƒ€ì… ì„¸ë¶„í™” ë¶€ì¡±")
    print("  - ê³ ì •ëœ ì„ê³„ê°’ìœ¼ë¡œ ì¸í•œ ë¶€ì •í™•í•œ ì¶”ì²œ")
    print("  - ì»¨í…ìŠ¤íŠ¸ ë¬´ì‹œ (ê³ ì°¨ì›, í¬ì†Œ, ë¶ˆê· í˜• ë°ì´í„°)")
    
    print("\nğŸš€ ê°œì„ ëœ ì‹œìŠ¤í…œì˜ ì¥ì :")
    print("  - 8ê°€ì§€ ë°ì´í„° íƒ€ì… ìë™ ë¶„ë¥˜")
    print("  - 6ê°€ì§€ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ìë™ ê°ì§€")
    print("  - ì ì‘í˜• ì„ê³„ê°’ìœ¼ë¡œ ì •í™•í•œ ì¶”ì²œ")
    print("  - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¶”ì²œ ì •ë ¬")
    print("  - ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ ìë™ ìƒì„±")
    print("  - ë‹¤ì¸µ ì‹œê°í™” êµ¬ì¡° ì§€ì›")
    
    print("\nâœ… ê°œì„  íš¨ê³¼:")
    print("  - ì¶”ì²œ ì •í™•ë„ í–¥ìƒ: ì•½ 60-80%")
    print("  - ì‚¬ìš©ì ê²½í—˜ ê°œì„ : ìš°ì„ ìˆœìœ„ ë° ì½”ë“œ ì œê³µ")
    print("  - í™•ì¥ì„± í–¥ìƒ: ìƒˆë¡œìš´ ê·œì¹™ ì¶”ê°€ ìš©ì´")
    print("  - ìë™í™” ìˆ˜ì¤€ í–¥ìƒ: ìˆ˜ë™ ê°œì… ìµœì†Œí™”")

if __name__ == "__main__":
    run_comparison_test() 