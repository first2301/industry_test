import pandas as pd
import numpy as np
from sklearn.preprocessing import (
    StandardScaler, MinMaxScaler, RobustScaler,
    LabelEncoder, OneHotEncoder, OrdinalEncoder
)
from sklearn.impute import SimpleImputer
from sklearn.ensemble import IsolationForest
from sklearn.feature_selection import VarianceThreshold
from scipy import stats
from scipy.stats import boxcox, yeojohnson
import warnings
warnings.filterwarnings('ignore')

class SpecificPreprocessingMethods:
    """êµ¬ì²´ì ì¸ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ì˜ ì‹¤ì œ êµ¬í˜„"""
    
    def __init__(self):
        self.fitted_transformers = {}
    
    # =============================================================================
    # ë³€í™˜ ë°©ë²•ë“¤ (Transform Methods)
    # =============================================================================
    
    def yeo_johnson_transform(self, data: pd.Series, column_name: str) -> pd.Series:
        """ê·¹ë„ë¡œ ì™œë„ê°€ ë†’ì€ ê²½ìš° - Yeo-Johnson ë³€í™˜"""
        transformed_data, lambda_param = yeojohnson(data.dropna())
        result = pd.Series(transformed_data, index=data.dropna().index)
        print(f"ğŸ”„ {column_name}: Yeo-Johnson ë³€í™˜ ì ìš© (Î»={lambda_param:.3f})")
        return result
    
    def boxcox_transform(self, data: pd.Series, column_name: str) -> pd.Series:
        """ë†’ì€ ì™œë„ + ì²¨ë„ - Box-Cox ë³€í™˜"""
        if (data <= 0).any():
            # ìŒìˆ˜ë‚˜ 0ì´ ìˆìœ¼ë©´ ìƒìˆ˜ë¥¼ ë”í•´ì„œ ì–‘ìˆ˜ë¡œ ë§Œë“¦
            data_positive = data - data.min() + 1
            transformed_data, lambda_param = boxcox(data_positive.dropna())
        else:
            transformed_data, lambda_param = boxcox(data.dropna())
        
        result = pd.Series(transformed_data, index=data.dropna().index)
        print(f"ğŸ”„ {column_name}: Box-Cox ë³€í™˜ ì ìš© (Î»={lambda_param:.3f})")
        return result
    
    def log_transform(self, data: pd.Series, column_name: str) -> pd.Series:
        """ì ë‹¹íˆ ë†’ì€ ì™œë„ - ë¡œê·¸ ë³€í™˜"""
        if (data <= 0).any():
            # ìŒìˆ˜ë‚˜ 0ì´ ìˆìœ¼ë©´ ìƒìˆ˜ë¥¼ ë”í•´ì„œ ì–‘ìˆ˜ë¡œ ë§Œë“¦
            data_positive = data - data.min() + 1
            result = np.log(data_positive)
        else:
            result = np.log(data)
        print(f"ğŸ”„ {column_name}: ë¡œê·¸ ë³€í™˜ ì ìš©")
        return result
    
    def sqrt_transform(self, data: pd.Series, column_name: str) -> pd.Series:
        """ì¤‘ê°„ ì™œë„ - ì œê³±ê·¼ ë³€í™˜"""
        if (data < 0).any():
            # ìŒìˆ˜ê°€ ìˆìœ¼ë©´ ìƒìˆ˜ë¥¼ ë”í•´ì„œ ì–‘ìˆ˜ë¡œ ë§Œë“¦
            data_positive = data - data.min()
            result = np.sqrt(data_positive)
        else:
            result = np.sqrt(data)
        print(f"ğŸ”„ {column_name}: ì œê³±ê·¼ ë³€í™˜ ì ìš©")
        return result
    
    def winsorize(self, data: pd.Series, column_name: str, limits: tuple = (0.05, 0.05)) -> pd.Series:
        """ë†’ì€ ì²¨ë„ - ìœˆì €í™” (ê·¹ê°’ ì œí•œ)"""
        from scipy.stats.mstats import winsorize
        result = pd.Series(winsorize(data, limits=limits), index=data.index)
        print(f"ğŸ”„ {column_name}: ìœˆì €í™” ì ìš© (í•˜ìœ„ {limits[0]*100}%, ìƒìœ„ {limits[1]*100}% ì œí•œ)")
        return result
    
    # =============================================================================
    # ì´ìƒì¹˜ íƒì§€ ë°©ë²•ë“¤ (Outlier Detection)
    # =============================================================================
    
    def isolation_forest(self, data: pd.DataFrame, column_name: str, contamination: float = 0.1) -> pd.Series:
        """ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€ - Isolation Forest"""
        iso_forest = IsolationForest(contamination=contamination, random_state=42)
        outliers = iso_forest.fit_predict(data.select_dtypes(include=[np.number]))
        result = pd.Series(outliers == -1, index=data.index)
        print(f"ğŸ” {column_name}: Isolation Forest ì´ìƒì¹˜ íƒì§€ ({result.sum()}ê°œ ì´ìƒì¹˜ ë°œê²¬)")
        return result
    
    def iqr_method(self, data: pd.Series, column_name: str) -> pd.Series:
        """ë‹¨ë³€ëŸ‰ ì´ìƒì¹˜ íƒì§€ - IQR ë°©ë²•"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = (data < lower_bound) | (data > upper_bound)
        print(f"ğŸ” {column_name}: IQR ë°©ë²• ì´ìƒì¹˜ íƒì§€ ({outliers.sum()}ê°œ ì´ìƒì¹˜ ë°œê²¬)")
        return outliers
    
    def z_score_method(self, data: pd.Series, column_name: str, threshold: float = 3.0) -> pd.Series:
        """ì •ê·œë¶„í¬ ì´ìƒì¹˜ íƒì§€ - Z-score ë°©ë²•"""
        z_scores = np.abs(stats.zscore(data.dropna()))
        outliers = pd.Series(False, index=data.index)
        outliers.loc[data.dropna().index] = z_scores > threshold
        print(f"ğŸ” {column_name}: Z-score ë°©ë²• ì´ìƒì¹˜ íƒì§€ ({outliers.sum()}ê°œ ì´ìƒì¹˜ ë°œê²¬)")
        return outliers
    
    # =============================================================================
    # ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë“¤ (Scaling Methods)
    # =============================================================================
    
    def robust_scaler(self, data: pd.Series, column_name: str) -> pd.Series:
        """ë†’ì€ ë¶„ì‚° + ì´ìƒì¹˜ - Robust Scaler"""
        scaler = RobustScaler()
        scaled_data = scaler.fit_transform(data.values.reshape(-1, 1)).flatten()
        self.fitted_transformers[f'{column_name}_robust_scaler'] = scaler
        print(f"âš–ï¸ {column_name}: Robust Scaler ì ìš© (ì¤‘ì•™ê°’ ê¸°ë°˜)")
        return pd.Series(scaled_data, index=data.index)
    
    def standard_scaler(self, data: pd.Series, column_name: str) -> pd.Series:
        """ì ë‹¹í•œ ë¶„ì‚° - Standard Scaler"""
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(data.values.reshape(-1, 1)).flatten()
        self.fitted_transformers[f'{column_name}_standard_scaler'] = scaler
        print(f"âš–ï¸ {column_name}: Standard Scaler ì ìš© (í‰ê· =0, í‘œì¤€í¸ì°¨=1)")
        return pd.Series(scaled_data, index=data.index)
    
    def min_max_scaler(self, data: pd.Series, column_name: str) -> pd.Series:
        """ê¸°ë³¸ ìŠ¤ì¼€ì¼ë§ - MinMax Scaler"""
        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(data.values.reshape(-1, 1)).flatten()
        self.fitted_transformers[f'{column_name}_minmax_scaler'] = scaler
        print(f"âš–ï¸ {column_name}: MinMax Scaler ì ìš© (ë²”ìœ„ [0,1])")
        return pd.Series(scaled_data, index=data.index)
    
    # =============================================================================
    # ì¸ì½”ë”© ë°©ë²•ë“¤ (Encoding Methods)
    # =============================================================================
    
    def binary_encode(self, data: pd.Series, column_name: str) -> pd.DataFrame:
        """ì´ì§„ ë³€ìˆ˜ - Binary Encoding"""
        # ê°„ë‹¨í•œ ì´ì§„ ì¸ì½”ë”©
        unique_values = data.unique()
        mapping = {unique_values[0]: 0, unique_values[1]: 1}
        result = pd.DataFrame({f'{column_name}_binary': data.map(mapping)})
        print(f"ğŸ·ï¸ {column_name}: Binary Encoding ì ìš© ({mapping})")
        return result
    
    def one_hot_encode(self, data: pd.Series, column_name: str) -> pd.DataFrame:
        """ë‚®ì€ ì¹´ë””ë„ë¦¬í‹° - One-Hot Encoding"""
        encoded = pd.get_dummies(data, prefix=column_name)
        print(f"ğŸ·ï¸ {column_name}: One-Hot Encoding ì ìš© ({encoded.shape[1]}ê°œ ì»¬ëŸ¼ ìƒì„±)")
        return encoded
    
    def ordinal_encode(self, data: pd.Series, column_name: str) -> pd.Series:
        """ì¤‘ê°„ ì¹´ë””ë„ë¦¬í‹° - Ordinal Encoding"""
        encoder = OrdinalEncoder()
        encoded_data = encoder.fit_transform(data.values.reshape(-1, 1)).flatten()
        self.fitted_transformers[f'{column_name}_ordinal_encoder'] = encoder
        print(f"ğŸ·ï¸ {column_name}: Ordinal Encoding ì ìš© ({len(encoder.categories_[0])}ê°œ ì¹´í…Œê³ ë¦¬)")
        return pd.Series(encoded_data, index=data.index)
    
    def target_encode(self, data: pd.Series, target: pd.Series, column_name: str) -> pd.Series:
        """ë†’ì€ ì¹´ë””ë„ë¦¬í‹° - Target Encoding"""
        target_means = data.groupby(data).apply(lambda x: target.loc[x.index].mean())
        encoded_data = data.map(target_means)
        print(f"ğŸ·ï¸ {column_name}: Target Encoding ì ìš© ({len(target_means)}ê°œ ì¹´í…Œê³ ë¦¬)")
        return encoded_data
    
    def feature_hashing(self, data: pd.Series, column_name: str, n_features: int = 100) -> pd.DataFrame:
        """ë§¤ìš° ë†’ì€ ì¹´ë””ë„ë¦¬í‹° - Feature Hashing"""
        from sklearn.feature_extraction import FeatureHasher
        hasher = FeatureHasher(n_features=n_features, input_type='string')
        hashed_features = hasher.fit_transform(data.astype(str).values.reshape(-1, 1))
        
        result = pd.DataFrame(
            hashed_features.toarray(),
            columns=[f'{column_name}_hash_{i}' for i in range(n_features)],
            index=data.index
        )
        print(f"ğŸ·ï¸ {column_name}: Feature Hashing ì ìš© ({n_features}ê°œ í•´ì‹œ í”¼ì²˜)")
        return result
    
    # =============================================================================
    # íŠ¹ì„± ì„ íƒ ë°©ë²•ë“¤ (Feature Selection)
    # =============================================================================
    
    def drop_constant(self, data: pd.Series, column_name: str) -> None:
        """ìƒìˆ˜ ì»¬ëŸ¼ ì œê±°"""
        print(f"ğŸ—‘ï¸ {column_name}: ìƒìˆ˜ ì»¬ëŸ¼ ì œê±° (ê³ ìœ ê°’ {data.nunique()}ê°œ)")
        return None
    
    def feature_selection(self, data: pd.Series, column_name: str, threshold: float = 0.1) -> pd.Series:
        """ì €ë¶„ì‚° íŠ¹ì„± ì„ íƒ"""
        selector = VarianceThreshold(threshold=threshold)
        selected = selector.fit_transform(data.values.reshape(-1, 1))
        if selected.shape[1] == 0:
            print(f"ğŸ—‘ï¸ {column_name}: ì €ë¶„ì‚°ìœ¼ë¡œ ì¸í•œ íŠ¹ì„± ì œê±°")
            return None
        else:
            print(f"âœ… {column_name}: ë¶„ì‚° ì„ê³„ê°’ í†µê³¼")
            return data

# =============================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# =============================================================================

def demonstrate_specific_preprocessing():
    """êµ¬ì²´ì ì¸ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ì˜ ì‚¬ìš© ì˜ˆì‹œ"""
    print("=" * 80)
    print("ğŸ”§ êµ¬ì²´ì ì¸ ì „ì²˜ë¦¬ ë°©ë²•ë“¤ - ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 1000
    
    # ë‹¤ì–‘í•œ ë¶„í¬ì˜ ë°ì´í„° ìƒì„±
    normal_data = pd.Series(np.random.normal(0, 1, n_samples), name='normal_col')
    skewed_data = pd.Series(np.random.exponential(2, n_samples), name='skewed_col')
    very_skewed_data = pd.Series(np.random.pareto(0.1, n_samples), name='very_skewed_col')
    
    # ë²”ì£¼í˜• ë°ì´í„° ìƒì„±
    low_card_data = pd.Series(np.random.choice(['A', 'B', 'C'], n_samples), name='low_card')
    high_card_data = pd.Series([f'category_{i}' for i in np.random.randint(0, 500, n_samples)], name='high_card')
    
    # ì „ì²˜ë¦¬ ê°ì²´ ìƒì„±
    preprocessor = SpecificPreprocessingMethods()
    
    print("\nğŸ“Š ë³€í™˜ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸:")
    print("-" * 50)
    
    # ë³€í™˜ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸
    transformed_normal = preprocessor.winsorize(normal_data, 'normal_col')
    transformed_skewed = preprocessor.log_transform(skewed_data, 'skewed_col')
    transformed_very_skewed = preprocessor.yeo_johnson_transform(very_skewed_data, 'very_skewed_col')
    
    print("\nğŸ” ì´ìƒì¹˜ íƒì§€ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸:")
    print("-" * 50)
    
    # ì´ìƒì¹˜ íƒì§€ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸
    outliers_iqr = preprocessor.iqr_method(normal_data, 'normal_col')
    outliers_zscore = preprocessor.z_score_method(skewed_data, 'skewed_col')
    
    print("\nâš–ï¸ ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸:")
    print("-" * 50)
    
    # ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸
    scaled_robust = preprocessor.robust_scaler(very_skewed_data, 'very_skewed_col')
    scaled_standard = preprocessor.standard_scaler(normal_data, 'normal_col')
    scaled_minmax = preprocessor.min_max_scaler(skewed_data, 'skewed_col')
    
    print("\nğŸ·ï¸ ì¸ì½”ë”© ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸:")
    print("-" * 50)
    
    # ì¸ì½”ë”© ë°©ë²•ë“¤ í…ŒìŠ¤íŠ¸
    encoded_onehot = preprocessor.one_hot_encode(low_card_data, 'low_card')
    encoded_ordinal = preprocessor.ordinal_encode(low_card_data, 'low_card')
    encoded_hash = preprocessor.feature_hashing(high_card_data, 'high_card', n_features=50)
    
    print("\nâœ… ëª¨ë“  ì „ì²˜ë¦¬ ë°©ë²• í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 80)

if __name__ == "__main__":
    demonstrate_specific_preprocessing() 