import pandas as pd
import numpy as np
import warnings
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataType(Enum):
    """ë°ì´í„° íƒ€ì… ë¶„ë¥˜"""
    NUMERIC_CONTINUOUS = "numeric_continuous"
    NUMERIC_DISCRETE = "numeric_discrete"
    CATEGORICAL_NOMINAL = "categorical_nominal"
    CATEGORICAL_ORDINAL = "categorical_ordinal"
    DATETIME = "datetime"
    TEXT = "text"
    BOOLEAN = "boolean"
    ID = "id"

class DataContext(Enum):
    """ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    GENERAL = "general"
    TIMESERIES = "timeseries"
    IMBALANCED = "imbalanced"
    HIGH_DIMENSIONAL = "high_dimensional"
    SPARSE = "sparse"
    MULTIMODAL = "multimodal"

@dataclass
class DataProfiler:
    """ë°ì´í„° íŠ¹ì„± í”„ë¡œíŒŒì¼ë§"""
    column_name: str
    data_type: DataType
    missing_ratio: float
    unique_count: int
    unique_ratio: float
    skewness: Optional[float] = None
    kurtosis: Optional[float] = None
    mean: Optional[float] = None
    std: Optional[float] = None
    cv: Optional[float] = None  # ë³€ë™ê³„ìˆ˜
    outlier_ratio: Optional[float] = None
    is_constant: bool = False
    is_id_like: bool = False
    text_length_stats: Optional[Dict] = None
    datetime_range: Optional[Tuple] = None

class EnhancedPreprocessingRecommender:
    """
    ê³ ë„í™”ëœ ì „ì²˜ë¦¬ ì¶”ì²œ ì‹œìŠ¤í…œ
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â–¸ ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì… ì§€ì›
    â–¸ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì²œ
    â–¸ ì ì‘í˜• ì„ê³„ê°’
    â–¸ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì¶”ì²œ
    """
    
    def __init__(self):
        self.profiles: Dict[str, DataProfiler] = {}
        self.context: DataContext = DataContext.GENERAL
        self.thresholds = self._get_adaptive_thresholds()
    
    def _get_adaptive_thresholds(self) -> Dict[str, float]:
        """ì ì‘í˜• ì„ê³„ê°’ ì„¤ì •"""
        return {
            'missing_low': 0.05,
            'missing_medium': 0.15,
            'missing_high': 0.40,
            'skewness_moderate': 1.0,
            'skewness_high': 2.0,
            'kurtosis_high': 7.0,
            'cv_low': 0.1,
            'cv_high': 1.0,
            'correlation_high': 0.8,
            'correlation_very_high': 0.95,
            'unique_ratio_low': 0.01,
            'unique_ratio_high': 0.8,
            'outlier_ratio_threshold': 0.05
        }
    
    def profile_data(self, df: pd.DataFrame) -> None:
        """ë°ì´í„° í”„ë¡œíŒŒì¼ë§"""
        self.profiles.clear()
        
        # ì „ì²´ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        self.context = self._analyze_data_context(df)
        
        # ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì„ê³„ê°’ ì¡°ì •
        self._adjust_thresholds_by_context()
        
        for col in df.columns:
            profile = self._profile_column(df, col)
            self.profiles[col] = profile
    
    def _analyze_data_context(self, df: pd.DataFrame) -> DataContext:
        """ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        n_rows, n_cols = df.shape
        
        # ì‹œê³„ì—´ ë°ì´í„° ê²€ì‚¬
        datetime_cols = df.select_dtypes(include=['datetime64']).columns
        if len(datetime_cols) > 0 or any('date' in col.lower() or 'time' in col.lower() for col in df.columns):
            return DataContext.TIMESERIES
        
        # ê³ ì°¨ì› ë°ì´í„° ê²€ì‚¬
        if n_cols > 100:
            return DataContext.HIGH_DIMENSIONAL
        
        # í¬ì†Œ ë°ì´í„° ê²€ì‚¬
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            zero_ratio = (df[numeric_cols] == 0).sum().sum() / (n_rows * len(numeric_cols))
            if zero_ratio > 0.8:
                return DataContext.SPARSE
        
        # ë¶ˆê· í˜• ë°ì´í„° ê²€ì‚¬ (íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ë¶ˆê· í˜• ì •ë„ë¡œ íŒë‹¨
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        for col in categorical_cols:
            if df[col].nunique() < 10:
                value_counts = df[col].value_counts()
                if len(value_counts) > 1:
                    imbalance_ratio = value_counts.iloc[0] / value_counts.iloc[1]
                    if imbalance_ratio > 10:
                        return DataContext.IMBALANCED
        
        return DataContext.GENERAL
    
    def _adjust_thresholds_by_context(self) -> None:
        """ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì„ê³„ê°’ ì¡°ì •"""
        if self.context == DataContext.HIGH_DIMENSIONAL:
            self.thresholds['missing_high'] = 0.20  # ê³ ì°¨ì›ì—ì„œëŠ” ë” ì—„ê²©
            self.thresholds['correlation_high'] = 0.7
        elif self.context == DataContext.IMBALANCED:
            self.thresholds['missing_medium'] = 0.10  # ë¶ˆê· í˜• ë°ì´í„°ì—ì„œëŠ” ë” ë³´ìˆ˜ì 
        elif self.context == DataContext.SPARSE:
            self.thresholds['missing_high'] = 0.60  # í¬ì†Œ ë°ì´í„°ì—ì„œëŠ” ë” ê´€ëŒ€
    
    def _profile_column(self, df: pd.DataFrame, col: str) -> DataProfiler:
        """ê°œë³„ ì»¬ëŸ¼ í”„ë¡œíŒŒì¼ë§"""
        series = df[col]
        missing_ratio = series.isnull().mean()
        unique_count = series.nunique()
        unique_ratio = unique_count / len(series)
        
        # ë°ì´í„° íƒ€ì… ë¶„ë¥˜
        data_type = self._classify_data_type(series)
        
        # ê¸°ë³¸ í”„ë¡œíŒŒì¼ ìƒì„±
        profile = DataProfiler(
            column_name=col,
            data_type=data_type,
            missing_ratio=missing_ratio,
            unique_count=unique_count,
            unique_ratio=unique_ratio,
            is_constant=unique_count <= 1,
            is_id_like=self._is_id_like(series)
        )
        
        # ìˆ˜ì¹˜í˜• ë°ì´í„° ì¶”ê°€ ë¶„ì„
        if data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:
            numeric_data = pd.to_numeric(series, errors='coerce').dropna()
            if len(numeric_data) > 0:
                profile.mean = numeric_data.mean()
                profile.std = numeric_data.std()
                profile.cv = profile.std / profile.mean if profile.mean != 0 else np.inf
                profile.skewness = numeric_data.skew()
                profile.kurtosis = numeric_data.kurtosis()
                profile.outlier_ratio = self._calculate_outlier_ratio(numeric_data)
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€ ë¶„ì„
        elif data_type == DataType.TEXT:
            text_data = series.dropna().astype(str)
            if len(text_data) > 0:
                lengths = text_data.str.len()
                profile.text_length_stats = {
                    'mean_length': lengths.mean(),
                    'max_length': lengths.max(),
                    'min_length': lengths.min(),
                    'std_length': lengths.std()
                }
        
        # ë‚ ì§œ/ì‹œê°„ ë°ì´í„° ì¶”ê°€ ë¶„ì„
        elif data_type == DataType.DATETIME:
            datetime_data = pd.to_datetime(series, errors='coerce').dropna()
            if len(datetime_data) > 0:
                profile.datetime_range = (datetime_data.min(), datetime_data.max())
        
        return profile
    
    def _classify_data_type(self, series: pd.Series) -> DataType:
        """ë°ì´í„° íƒ€ì… ë¶„ë¥˜"""
        col_name = series.name.lower()
        
        # ID ì»¬ëŸ¼ ê²€ì‚¬
        if self._is_id_like(series):
            return DataType.ID
        
        # ë¶ˆë¦° íƒ€ì… ê²€ì‚¬
        if series.dtype == bool or set(series.dropna().unique()) <= {0, 1, True, False}:
            return DataType.BOOLEAN
        
        # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ê²€ì‚¬
        if series.dtype.name.startswith('datetime') or 'date' in col_name or 'time' in col_name:
            return DataType.DATETIME
        
        # ìˆ˜ì¹˜í˜• íƒ€ì… ê²€ì‚¬
        if pd.api.types.is_numeric_dtype(series):
            # ì´ì‚°í˜• vs ì—°ì†í˜• íŒë‹¨
            unique_count = series.nunique()
            if unique_count <= 20 and series.dtype in ['int64', 'int32']:
                return DataType.NUMERIC_DISCRETE
            else:
                return DataType.NUMERIC_CONTINUOUS
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ë¶„ë¥˜
        if series.dtype == 'object':
            text_data = series.dropna().astype(str)
            if len(text_data) > 0:
                avg_length = text_data.str.len().mean()
                if avg_length > 50:  # í‰ê·  ê¸¸ì´ 50ì ì´ìƒì´ë©´ í…ìŠ¤íŠ¸
                    return DataType.TEXT
        
        # ë²”ì£¼í˜• íƒ€ì… ê²€ì‚¬
        if series.dtype.name == 'category' or series.dtype == 'object':
            unique_count = series.nunique()
            if unique_count <= 50:
                # ìˆœì„œí˜• vs ëª…ëª©í˜• íŒë‹¨ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                unique_vals = series.dropna().astype(str).unique()
                if any(val in ['low', 'medium', 'high', 'small', 'large'] for val in unique_vals):
                    return DataType.CATEGORICAL_ORDINAL
                return DataType.CATEGORICAL_NOMINAL
        
        return DataType.CATEGORICAL_NOMINAL
    
    def _is_id_like(self, series: pd.Series) -> bool:
        """ID ì»¬ëŸ¼ ì—¬ë¶€ íŒë‹¨"""
        col_name = series.name.lower()
        
        # ì»¬ëŸ¼ëª… ê¸°ë°˜ íŒë‹¨
        if any(keyword in col_name for keyword in ['id', 'key', 'index', 'idx']):
            return True
        
        # ê³ ìœ ê°’ ë¹„ìœ¨ ê¸°ë°˜ íŒë‹¨
        if series.nunique() / len(series) > 0.95:
            return True
        
        return False
    
    def _calculate_outlier_ratio(self, data: pd.Series) -> float:
        """ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚° (IQR ë°©ë²•)"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        outliers = data[(data < lower_bound) | (data > upper_bound)]
        return len(outliers) / len(data)
    
    def recommend(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:
        """ì „ì²˜ë¦¬ ì¶”ì²œ"""
        self.profile_data(df)
        recommendations = {}
        
        for col, profile in self.profiles.items():
            col_recs = self._generate_column_recommendations(df, profile)
            recommendations[col] = {
                'data_type': profile.data_type.value,
                'recommendations': col_recs,
                'priority': self._calculate_priority(profile),
                'context': self.context.value
            }
        
        # ì „ì—­ ì¶”ì²œì‚¬í•­ ì¶”ê°€
        global_recs = self._generate_global_recommendations(df)
        if global_recs:
            recommendations['_global'] = {
                'data_type': 'global',
                'recommendations': global_recs,
                'priority': 'high',
                'context': self.context.value
            }
        
        return recommendations
    
    def _generate_column_recommendations(self, df: pd.DataFrame, profile: DataProfiler) -> List[str]:
        """ì»¬ëŸ¼ë³„ ì¶”ì²œ ìƒì„±"""
        recs = []
        
        # ID ì»¬ëŸ¼ ì²˜ë¦¬
        if profile.data_type == DataType.ID:
            recs.append('consider_dropping_id_column')
            return recs
        
        # ìƒìˆ˜ ì»¬ëŸ¼ ì²˜ë¦¬
        if profile.is_constant:
            recs.append('drop_constant_column')
            return recs
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        if profile.missing_ratio > self.thresholds['missing_high']:
            recs.append('drop_column_excessive_missing')
        elif profile.missing_ratio > self.thresholds['missing_medium']:
            recs.append('advanced_imputation_knn_iterative')
        elif profile.missing_ratio > self.thresholds['missing_low']:
            if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:
                recs.append('impute_median_mode')
            else:
                recs.append('impute_mode_frequent')
        
        # ìˆ˜ì¹˜í˜• ë°ì´í„° ì²˜ë¦¬
        if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:
            recs.extend(self._get_numeric_recommendations(profile))
        
        # ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬
        elif profile.data_type in [DataType.CATEGORICAL_NOMINAL, DataType.CATEGORICAL_ORDINAL]:
            recs.extend(self._get_categorical_recommendations(profile))
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
        elif profile.data_type == DataType.TEXT:
            recs.extend(self._get_text_recommendations(profile))
        
        # ë‚ ì§œ/ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
        elif profile.data_type == DataType.DATETIME:
            recs.extend(self._get_datetime_recommendations(profile))
        
        return recs
    
    def _get_numeric_recommendations(self, profile: DataProfiler) -> List[str]:
        """ìˆ˜ì¹˜í˜• ë°ì´í„° ì¶”ì²œ"""
        recs = []
        
        # ë¶„ì‚° ê²€ì‚¬
        if profile.cv and profile.cv < self.thresholds['cv_low']:
            recs.append('consider_low_variance_removal')
        
        # ì´ìƒì¹˜ ì²˜ë¦¬
        if profile.outlier_ratio and profile.outlier_ratio > self.thresholds['outlier_ratio_threshold']:
            recs.append('outlier_detection_treatment')
        
        # ë¶„í¬ íŠ¹ì„± ê¸°ë°˜ ì¶”ì²œ
        if profile.skewness and profile.kurtosis:
            if abs(profile.skewness) > self.thresholds['skewness_high'] or profile.kurtosis > self.thresholds['kurtosis_high']:
                recs.append('distribution_transformation_log_boxcox')
            elif abs(profile.skewness) > self.thresholds['skewness_moderate']:
                recs.append('robust_scaling_iqr')
        
        # ìŠ¤ì¼€ì¼ë§ ì¶”ì²œ
        if profile.cv and profile.cv > self.thresholds['cv_high']:
            recs.append('standardization_zscore')
        else:
            recs.append('normalization_minmax')
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì²œ
        if self.context == DataContext.HIGH_DIMENSIONAL:
            recs.append('dimensionality_reduction_pca')
        elif self.context == DataContext.SPARSE:
            recs.append('sparse_feature_selection')
        
        return recs
    
    def _get_categorical_recommendations(self, profile: DataProfiler) -> List[str]:
        """ë²”ì£¼í˜• ë°ì´í„° ì¶”ì²œ"""
        recs = []
        
        # ì¸ì½”ë”© ì¶”ì²œ
        if profile.unique_count <= 10:
            recs.append('one_hot_encoding')
        elif profile.unique_count <= 50:
            if profile.data_type == DataType.CATEGORICAL_ORDINAL:
                recs.append('ordinal_encoding')
            else:
                recs.append('label_encoding')
        elif profile.unique_count <= 1000:
            recs.append('target_encoding')
        else:
            recs.append('hashing_encoding')
        
        # ê³ ì¹´ë””ë„ë¦¬í‹° ì²˜ë¦¬
        if profile.unique_count > 100:
            recs.append('high_cardinality_feature_selection')
        
        # í¬ì†Œ ë²”ì£¼ ì²˜ë¦¬
        if profile.unique_ratio < self.thresholds['unique_ratio_low']:
            recs.append('rare_category_grouping')
        
        return recs
    
    def _get_text_recommendations(self, profile: DataProfiler) -> List[str]:
        """í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ì²œ"""
        recs = []
        
        if profile.text_length_stats:
            avg_length = profile.text_length_stats['mean_length']
            
            if avg_length < 20:
                recs.extend(['text_preprocessing_basic', 'bag_of_words'])
            elif avg_length < 100:
                recs.extend(['text_preprocessing_advanced', 'tfidf_vectorization'])
            else:
                recs.extend(['text_preprocessing_advanced', 'text_embeddings', 'topic_modeling'])
        
        recs.append('text_feature_extraction')
        return recs
    
    def _get_datetime_recommendations(self, profile: DataProfiler) -> List[str]:
        """ë‚ ì§œ/ì‹œê°„ ë°ì´í„° ì¶”ì²œ"""
        recs = []
        
        recs.extend([
            'datetime_feature_extraction',
            'time_based_features',
            'cyclical_encoding'
        ])
        
        if self.context == DataContext.TIMESERIES:
            recs.extend([
                'time_series_decomposition',
                'lag_features',
                'rolling_statistics'
            ])
        
        return recs
    
    def _calculate_priority(self, profile: DataProfiler) -> str:
        """ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        if profile.is_constant or profile.missing_ratio > self.thresholds['missing_high']:
            return 'high'
        elif profile.missing_ratio > self.thresholds['missing_medium']:
            return 'medium'
        else:
            return 'low'
    
    def _generate_global_recommendations(self, df: pd.DataFrame) -> List[str]:
        """ì „ì—­ ì¶”ì²œì‚¬í•­"""
        recs = []
        
        # ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr().abs()
            high_corr_pairs = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    if corr_matrix.iloc[i, j] > self.thresholds['correlation_very_high']:
                        high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))
            
            if high_corr_pairs:
                recs.append('multicollinearity_detection_vif')
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì „ì—­ ì¶”ì²œ
        if self.context == DataContext.IMBALANCED:
            recs.extend(['class_balancing_smote', 'stratified_sampling'])
        elif self.context == DataContext.HIGH_DIMENSIONAL:
            recs.extend(['feature_selection_univariate', 'regularization_techniques'])
        elif self.context == DataContext.SPARSE:
            recs.extend(['sparse_matrix_optimization', 'feature_hashing'])
        
        return recs

class EnhancedVisualizationRecommender:
    """
    ê³ ë„í™”ëœ ì‹œê°í™” ì¶”ì²œ ì‹œìŠ¤í…œ
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â–¸ ë¶„ì„ ëª©ì  ê¸°ë°˜ ì¶”ì²œ
    â–¸ ë‹¤ë³€ëŸ‰ ì‹œê°í™” ì§€ì›
    â–¸ ìƒí˜¸ì‘ìš© ì‹œê°í™”
    â–¸ ë°ì´í„° í’ˆì§ˆ ì‹œê°í™”
    """
    
    def __init__(self, analysis_purpose: str = 'exploratory'):
        self.analysis_purpose = analysis_purpose  # 'exploratory', 'confirmatory', 'presentation'
        self.data_profiles: Dict[str, DataProfiler] = {}
        self.context: DataContext = DataContext.GENERAL
    
    def recommend(self, df: pd.DataFrame, data_profiles: Dict[str, DataProfiler] = None, 
                 context: DataContext = DataContext.GENERAL) -> Dict[str, Dict[str, Any]]:
        """ì‹œê°í™” ì¶”ì²œ"""
        
        if data_profiles:
            self.data_profiles = data_profiles
        else:
            # ê°„ë‹¨í•œ í”„ë¡œíŒŒì¼ë§
            preprocessor = EnhancedPreprocessingRecommender()
            preprocessor.profile_data(df)
            self.data_profiles = preprocessor.profiles
        
        self.context = context
        
        recommendations = {}
        
        # ë‹¨ë³€ëŸ‰ ì‹œê°í™”
        for col, profile in self.data_profiles.items():
            if col.startswith('_'):
                continue
            
            col_viz = self._get_univariate_visualizations(profile)
            recommendations[col] = {
                'type': 'univariate',
                'visualizations': col_viz,
                'priority': self._calculate_viz_priority(profile)
            }
        
        # ì´ë³€ëŸ‰ ì‹œê°í™”
        bivariate_viz = self._get_bivariate_visualizations(df)
        if bivariate_viz:
            recommendations['_bivariate'] = {
                'type': 'bivariate',
                'visualizations': bivariate_viz,
                'priority': 'medium'
            }
        
        # ë‹¤ë³€ëŸ‰ ì‹œê°í™”
        multivariate_viz = self._get_multivariate_visualizations(df)
        if multivariate_viz:
            recommendations['_multivariate'] = {
                'type': 'multivariate',
                'visualizations': multivariate_viz,
                'priority': 'high'
            }
        
        # ë°ì´í„° í’ˆì§ˆ ì‹œê°í™”
        quality_viz = self._get_data_quality_visualizations(df)
        if quality_viz:
            recommendations['_data_quality'] = {
                'type': 'data_quality',
                'visualizations': quality_viz,
                'priority': 'high'
            }
        
        return recommendations
    
    def _get_univariate_visualizations(self, profile: DataProfiler) -> List[str]:
        """ë‹¨ë³€ëŸ‰ ì‹œê°í™” ì¶”ì²œ"""
        viz_list = []
        
        if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:
            viz_list.extend(['histogram', 'box_plot', 'violin_plot'])
            
            if profile.skewness and abs(profile.skewness) > 1:
                viz_list.append('qq_plot')
            
            if profile.outlier_ratio and profile.outlier_ratio > 0.05:
                viz_list.append('outlier_detection_plot')
        
        elif profile.data_type in [DataType.CATEGORICAL_NOMINAL, DataType.CATEGORICAL_ORDINAL]:
            viz_list.extend(['bar_chart', 'horizontal_bar_chart'])
            
            if profile.unique_count <= 10:
                viz_list.append('pie_chart')
            
            if profile.unique_count > 20:
                viz_list.append('pareto_chart')
        
        elif profile.data_type == DataType.TEXT:
            viz_list.extend(['word_cloud', 'text_length_distribution', 'n_gram_frequency'])
        
        elif profile.data_type == DataType.DATETIME:
            viz_list.extend(['time_series_plot', 'temporal_distribution'])
            
            if self.context == DataContext.TIMESERIES:
                viz_list.extend(['seasonal_decomposition', 'autocorrelation_plot'])
        
        return viz_list
    
    def _get_bivariate_visualizations(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ì´ë³€ëŸ‰ ì‹œê°í™” ì¶”ì²œ"""
        viz_list = []
        
        numeric_cols = [col for col, profile in self.data_profiles.items() 
                       if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]]
        
        categorical_cols = [col for col, profile in self.data_profiles.items() 
                          if profile.data_type in [DataType.CATEGORICAL_NOMINAL, DataType.CATEGORICAL_ORDINAL]]
        
        # ìˆ˜ì¹˜í˜• vs ìˆ˜ì¹˜í˜•
        if len(numeric_cols) >= 2:
            for i in range(len(numeric_cols)):
                for j in range(i+1, len(numeric_cols)):
                    col1, col2 = numeric_cols[i], numeric_cols[j]
                    
                    # ìƒê´€ê´€ê³„ ê³„ì‚°
                    corr_val = df[col1].corr(df[col2])
                    
                    viz_recommendations = ['scatter_plot']
                    
                    if abs(corr_val) > 0.7:
                        viz_recommendations.extend(['regression_line', 'correlation_heatmap'])
                    
                    if abs(corr_val) > 0.3:
                        viz_recommendations.append('joint_plot')
                    
                    viz_list.append({
                        'variables': [col1, col2],
                        'relationship': 'numeric_numeric',
                        'correlation': corr_val,
                        'visualizations': viz_recommendations
                    })
        
        # ë²”ì£¼í˜• vs ìˆ˜ì¹˜í˜•
        for cat_col in categorical_cols:
            for num_col in numeric_cols:
                viz_recommendations = ['grouped_box_plot', 'violin_plot_grouped']
                
                if self.data_profiles[cat_col].unique_count <= 10:
                    viz_recommendations.append('strip_plot')
                
                viz_list.append({
                    'variables': [cat_col, num_col],
                    'relationship': 'categorical_numeric',
                    'visualizations': viz_recommendations
                })
        
        # ë²”ì£¼í˜• vs ë²”ì£¼í˜•
        if len(categorical_cols) >= 2:
            for i in range(len(categorical_cols)):
                for j in range(i+1, len(categorical_cols)):
                    col1, col2 = categorical_cols[i], categorical_cols[j]
                    
                    viz_recommendations = ['contingency_table_heatmap', 'grouped_bar_chart']
                    
                    if (self.data_profiles[col1].unique_count <= 10 and 
                        self.data_profiles[col2].unique_count <= 10):
                        viz_recommendations.append('mosaic_plot')
                    
                    viz_list.append({
                        'variables': [col1, col2],
                        'relationship': 'categorical_categorical',
                        'visualizations': viz_recommendations
                    })
        
        return viz_list[:10]  # ìµœëŒ€ 10ê°œ ì¶”ì²œ
    
    def _get_multivariate_visualizations(self, df: pd.DataFrame) -> List[str]:
        """ë‹¤ë³€ëŸ‰ ì‹œê°í™” ì¶”ì²œ"""
        viz_list = []
        
        numeric_cols = [col for col, profile in self.data_profiles.items() 
                       if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]]
        
        if len(numeric_cols) >= 3:
            viz_list.extend(['correlation_matrix', 'pair_plot', 'parallel_coordinates'])
            
            if len(numeric_cols) >= 4:
                viz_list.append('3d_scatter_plot')
            
            if len(numeric_cols) >= 5:
                viz_list.extend(['pca_biplot', 'radar_chart'])
        
        if self.context == DataContext.HIGH_DIMENSIONAL:
            viz_list.extend(['dimensionality_reduction_plot', 'feature_importance_plot'])
        
        return viz_list
    
    def _get_data_quality_visualizations(self, df: pd.DataFrame) -> List[str]:
        """ë°ì´í„° í’ˆì§ˆ ì‹œê°í™” ì¶”ì²œ"""
        viz_list = []
        
        # ê²°ì¸¡ê°’ ì‹œê°í™”
        total_missing = df.isnull().sum().sum()
        if total_missing > 0:
            viz_list.extend(['missing_value_matrix', 'missing_value_heatmap'])
        
        # ì¤‘ë³µê°’ ì‹œê°í™”
        duplicate_count = df.duplicated().sum()
        if duplicate_count > 0:
            viz_list.append('duplicate_detection_plot')
        
        # ì´ìƒì¹˜ ì‹œê°í™”
        numeric_cols = [col for col, profile in self.data_profiles.items() 
                       if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]]
        
        if numeric_cols:
            viz_list.extend(['outlier_detection_plot', 'data_distribution_overview'])
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦ ì‹œê°í™”
        viz_list.append('data_type_validation_plot')
        
        return viz_list
    
    def _calculate_viz_priority(self, profile: DataProfiler) -> str:
        """ì‹œê°í™” ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        if profile.missing_ratio > 0.3 or profile.is_constant:
            return 'high'
        elif profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:
            return 'medium'
        else:
            return 'low'

class EnhancedRecommendationEngine:
    """
    ê³ ë„í™”ëœ ì¶”ì²œ ì—”ì§„
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    â–¸ í†µí•© ì¶”ì²œ ì‹œìŠ¤í…œ
    â–¸ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ì •ë ¬
    â–¸ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ ìƒì„±
    """
    
    def __init__(self, analysis_purpose: str = 'exploratory'):
        self.preprocessor = EnhancedPreprocessingRecommender()
        self.visualizer = EnhancedVisualizationRecommender(analysis_purpose)
        self.analysis_purpose = analysis_purpose
    
    def run(self, df: pd.DataFrame, target_column: str = None) -> Dict[str, Any]:
        """í†µí•© ì¶”ì²œ ì‹¤í–‰"""
        
        # ë°ì´í„° ê²€ì¦
        if df.empty:
            raise ValueError("ì…ë ¥ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # ì „ì²˜ë¦¬ ì¶”ì²œ
        logger.info("ì „ì²˜ë¦¬ ì¶”ì²œ ì‹œì‘...")
        preprocessing_recs = self.preprocessor.recommend(df)
        
        # ì‹œê°í™” ì¶”ì²œ
        logger.info("ì‹œê°í™” ì¶”ì²œ ì‹œì‘...")
        visualization_recs = self.visualizer.recommend(
            df, 
            self.preprocessor.profiles, 
            self.preprocessor.context
        )
        
        # ê²°ê³¼ í†µí•©
        result = {
            'data_info': {
                'shape': df.shape,
                'columns': list(df.columns),
                'dtypes': df.dtypes.to_dict(),
                'missing_values': df.isnull().sum().to_dict(),
                'context': self.preprocessor.context.value
            },
            'preprocessing': preprocessing_recs,
            'visualization': visualization_recs,
            'summary': self._generate_summary(preprocessing_recs, visualization_recs),
            'code_templates': self._generate_code_templates(preprocessing_recs, visualization_recs)
        }
        
        return result
    
    def _generate_summary(self, preprocessing_recs: Dict, visualization_recs: Dict) -> Dict[str, Any]:
        """ì¶”ì²œ ìš”ì•½ ìƒì„±"""
        
        # ìš°ì„ ìˆœìœ„ë³„ ì „ì²˜ë¦¬ ì¶”ì²œ ì§‘ê³„
        priority_counts = {'high': 0, 'medium': 0, 'low': 0}
        for col, rec in preprocessing_recs.items():
            if col.startswith('_'):
                continue
            priority = rec.get('priority', 'low')
            priority_counts[priority] += 1
        
        # ì£¼ìš” ì¶”ì²œì‚¬í•­ ì¶”ì¶œ
        high_priority_recs = []
        for col, rec in preprocessing_recs.items():
            if rec.get('priority') == 'high':
                high_priority_recs.append(f"{col}: {rec['recommendations'][:2]}")
        
        return {
            'total_columns': len([col for col in preprocessing_recs.keys() if not col.startswith('_')]),
            'priority_distribution': priority_counts,
            'high_priority_recommendations': high_priority_recs[:5],
            'recommended_visualizations': len(visualization_recs),
            'analysis_context': self.preprocessor.context.value
        }
    
    def _generate_code_templates(self, preprocessing_recs: Dict, visualization_recs: Dict) -> Dict[str, str]:
        """ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ í…œí”Œë¦¿ ìƒì„±"""
        
        templates = {}
        
        # ì „ì²˜ë¦¬ ì½”ë“œ í…œí”Œë¦¿
        preprocessing_code = self._generate_preprocessing_code(preprocessing_recs)
        templates['preprocessing'] = preprocessing_code
        
        # ì‹œê°í™” ì½”ë“œ í…œí”Œë¦¿
        visualization_code = self._generate_visualization_code(visualization_recs)
        templates['visualization'] = visualization_code
        
        return templates
    
    def _generate_preprocessing_code(self, recommendations: Dict) -> str:
        """ì „ì²˜ë¦¬ ì½”ë“œ ìƒì„±"""
        code_lines = [
            "import pandas as pd",
            "import numpy as np",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder",
            "from sklearn.impute import SimpleImputer, KNNImputer",
            "",
            "# ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸",
            "def preprocess_data(df):",
            "    df_processed = df.copy()",
            ""
        ]
        
        for col, rec in recommendations.items():
            if col.startswith('_'):
                continue
            
            for recommendation in rec['recommendations'][:3]:  # ìƒìœ„ 3ê°œ ì¶”ì²œë§Œ
                if 'drop' in recommendation:
                    code_lines.append(f"    # {col}: {recommendation}")
                    code_lines.append(f"    df_processed = df_processed.drop('{col}', axis=1)")
                elif 'impute' in recommendation:
                    code_lines.append(f"    # {col}: {recommendation}")
                    code_lines.append(f"    imputer = SimpleImputer(strategy='median')")
                    code_lines.append(f"    df_processed['{col}'] = imputer.fit_transform(df_processed[['{col}']])")
                elif 'standardization' in recommendation:
                    code_lines.append(f"    # {col}: {recommendation}")
                    code_lines.append(f"    scaler = StandardScaler()")
                    code_lines.append(f"    df_processed['{col}'] = scaler.fit_transform(df_processed[['{col}']])")
        
        code_lines.extend([
            "",
            "    return df_processed",
            "",
            "# ì‚¬ìš© ì˜ˆì‹œ:",
            "# processed_df = preprocess_data(df)"
        ])
        
        return "\n".join(code_lines)
    
    def _generate_visualization_code(self, recommendations: Dict) -> str:
        """ì‹œê°í™” ì½”ë“œ ìƒì„±"""
        code_lines = [
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import plotly.express as px",
            "",
            "# ë°ì´í„° ì‹œê°í™” í•¨ìˆ˜ë“¤",
            "def create_visualizations(df):",
            "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))",
            "    axes = axes.flatten()",
            ""
        ]
        
        plot_idx = 0
        for col, rec in recommendations.items():
            if col.startswith('_') or plot_idx >= 4:
                continue
            
            viz_type = rec['visualizations'][0] if rec['visualizations'] else 'histogram'
            
            if viz_type == 'histogram':
                code_lines.append(f"    # {col} íˆìŠ¤í† ê·¸ë¨")
                code_lines.append(f"    axes[{plot_idx}].hist(df['{col}'].dropna(), bins=30)")
                code_lines.append(f"    axes[{plot_idx}].set_title('{col} Distribution')")
            elif viz_type == 'bar_chart':
                code_lines.append(f"    # {col} ë§‰ëŒ€ ì°¨íŠ¸")
                code_lines.append(f"    df['{col}'].value_counts().plot(kind='bar', ax=axes[{plot_idx}])")
                code_lines.append(f"    axes[{plot_idx}].set_title('{col} Value Counts')")
            
            plot_idx += 1
        
        code_lines.extend([
            "",
            "    plt.tight_layout()",
            "    plt.show()",
            "",
            "# ì‚¬ìš© ì˜ˆì‹œ:",
            "# create_visualizations(df)"
        ])
        
        return "\n".join(code_lines)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    sample_data = pd.DataFrame({
        'id': range(1000),
        'age': np.random.normal(35, 10, 1000),
        'income': np.random.exponential(50000, 1000),
        'category': np.random.choice(['A', 'B', 'C'], 1000),
        'text_field': ['sample text ' * np.random.randint(1, 10) for _ in range(1000)],
        'binary_field': np.random.choice([0, 1], 1000)
    })
    
    # ê²°ì¸¡ê°’ ì¶”ê°€
    sample_data.loc[np.random.choice(1000, 100), 'age'] = np.nan
    sample_data.loc[np.random.choice(1000, 50), 'income'] = np.nan
    
    # ì¶”ì²œ ì—”ì§„ ì‹¤í–‰
    engine = EnhancedRecommendationEngine(analysis_purpose='exploratory')
    recommendations = engine.run(sample_data)
    
    # ê²°ê³¼ ì¶œë ¥
    print("=" * 60)
    print("ğŸ“Š ê³ ë„í™”ëœ ë°ì´í„° ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œ")
    print("=" * 60)
    print(f"ë°ì´í„° í˜•íƒœ: {recommendations['data_info']['shape']}")
    print(f"ë¶„ì„ ì»¨í…ìŠ¤íŠ¸: {recommendations['data_info']['context']}")
    print("\nì „ì²˜ë¦¬ ì¶”ì²œ:")
    for col, rec in recommendations['preprocessing'].items():
        if not col.startswith('_'):
            print(f"  {col}: {rec['recommendations'][:2]}")
    
    print(f"\nì½”ë“œ í…œí”Œë¦¿ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.") 