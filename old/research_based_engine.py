import pandas as pd
import numpy as np
import yaml
from dataclasses import dataclass, field
from typing import Any, Dict, List, Tuple, Optional
import logging
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import IsolationForest
import warnings
import re
from datetime import datetime
warnings.filterwarnings('ignore')

logger = logging.getLogger(__name__)

@dataclass
class ResearchRuleSet:
    """ì—°êµ¬ ê¸°ë°˜ ê·œì¹™ ì§‘í•©"""
    numeric: Dict[str, Any] = field(default_factory=dict)
    categorical: Dict[str, Any] = field(default_factory=dict)
    datetime: Dict[str, Any] = field(default_factory=dict)
    data_quality: Dict[str, Any] = field(default_factory=dict)
    feature_selection: Dict[str, Any] = field(default_factory=dict)

    @classmethod
    def load(cls, path: str = "research_based_rules.yaml") -> "ResearchRuleSet":
        with open(path, encoding="utf-8") as f:
            raw = yaml.safe_load(f)
        return cls(**raw)

class ResearchBasedPreprocessingRecommender:
    """ì—°êµ¬ ê¸°ë°˜ ì „ì²˜ë¦¬ ì¶”ì²œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, rules: ResearchRuleSet):
        self.rules = rules
    
    def _is_datetime_column(self, s: pd.Series, col_name: str) -> bool:
        """ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ì¸ì§€ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜"""
        
        # 1. ì»¬ëŸ¼ëª… ê¸°ë°˜ íŒë‹¨
        col_lower = col_name.lower()
        date_keywords = ['date', 'time', 'dt', 'datetime', 'timestamp', 'created', 'updated']
        if any(keyword in col_lower for keyword in date_keywords):
            return True
        
        # 2. ë°ì´í„° íŒ¨í„´ ê¸°ë°˜ íŒë‹¨
        if s.dtype == 'object':
            # ìƒ˜í”Œ ë°ì´í„°ë¡œ ë‚ ì§œ íŒ¨í„´ í…ŒìŠ¤íŠ¸
            sample_data = s.dropna().head(100)
            if len(sample_data) == 0:
                return False
            
            # ë‹¤ì–‘í•œ ë‚ ì§œ íŒ¨í„´ ì •ì˜
            date_patterns = [
                r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
                r'\d{2}/\d{2}/\d{4}',  # MM/DD/YYYY
                r'\d{2}-\d{2}-\d{4}',  # MM-DD-YYYY
                r'\d{4}/\d{2}/\d{2}',  # YYYY/MM/DD
                r'\d{2}/\d{2}/\d{2}',  # MM/DD/YY
                r'\d{8}',              # YYYYMMDD
                r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}',  # YYYY-MM-DD HH:MM:SS
                r'\d{2}:\d{2}:\d{2}',  # HH:MM:SS
            ]
            
            # íŒ¨í„´ ë§¤ì¹­ í…ŒìŠ¤íŠ¸
            pattern_matches = 0
            total_tested = 0
            
            for value in sample_data:
                value_str = str(value).strip()
                if value_str and value_str != 'nan':
                    total_tested += 1
                    for pattern in date_patterns:
                        if re.match(pattern, value_str):
                            pattern_matches += 1
                            break
            
            # 70% ì´ìƒì´ ë‚ ì§œ íŒ¨í„´ì´ë©´ ë‚ ì§œ ì»¬ëŸ¼ìœ¼ë¡œ íŒë‹¨
            if total_tested > 0 and pattern_matches / total_tested > 0.7:
                return True
            
            # 3. ì‹¤ì œ ë‚ ì§œ ë³€í™˜ í…ŒìŠ¤íŠ¸
            try:
                pd.to_datetime(sample_data, errors='raise')
                return True
            except (ValueError, TypeError):
                pass
        
        return False
    
    def _extract_datetime_features(self, s: pd.Series) -> Dict[str, Any]:
        """ë‚ ì§œ/ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ"""
        try:
            # object íƒ€ì…ì„ datetimeìœ¼ë¡œ ë³€í™˜
            datetime_series = pd.to_datetime(s, errors='coerce')
            valid_dates = datetime_series.dropna()
            
            if len(valid_dates) == 0:
                return {}
            
            # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
            date_range = valid_dates.max() - valid_dates.min()
            
            # ì‹œê°„ ë‹¨ìœ„ë³„ ê³ ìœ ê°’ ê°œìˆ˜
            unique_years = valid_dates.dt.year.nunique()
            unique_months = valid_dates.dt.month.nunique()
            unique_days = valid_dates.dt.day.nunique()
            unique_hours = valid_dates.dt.hour.nunique() if hasattr(valid_dates.dt, 'hour') else 0
            
            return {
                'date_range_days': date_range.days,
                'unique_years': unique_years,
                'unique_months': unique_months,
                'unique_days': unique_days,
                'unique_hours': unique_hours,
                'is_timeseries': date_range.days > 30,  # 30ì¼ ì´ìƒì´ë©´ ì‹œê³„ì—´ë¡œ íŒë‹¨
                'has_time_component': unique_hours > 1,  # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ”ì§€
                'conversion_success_rate': len(valid_dates) / len(s)
            }
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_distribution(self, s: pd.Series) -> Dict[str, float]:
        """ë¶„í¬ íŠ¹ì„± ë¶„ì„ - ì—°êµ¬ ê¸°ë°˜ ë©”íŠ¸ë¦­"""
        skew = s.skew()
        kurt = s.kurtosis()
        mean = s.mean()
        std = s.std()
        cv = std / mean if mean != 0 else np.inf
        
        # ì´ìƒì¹˜ ë¹„ìœ¨ ê³„ì‚° (IQR ë°©ë²•)
        Q1, Q3 = s.quantile([0.25, 0.75])
        IQR = Q3 - Q1
        outlier_mask = (s < Q1 - 1.5 * IQR) | (s > Q3 + 1.5 * IQR)
        outlier_ratio = outlier_mask.mean()
        
        return {
            'skew': skew,
            'kurt': kurt,
            'cv': cv,
            'outlier_ratio': outlier_ratio,
            'unique_count': s.nunique(),
            'unique_ratio': s.nunique() / len(s)
        }
    
    def _apply_datetime_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:
        """ë‚ ì§œ/ì‹œê°„ ë°ì´í„° ê·œì¹™ ì ìš©"""
        rec: List[Tuple[str, str]] = []
        
        # ë‚ ì§œ íŠ¹ì„± ì¶”ì¶œ
        datetime_features = self._extract_datetime_features(s)
        
        if not datetime_features:
            return rec
        
        # ê¸°ë³¸ ë‚ ì§œ íŠ¹ì„± ì¶”ì¶œ
        rec.append(("datetime_extraction", "extract_date_time_features"))
        
        # ì‹œê³„ì—´ ë°ì´í„°ì¸ ê²½ìš°
        if datetime_features.get('is_timeseries', False):
            rec.extend([
                ("cyclical_encoding", "temporal_cyclical_features"),
                ("lag_features", "time_series_lag_features"),
                ("rolling_statistics", "time_series_rolling_features")
            ])
        
        # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
        if datetime_features.get('has_time_component', False):
            rec.append(("time_based_features", "extract_time_components"))
        
        # ë‚ ì§œ ë²”ìœ„ê°€ ë„“ì€ ê²½ìš°
        if datetime_features.get('date_range_days', 0) > 365:
            rec.append(("seasonal_decomposition", "long_term_temporal_patterns"))
        
        # ë³€í™˜ ì„±ê³µë¥ ì´ ë‚®ì€ ê²½ìš°
        if datetime_features.get('conversion_success_rate', 1.0) < 0.8:
            rec.append(("datetime_cleaning", "inconsistent_date_formats"))
        
        return rec
    
    def _apply_numeric_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:
        """ì—°êµ¬ ê¸°ë°˜ ìˆ˜ì¹˜í˜• ê·œì¹™ ì ìš©"""
        rec: List[Tuple[str, str]] = []
        r = self.rules.numeric
        
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        miss = s.isna().mean()
        stats_dict = self._analyze_distribution(s)
        
        # 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ì„¸ë¶„í™”ëœ ì„ê³„ê°’
        for cond in r["missing_ratio"]:
            if miss > cond.get("gt", -np.inf):
                rec.append((cond["action"], cond["why"]))
                break
        
        # 2. ë¶„í¬ ë¶„ì„ - ì—°êµ¬ ê¸°ë°˜ ì„ê³„ê°’
        for cond in r["distribution_analysis"]:
            skew_threshold = cond.get("skew", 0)
            kurt_threshold = cond.get("kurt", 1e9)
            
            if (abs(stats_dict['skew']) > skew_threshold or 
                stats_dict['kurt'] > kurt_threshold):
                rec.append((cond["action"], cond["why"]))
                break
        
        # 3. ë³€ë™ì„± ë¶„ì„
        for cond in r["variance_analysis"]:
            if stats_dict['cv'] < cond.get("cv_lt", -np.inf):
                rec.append((cond["action"], cond["why"]))
                break
        
        # 4. ì´ìƒì¹˜ íƒì§€ - ë‹¤ì¤‘ ë°©ë²•ë¡ 
        if "outlier_detection" in r:
            for cond in r["outlier_detection"]:
                if "dimensions_gt" in cond:
                    # ë‹¤ë³€ëŸ‰ ì´ìƒì¹˜ (ê³ ì°¨ì› ë°ì´í„°)
                    if len(s) > cond["dimensions_gt"]:
                        rec.append((cond["action"], cond["why"]))
                        break
                else:
                    # ë‹¨ë³€ëŸ‰ ì´ìƒì¹˜
                    rec.append((cond["action"], cond["why"]))
                    break
        
        # 5. ìŠ¤ì¼€ì¼ë§ ì „ëµ - ë³µí•© ì¡°ê±´
        if "scaling_strategy" in r:
            for cond in r["scaling_strategy"]:
                cv_condition = cond.get("cv_gt", 0)
                outlier_condition = cond.get("outlier_ratio_gt", 0)
                
                if (stats_dict['cv'] > cv_condition and 
                    stats_dict['outlier_ratio'] > outlier_condition):
                    rec.append((cond["action"], cond["why"]))
                    break
            else:
                # ê¸°ë³¸ ìŠ¤ì¼€ì¼ë§
                if r["scaling_strategy"]:
                    default_scaling = r["scaling_strategy"][-1]
                    rec.append((default_scaling["action"], default_scaling["why"]))
        
        # 6. ë„ë©”ì¸ íŠ¹í™” ê·œì¹™
        if "domain_specific" in r:
            for cond in r["domain_specific"]:
                if "name_contains" in cond:
                    if cond["name_contains"].lower() in col.lower():
                        # ì¶”ê°€ ì¡°ê±´ ì²´í¬
                        if "skew_lt" in cond and stats_dict['skew'] < cond["skew_lt"]:
                            rec.append((cond["action"], cond["why"]))
                        elif "unique_lt" in cond and stats_dict['unique_count'] < cond["unique_lt"]:
                            rec.append((cond["action"], cond["why"]))
                        elif "unique_ratio_gt" in cond and stats_dict['unique_ratio'] > cond["unique_ratio_gt"]:
                            rec.append((cond["action"], cond["why"]))
                        else:
                            rec.append((cond["action"], cond["why"]))
        
        return rec
    
    def _apply_categorical_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:
        """ì—°êµ¬ ê¸°ë°˜ ë²”ì£¼í˜• ê·œì¹™ ì ìš©"""
        rec: List[Tuple[str, str]] = []
        r = self.rules.categorical
        
        # ë‚ ì§œ ë°ì´í„°ì¸ì§€ ë¨¼ì € í™•ì¸
        if self._is_datetime_column(s, col):
            return self._apply_datetime_rules(col, s)
        
        miss = s.isna().mean()
        unique = s.nunique(dropna=True)
        
        # 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        for cond in r["missing_ratio"]:
            if miss > cond.get("gt", -np.inf):
                rec.append((cond["action"], cond["why"]))
                break
        
        # 2. ì¹´ë””ë„ë¦¬í‹° ì „ëµ - ì—°êµ¬ ê¸°ë°˜ ì„ê³„ê°’
        for cond in r["cardinality_strategy"]:
            if ("lte" in cond and unique <= cond["lte"]) or ("gt" in cond and unique > cond["gt"]):
                rec.append((cond["action"], cond["why"]))
                break
        
        # 3. í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê·œì¹™
        if "text_processing" in r:
            for cond in r["text_processing"]:
                if "name_contains" in cond and cond["name_contains"].lower() in col.lower():
                    rec.append((cond["action"], cond["why"]))
        
        # 4. ìˆœì„œí˜• ë°ì´í„° ë¶„ì„
        if "ordinal_analysis" in r:
            unique_values = set(s.dropna().astype(str).str.lower())
            for cond in r["ordinal_analysis"]:
                if "values_contain" in cond:
                    if any(val in unique_values for val in cond["values_contain"]):
                        rec.append((cond["action"], cond["why"]))
        
        return rec
    
    def _apply_data_quality_rules(self, df: pd.DataFrame) -> List[Tuple[str, str]]:
        """ë°ì´í„° í’ˆì§ˆ ê·œì¹™ ì ìš©"""
        rec: List[Tuple[str, str]] = []
        
        if "data_quality" in self.rules.data_quality:
            # ì¤‘ë³µ ë°ì´í„° ê²€ì‚¬
            duplicate_ratio = df.duplicated().mean()
            if duplicate_ratio > 0.1:
                rec.append(("remove_duplicates", "high_duplicate_ratio"))
            
            # ì¼ê´€ì„± ê²€ì‚¬ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
            for col in df.columns:
                if df[col].dtype == 'object':
                    # ë¬¸ìì—´ ê¸¸ì´ ì¼ê´€ì„± ê²€ì‚¬
                    str_lengths = df[col].astype(str).str.len()
                    if str_lengths.std() / str_lengths.mean() > 0.5:
                        rec.append(("data_cleaning", "inconsistent_data_patterns"))
        
        return rec
    
    def _apply_feature_selection_rules(self, df: pd.DataFrame) -> List[Tuple[str, str]]:
        """íŠ¹ì„± ì„ íƒ ê·œì¹™ ì ìš©"""
        rec: List[Tuple[str, str]] = []
        
        if "feature_selection" in self.rules.feature_selection:
            numeric_df = df.select_dtypes(include=[np.number])
            
            if len(numeric_df.columns) > 1:
                # ë‹¤ì¤‘ê³µì„ ì„± ê²€ì‚¬
                corr_matrix = numeric_df.corr().abs()
                high_corr_pairs = []
                for i in range(len(corr_matrix.columns)):
                    for j in range(i+1, len(corr_matrix.columns)):
                        if corr_matrix.iloc[i, j] > 0.95:
                            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))
                
                if high_corr_pairs:
                    rec.append(("remove_collinear", "high_correlation_collinearity"))
        
        return rec
    
    def recommend(self, df: pd.DataFrame) -> Dict[str, List[Dict[str, str]]]:
        """ì—°êµ¬ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ"""
        out: Dict[str, List[Dict[str, str]]] = {}
        
        numeric_cols = df.select_dtypes(include="number").columns
        categorical_cols = df.select_dtypes(include=["object", "category", "string"]).columns
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì²˜ë¦¬
        for c in numeric_cols:
            rec = self._apply_numeric_rules(c, df[c].dropna())
            out[c] = [{"action": a, "why": w} for a, w in rec]
        
        # ë²”ì£¼í˜• ì»¬ëŸ¼ ì²˜ë¦¬ (ë‚ ì§œ ë°ì´í„° í¬í•¨)
        for c in categorical_cols:
            rec = self._apply_categorical_rules(c, df[c])
            out[c] = [{"action": a, "why": w} for a, w in rec]
        
        # ë°ì´í„° í’ˆì§ˆ ê·œì¹™
        quality_rec = self._apply_data_quality_rules(df)
        if quality_rec:
            out["_data_quality"] = [{"action": a, "why": w} for a, w in quality_rec]
        
        # íŠ¹ì„± ì„ íƒ ê·œì¹™
        selection_rec = self._apply_feature_selection_rules(df)
        if selection_rec:
            out["_feature_selection"] = [{"action": a, "why": w} for a, w in selection_rec]
        
        return out

class ResearchBasedRecommendationEngine:
    """ì—°êµ¬ ê¸°ë°˜ ì¶”ì²œ ì—”ì§„"""
    
    def __init__(self, rule_path: str = "research_based_rules.yaml"):
        self.rules = ResearchRuleSet.load(rule_path)
        self.preproc = ResearchBasedPreprocessingRecommender(self.rules)
    
    def run(self, df: pd.DataFrame) -> Dict[str, Any]:
        return {
            "preprocessing": self.preproc.recommend(df),
        }

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_research_based_recommendations():
    """ì—°êµ¬ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("=" * 80)
    print("ğŸ”¬ ì—°êµ¬ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ì‹¤ì œ ë°ì´í„°ì™€ ìœ ì‚¬í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 1000
    
    # ì‹¤ì œ ë°ì´í„° íŠ¹ì„± ë°˜ì˜
    data = {
        'MIXA_PASTEUR_STATE': np.random.exponential(2, n_samples),  # ì™œë„ ~3
        'MIXB_PASTEUR_STATE': np.random.exponential(2, n_samples),  # ì™œë„ ~3
        'MIXA_PASTEUR_TEMP': -np.random.exponential(1, n_samples),  # ìŒìˆ˜ ì™œë„ ~-4
        'MIXB_PASTEUR_TEMP': -np.random.exponential(1, n_samples),  # ìŒìˆ˜ ì™œë„ ~-4
        'STD_DT': [f"2023-{i:02d}-{j:02d}" for i, j in zip(np.random.randint(1, 13, n_samples), np.random.randint(1, 29, n_samples))],
        'INSP': np.random.choice(['PASS', 'FAIL'], n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # ì—°êµ¬ ê¸°ë°˜ ì—”ì§„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    engine = ResearchBasedRecommendationEngine()
    recommendations = engine.run(df)
    
    print("\nğŸ“Š ë°ì´í„° íŠ¹ì„± ë¶„ì„:")
    for col in df.select_dtypes(include=[np.number]).columns:
        skew = df[col].skew()
        kurt = df[col].kurtosis()
        cv = df[col].std() / df[col].mean()
        print(f"  {col}: ì™œë„={skew:.2f}, ì²¨ë„={kurt:.2f}, CV={cv:.2f}")
    
    print("\nğŸ¯ ì—°êµ¬ ê¸°ë°˜ ì¶”ì²œ ê²°ê³¼:")
    for col, recs in recommendations['preprocessing'].items():
        if col.startswith('_'):
            print(f"\nğŸ”¹ {col}:")
        else:
            print(f"\nğŸ”¹ {col}:")
        for rec in recs:
            print(f"  âœ… {rec['action']}: {rec['why']}")
    
    return recommendations

if __name__ == "__main__":
    test_research_based_recommendations() 