import pandas as pd
import numpy as np
import yaml
from dataclasses import dataclass, field
from typing import Any, Dict, List, Tuple
import tempfile
import os

# YAML ê¸°ë°˜ ì‹œìŠ¤í…œ ë³µì œ (í…ŒìŠ¤íŠ¸ìš©)
@dataclass
class RuleSet:
    numeric: Dict[str, Any] = field(default_factory=dict)
    categorical: Dict[str, Any] = field(default_factory=dict)
    datetime: Dict[str, Any] = field(default_factory=dict)

    @classmethod
    def load(cls, path: str = "rules.yaml") -> "RuleSet":
        with open(path, encoding="utf-8") as f:
            raw = yaml.safe_load(f)
        return cls(**raw)

class PreprocessingRecommender:
    def __init__(self, rules: RuleSet):
        self.rules = rules

    def _apply_numeric_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:
        rec: List[Tuple[str, str]] = []
        r = self.rules.numeric

        # â‘  Missing
        miss = s.isna().mean()
        for cond in r["missing_ratio"]:
            if miss > cond.get("gt", -np.inf):
                rec.append((cond["action"], cond["why"]))
                break

        # â‘¡ Skew / Kurtosis
        skew, kurt = s.skew(), s.kurtosis()
        for cond in r["skew_kurt"]:
            if abs(skew) > cond.get("skew", 0) or kurt > cond.get("kurt", 1e9):
                rec.append((cond["action"], cond["why"]))
                break

        # â‘¢ Coefficient of Variation
        mean, std = s.mean(), s.std()
        cv = std / mean if mean and not np.isnan(mean) and mean != 0 else np.inf
        for cond in r["variance"]:
            if cv < cond.get("cv_lt", -np.inf):
                rec.append((cond["action"], cond["why"]))
                break

        # â‘£ Outlier Detection (í•­ìƒ ì²« ë²ˆì§¸ ë°©ë²• ì¶”ì²œ)
        if "outliers" in r and r["outliers"]:
            outlier_method = r["outliers"][0]
            rec.append((outlier_method["action"], outlier_method["why"]))

        # â‘¤ Scaling (CV ê¸°ë°˜)
        if "scaling" in r:
            for cond in r["scaling"]:
                if cv > cond.get("cv_gt", np.inf):
                    rec.append((cond["action"], cond["why"]))
                    break
            else:
                # ê¸°ë³¸ ìŠ¤ì¼€ì¼ë§
                if r["scaling"]:
                    default_scaling = r["scaling"][-1]
                    rec.append((default_scaling["action"], default_scaling["why"]))

        return rec

    def _apply_categorical_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:
        rec: List[Tuple[str, str]] = []
        r = self.rules.categorical

        miss = s.isna().mean()
        unique = s.nunique(dropna=True)

        for cond in r["missing_ratio"]:
            if miss > cond.get("gt", -np.inf):
                rec.append((cond["action"], cond["why"]))
                break

        for cond in r["cardinality"]:
            if (
                ("lte" in cond and unique <= cond["lte"])
                or ("gt" in cond and unique > cond["gt"])
            ):
                rec.append((cond["action"], cond["why"]))
                break
        return rec

    def recommend(self, df: pd.DataFrame) -> Dict[str, List[Dict[str, str]]]:
        out: Dict[str, List[Dict[str, str]]] = {}

        numeric_cols = df.select_dtypes(include="number").columns
        categorical_cols = df.select_dtypes(include=["object", "category", "string"]).columns

        # Numeric
        for c in numeric_cols:
            rec = self._apply_numeric_rules(c, df[c].dropna())
            out[c] = [{"action": a, "why": w} for a, w in rec]

        # Categorical
        for c in categorical_cols:
            rec = self._apply_categorical_rules(c, df[c])
            out[c] = [{"action": a, "why": w} for a, w in rec]

        return out

class RecommendationEngine:
    def __init__(self, rule_path: str = "rules.yaml"):
        self.rules = RuleSet.load(rule_path)
        self.preproc = PreprocessingRecommender(self.rules)

    def run(self, df: pd.DataFrame) -> Dict[str, Any]:
        return {
            "preprocessing": self.preproc.recommend(df),
        }

# í…ŒìŠ¤íŠ¸ìš© YAML ê·œì¹™ íŒŒì¼ ìƒì„±
def create_test_rules():
    rules = {
        'numeric': {
            'missing_ratio': [
                {'gt': 0.40, 'action': 'drop', 'why': 'missing_ratio_over_40'},
                {'gt': 0.15, 'action': 'advanced_impute', 'why': 'missing_ratio_15_40'},
                {'gt': 0.00, 'action': 'simple_impute', 'why': 'small_missing_values'}
            ],
            'skew_kurt': [
                {'skew': 3, 'kurt': 10, 'action': 'yeo_johnson_transform', 'why': 'extreme_skew_kurt'},
                {'skew': 2, 'kurt': 7, 'action': 'boxcox_transform', 'why': 'high_skew_kurt'},
                {'skew': 1.5, 'action': 'log_transform', 'why': 'moderate_high_skew'},
                {'skew': 1, 'action': 'sqrt_transform', 'why': 'moderate_skew'},
                {'kurt': 7, 'action': 'winsorize', 'why': 'high_kurtosis_only'}
            ],
            'variance': [
                {'cv_lt': 0.05, 'action': 'drop_constant', 'why': 'near_constant_values'},
                {'cv_lt': 0.1, 'action': 'feature_selection', 'why': 'low_variance'}
            ],
            'outliers': [
                {'action': 'isolation_forest', 'why': 'multivariate_outlier_detection'},
                {'action': 'iqr_method', 'why': 'univariate_outlier_detection'},
                {'action': 'z_score_method', 'why': 'normal_distribution_outliers'}
            ],
            'scaling': [
                {'cv_gt': 1.0, 'action': 'robust_scaler', 'why': 'high_variance_outliers'},
                {'cv_gt': 0.5, 'action': 'standard_scaler', 'why': 'moderate_variance'},
                {'action': 'min_max_scaler', 'why': 'bounded_scaling'}
            ]
        },
        'categorical': {
            'missing_ratio': [
                {'gt': 0.30, 'action': 'drop_column', 'why': 'missing_ratio_over_30'},
                {'gt': 0.10, 'action': 'impute_mode', 'why': 'missing_ratio_10_30'},
                {'gt': 0.05, 'action': 'impute_frequent', 'why': 'missing_ratio_5_10'}
            ],
            'cardinality': [
                {'lte': 2, 'action': 'binary_encode', 'why': 'binary_variable'},
                {'lte': 10, 'action': 'one_hot_encode', 'why': 'low_cardinality'},
                {'lte': 50, 'action': 'ordinal_encode', 'why': 'medium_cardinality'},
                {'lte': 1000, 'action': 'target_encode', 'why': 'high_cardinality'},
                {'gt': 1000, 'action': 'feature_hashing', 'why': 'very_high_cardinality'}
            ]
        }
    }
    return rules

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
def create_test_data():
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        # ìˆ˜ì¹˜í˜• ë°ì´í„° - ë‹¤ì–‘í•œ ë¶„í¬ì™€ ê²°ì¸¡ íŒ¨í„´
        'normal_col': np.random.normal(50, 15, n_samples),  # ì •ê·œë¶„í¬
        'skewed_col': np.random.exponential(2, n_samples),  # ì™œë„ ë†’ìŒ
        'high_missing': np.random.normal(100, 20, n_samples),  # ë†’ì€ ê²°ì¸¡ë¥ 
        'low_variance': np.ones(n_samples) * 10 + np.random.normal(0, 0.5, n_samples),  # ë‚®ì€ ë¶„ì‚°
        'very_skewed': np.random.pareto(0.5, n_samples),  # ë§¤ìš° ë†’ì€ ì™œë„
        
        # ë²”ì£¼í˜• ë°ì´í„° - ë‹¤ì–‘í•œ ì¹´ë””ë„ë¦¬í‹°
        'low_card': np.random.choice(['A', 'B', 'C'], n_samples),  # ì €ì¹´ë””ë„ë¦¬í‹°
        'medium_card': np.random.choice([f'Cat_{i}' for i in range(25)], n_samples),  # ì¤‘ê°„ ì¹´ë””ë„ë¦¬í‹°
        'high_card': np.random.choice([f'Item_{i}' for i in range(500)], n_samples),  # ê³ ì¹´ë””ë„ë¦¬í‹°
        'very_high_card': [f'ID_{i}' for i in range(n_samples)],  # ë§¤ìš° ê³ ì¹´ë””ë„ë¦¬í‹°
        'high_missing_cat': np.random.choice(['X', 'Y', 'Z'], n_samples),  # ë†’ì€ ê²°ì¸¡ë¥  ë²”ì£¼í˜•
    }
    
    df = pd.DataFrame(data)
    
    # ê²°ì¸¡ê°’ ì¶”ê°€
    df.loc[np.random.choice(n_samples, 500), 'high_missing'] = np.nan  # 50% ê²°ì¸¡
    df.loc[np.random.choice(n_samples, 400), 'high_missing_cat'] = np.nan  # 40% ê²°ì¸¡
    df.loc[np.random.choice(n_samples, 100), 'normal_col'] = np.nan  # 10% ê²°ì¸¡
    
    return df

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
def run_yaml_system_test():
    print("=" * 80)
    print("ğŸ§ª YAML ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ê·œì¹™ ìƒì„±
    df = create_test_data()
    rules = create_test_rules()
    
    # ì„ì‹œ YAML íŒŒì¼ ìƒì„±
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
        yaml.dump(rules, f, default_flow_style=False, allow_unicode=True)
        rules_file = f.name
    
    try:
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë³´:")
        print(f"   - ë°ì´í„° í˜•íƒœ: {df.shape}")
        print(f"   - ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(df.select_dtypes(include='number').columns)}ê°œ")
        print(f"   - ë²”ì£¼í˜• ì»¬ëŸ¼: {len(df.select_dtypes(include=['object']).columns)}ê°œ")
        print(f"   - ê²°ì¸¡ê°’ ìˆëŠ” ì»¬ëŸ¼: {df.isnull().any().sum()}ê°œ")
        
        # 2. ì¶”ì²œ ì—”ì§„ ì‹¤í–‰
        engine = RecommendationEngine(rules_file)
        recommendations = engine.run(df)
        
        print(f"\nğŸ”§ YAML ì¶”ì²œ ê²°ê³¼:")
        print(f"   - ì´ ì¶”ì²œëœ ì»¬ëŸ¼: {len(recommendations['preprocessing'])}ê°œ")
        
        # 3. ìƒì„¸ ì¶”ì²œ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒì„¸ ì¶”ì²œ ë‚´ìš©:")
        for col, recs in recommendations['preprocessing'].items():
            print(f"\nğŸ”¹ {col}:")
            
            # ì»¬ëŸ¼ íŠ¹ì„± ì •ë³´
            if col in df.select_dtypes(include='number').columns:
                series = df[col]
                missing_ratio = series.isnull().mean()
                if len(series.dropna()) > 0:
                    skew = series.skew()
                    print(f"   ğŸ“Š íŠ¹ì„±: ê²°ì¸¡ë¥ ={missing_ratio:.2%}, ì™œë„={skew:.2f}")
                else:
                    print(f"   ğŸ“Š íŠ¹ì„±: ê²°ì¸¡ë¥ ={missing_ratio:.2%}")
            else:
                missing_ratio = df[col].isnull().mean()
                unique_count = df[col].nunique()
                print(f"   ğŸ“Š íŠ¹ì„±: ê²°ì¸¡ë¥ ={missing_ratio:.2%}, ê³ ìœ ê°’={unique_count}ê°œ")
            
            # ì¶”ì²œ ê²°ê³¼
            if recs:
                for rec in recs:
                    print(f"   âœ… {rec['action']}: {rec['why']}")
            else:
                print(f"   âšª ì¶”ì²œ ì—†ìŒ")
        
        # 4. ê·œì¹™ ë§¤ì¹­ ë¶„ì„
        print(f"\nğŸ¯ ê·œì¹™ ë§¤ì¹­ ë¶„ì„:")
        action_counts = {}
        for col, recs in recommendations['preprocessing'].items():
            for rec in recs:
                action = rec['action']
                action_counts[action] = action_counts.get(action, 0) + 1
        
        print(f"   ì¶”ì²œëœ ì•¡ì…˜ ë¶„í¬:")
        for action, count in sorted(action_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {action}: {count}íšŒ")
        
        # 5. ì‹œìŠ¤í…œ íŠ¹ì„± ë¶„ì„
        print(f"\nğŸ” ì‹œìŠ¤í…œ íŠ¹ì„± ë¶„ì„:")
        print(f"   ì¥ì :")
        print(f"   âœ… ì™¸ë¶€ YAML íŒŒì¼ë¡œ ê·œì¹™ ê´€ë¦¬")
        print(f"   âœ… ìˆœì°¨ì  ê·œì¹™ ë§¤ì¹­ìœ¼ë¡œ ìš°ì„ ìˆœìœ„ ì ìš©")
        print(f"   âœ… ì„¤ëª… ê°€ëŠ¥í•œ ì¶”ì²œ (action + why)")
        print(f"   âœ… ë°ì´í„° íƒ€ì…ë³„ ë…ë¦½ì  ì²˜ë¦¬")
        
        print(f"\n   í•œê³„ì :")
        print(f"   âŒ ê³ ì •ëœ ì„ê³„ê°’ (ëª¨ë“  ë°ì´í„°ì— ë™ì¼ ì ìš©)")
        print(f"   âŒ ë‹¨ìˆœ ì¡°ê±´ ë§¤ì¹­ (ë³µí•© ì¡°ê±´ ë¯¸ì§€ì›)")
        print(f"   âŒ ì»¨í…ìŠ¤íŠ¸ ë¯¸ê³ ë ¤ (ë°ì´í„° ì „ì²´ íŠ¹ì„± ë°˜ì˜ ë¶€ì¡±)")
        print(f"   âŒ ê¸°ë³¸ ë°ì´í„° íƒ€ì…ë§Œ ì§€ì› (ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜•)")
        
        # 6. ê°œì„  ë°©í–¥ ì œì‹œ
        print(f"\nğŸ’¡ ê°œì„  ë°©í–¥:")
        print(f"   1. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ì‘í˜• ì„ê³„ê°’")
        print(f"   2. ë°ì´í„° íƒ€ì… ì„¸ë¶„í™” (ID, í…ìŠ¤íŠ¸, ë¶ˆë¦° ë“±)")
        print(f"   3. ë³µí•© ì¡°ê±´ ë° ìƒí˜¸ì‘ìš© ì§€ì›")
        print(f"   4. ìš°ì„ ìˆœìœ„ ì‹œìŠ¤í…œ ë° ì½”ë“œ ìƒì„±")
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(rules_file):
            os.unlink(rules_file)
    
    print(f"\n" + "=" * 80)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 80)

if __name__ == "__main__":
    run_yaml_system_test() 