# ëŒ€ì‹œë³´ë“œ UI
# https://docs.streamlit.io/knowledge-base/deploy/increase-file-uploader-limit-streamlit-cloud
# https://docs.streamlit.io/library/advanced-features/configuration#set-configuration-options
# ì‹¤í–‰ë°©ë²•1: streamlit run recomendation.py
# ì‹¤í–‰ë°©ë²•2: streamlit run recomendation.py --server.maxUploadSize 500 --server.maxMessageSize 500 (ì—…ë¡œë“œ íŒŒì¼ ìš©ëŸ‰ ì¦ëŒ€í•  ê²½ìš°)
# import time # ì½”ë“œ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œ ì‚¬ìš©
# sqlite:///./database/database.db
# AxiosError: Request failed with status code 403 in streamlit ë°œìƒ ì‹œ, enableXsrfProtection ì…ë ¥í•˜ì—¬ ì‹¤í–‰
# streamlit run recomendation.py --server.enableXsrfProtection false

# streamlit 1.24.0 ì´ìƒ ë²„ì „ì—ì„œ íŒŒì¼ ì—…ë¡œë“œí•  ê²½ìš° AxiosError: Request failed with status code 403 ë°œìƒí•  ìˆ˜ ìˆìŒ
# AxiosError 403 ì—ëŸ¬ ë°œìƒ ì‹œ streamlit==1.24.0 ë²„ì „ìœ¼ë¡œ ë³€ê²½ 
# pip install streamlit==1.24.0

# import ray
import json
import requests
import pandas as pd
import streamlit as st
from lib.template import Template
from lib.prepro import Preprocessing
# from database.connector import Database # , SelectTB
from io import StringIO
import time
import numpy as np
import plotly.express as px


# import matplotlib.pyplot as plt
# import seaborn as sns


st.set_page_config(layout="wide")
st.sidebar.title("Details")

# ë¶„ë¥˜, ì´ìƒ íƒì§€ ë“± ì¶”ì²œë°›ì„ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„ íƒ
option = st.sidebar.selectbox(
    'ë¨¸ì‹ ëŸ¬ë‹ ìœ í˜• ì„ íƒ', ('ë¶„ë¥˜', 'êµ°ì§‘', 'íšŒê·€'))
connecton_option = st.sidebar.selectbox(
    'Select how to upload data', ('File_upload'))

uploaded_file = None
df = None

if connecton_option == 'File_upload':
    uploaded_file = st.sidebar.file_uploader("csv file upload", type="csv") # íŒŒì¼ ì—…ë¡œë“œ
    if uploaded_file:
        df = pd.read_csv(uploaded_file)

with st.spinner('Wait for it...'):
    updated_df = None
    # Uploaded data Dashboard
    if uploaded_file is not None or df is not None:
        template = Template(df)
        st.subheader('ë°ì´í„° ë¶„ì„')
        col_list = df.columns.tolist() # ë°ì´í„° ì „ì²˜ë¦¬ ì˜µì…˜ ì„¤ì • ë¦¬ìŠ¤íŠ¸
        target_feture = ""
        if option == 'ë¶„ë¥˜' or option == 'íšŒê·€':
            target_feture = st.sidebar.multiselect('Select Target Column', options=col_list)
        elif option == 'êµ°ì§‘':
            st.sidebar.info("êµ°ì§‘ ë¶„ì„ì€ ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ íƒ€ê²Ÿ ì»¬ëŸ¼ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        data_to_drop = st.sidebar.multiselect('Drop Cloumns', options=col_list)
        data_for_labelencoding = st.sidebar.multiselect('Choose LabelEncoding column name', options=col_list)
        
        tab_eda_df, tab_eda_info, tab_Label_counts = st.tabs(['Original data', 'Null information', 'Target Data Counts']) # tab_Label_counts Labels counts
        # tab_eda_df, tab_eda_info tab UI Template
        template.eda_df(tab_eda_df=tab_eda_df, tab_eda_info=tab_eda_info)
        label_to_drop = ""
        with tab_Label_counts: # Target Data ì •ë³´ ì¶œë ¥ ë° ì‹œê°í™”
            if target_feture:            
                label_to_drop = template.label_to_drop(target_feture) # ì œê±°í•  Target ë°ì´í„° ì„ íƒ
            else:
                template.sample_df()

  
        if data_for_labelencoding:
            prepro = Preprocessing()
            if updated_df is None:
                # st.write(type(df[data_for_labelencoding]))
                df = prepro.encoded_df(df, data_for_labelencoding[0])
                updated_df = df
            if updated_df is not None:
                updated_df = prepro.encoded_df(updated_df, data_for_labelencoding[0])

       
        if data_to_drop:
            for data in data_to_drop:
                updated_df = df.drop(data_to_drop, axis=1)

     
        try:
            if label_to_drop:
                target_feture = target_feture[0]
                label_to_drop = label_to_drop[0]
                updated_df = df[df[target_feture] != label_to_drop]
        except ValueError:
            st.write('1ê°œ ì´ìƒ ë°ì´í„°ê°€ ë‚¨ì•„ìˆì–´ì•¼ í•©ë‹ˆë‹¤.')

        # ë°ì´í„° ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¶œë ¥
        if updated_df is not None: 
            st.subheader('ë°ì´í„° ì „ì²˜ë¦¬')
            st.dataframe(updated_df, use_container_width=True)
        
        if st.sidebar.button("ì´ˆê¸°í™”"):
            st.cache_resource.clear()


#################### Starting ML traning
        button_for_training = st.sidebar.button("ë¨¸ì‹ ëŸ¬ë‹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", key="button1") 
        if button_for_training: # ë¶„ë¥˜, ì´ìƒíƒì§€ ì˜µì…˜ì— ë”°ë¼ ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ì§„í–‰
            start_time = time.time()
            # start_time = time.time() # í•™ìŠµ ì‹œê°„ ì²´í¬ ì‹œ ì„¤ì •

            if option == 'ë¶„ë¥˜' or option == 'êµ°ì§‘' or option == 'íšŒê·€':
                st.subheader('ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ê²°ê³¼')
                with st.spinner('Wait for it...'):
                    if updated_df is None:
                        updated_df = df   

                    # ë°ì´í„° ê²€ì¦
                    if option != 'êµ°ì§‘':  # êµ°ì§‘ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ íƒ€ê²Ÿ ì»¬ëŸ¼ ê²€ì¦
                        if target_feture is None or len(target_feture) == 0:
                            st.error("íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                            st.stop()
                    
                    # ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì„ íƒ
                    numeric_df = updated_df.select_dtypes(include=[np.number])
                    if len(numeric_df.columns) == 0:
                        st.error("ìˆ˜ì¹˜í˜• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()
                    
                    # ê° ë¨¸ì‹ ëŸ¬ë‹ ìœ í˜•ë³„ ê²€ì¦
                    if option == 'ë¶„ë¥˜':
                        # ë¶„ë¥˜: íƒ€ê²Ÿì´ ë²”ì£¼í˜•ì´ì–´ì•¼ í•¨
                        if target_feture[0] not in updated_df.columns:
                            st.error(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_feture[0]}'ì´ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                            st.stop()
                        
                        # ë¶„ë¥˜ì˜ ê²½ìš° íƒ€ê²Ÿì´ ë²”ì£¼í˜•ì´ì–´ì•¼ í•˜ë¯€ë¡œ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹Œ ì»¬ëŸ¼ë„ í—ˆìš©
                        target_unique_count = updated_df[target_feture[0]].nunique()
                        if target_unique_count < 2:
                            st.error("ë¶„ë¥˜ë¥¼ ìœ„í•´ì„œëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ì— ìµœì†Œ 2ê°œ ì´ìƒì˜ í´ë˜ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                            st.stop()
                        elif target_unique_count > 50:
                            st.warning(f"íƒ€ê²Ÿ ì»¬ëŸ¼ì— {target_unique_count}ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ ë¶„ë¥˜ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    elif option == 'íšŒê·€':
                        # íšŒê·€: íƒ€ê²Ÿì´ ìˆ˜ì¹˜í˜•ì´ì–´ì•¼ í•¨
                        if target_feture[0] not in numeric_df.columns:
                            st.error(f"íšŒê·€ë¥¼ ìœ„í•´ì„œëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_feture[0]}'ì´ ìˆ˜ì¹˜í˜•ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                            st.stop()
                        
                        # íšŒê·€ì˜ ê²½ìš° íƒ€ê²Ÿì—ì„œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì œì™¸
                        feature_cols = [col for col in numeric_df.columns if col != target_feture[0]]
                        if len(feature_cols) == 0:
                            st.error("íšŒê·€ë¥¼ ìœ„í•œ íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            st.stop()
                        numeric_df = updated_df[feature_cols + [target_feture[0]]]
                    
                    elif option == 'êµ°ì§‘':
                        # êµ°ì§‘: íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ì–´ì•¼ í•¨ (ë¹„ì§€ë„ í•™ìŠµ)
                        # st.warning("êµ°ì§‘ ë¶„ì„ì€ ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        # êµ°ì§‘ì˜ ê²½ìš° ëª¨ë“  ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©
                        feature_cols = numeric_df.columns.tolist()
                        if len(feature_cols) == 0:
                            st.error("êµ°ì§‘ ë¶„ì„ì„ ìœ„í•œ íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            st.stop()
                        numeric_df = updated_df[feature_cols]
                        # êµ°ì§‘ì˜ ê²½ìš° íƒ€ê²Ÿ ì •ë³´ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì • (ë°±ì—”ë“œì—ì„œ ë¬´ì‹œë¨)
                        target_feture = ""

                    
                    json_data = numeric_df.to_json() # pandas DataFrameë¥¼ json í˜•íƒœë¡œ ë³€í™˜
                    data_dump = json.dumps({'json_data':json_data, 'target': target_feture}) # í•™ìŠµ ë°ì´í„°, Target Data ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ì§ë ¬í™”(serialize)
                    data = json.loads(data_dump) # jsonì„ íŒŒì´ì¬ ê°ì²´ë¡œ ë³€í™˜
                    
                    try:
                        if option == 'ë¶„ë¥˜':
                            response = requests.post('http://127.0.0.1:8000/automl/classification', json=data, timeout=300)
                        elif option == 'êµ°ì§‘':
                            response = requests.post('http://127.0.0.1:8000/automl/clustering', json=data, timeout=300)
                        elif option == 'íšŒê·€':
                            response = requests.post('http://127.0.0.1:8000/automl/regression', json=data, timeout=300)
                        
                        if response.status_code == 200: 
                            json_data = response.json() 
                            data = json.loads(json_data['result'])
                             # ë¶„ë¥˜ ëª¨ë¸
                            if option == 'ë¶„ë¥˜':
                                # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í†µí•©í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                                clf_results = {}
                                scoring_methods = ['accuracy', 'recall', 'precision', 'f1_weighted']
                                
                                # ê° ì§€í‘œë³„ ê²°ê³¼ ìˆ˜ì§‘
                                for i, scoring_method in enumerate(scoring_methods):
                                    if str(i) in data:
                                        best_json = data[str(i)]["best"]
                                        best_data = json.loads(best_json)
                                        clf_results[scoring_method] = best_data.get('models', {})
                                
                                # ëª¨ë“  ëª¨ë¸ëª… ìˆ˜ì§‘
                                all_models = set()
                                for models_dict in clf_results.values():
                                    all_models.update(models_dict.keys())
                                
                                # í†µí•© DataFrame ìƒì„±
                                integrated_data = []
                                for model_name in all_models:
                                    model_scores = {'Model': model_name}
                                    for scoring_method in scoring_methods:
                                        if scoring_method in clf_results:
                                            model_scores[scoring_method] = clf_results[scoring_method].get(model_name, 0.0)
                                        else:
                                            model_scores[scoring_method] = 0.0
                                    integrated_data.append(model_scores)
                                
                                # DataFrame ìƒì„± ë° ì •ë ¬
                                if integrated_data:
                                    sorted_df = pd.DataFrame(integrated_data).set_index('Model')
                                    # f1_weighted ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì—†ìœ¼ë©´ accuracy)
                                    if 'f1_weighted' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='f1_weighted', ascending=False)
                                    elif 'accuracy' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='accuracy', ascending=False)
                                    else:
                                        sorted_df = sorted_df.sort_values(by=sorted_df.columns[0], ascending=False)
                                else:
                                    sorted_df = pd.DataFrame({'Score': [0.0]}, index=['No Models'])
                            if option == 'íšŒê·€':
                                # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í†µí•©í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                                reg_results = {}
                                scoring_methods = ['neg_mean_squared_error', 'neg_mean_absolute_error']
                                
                                # ê° ì§€í‘œë³„ ê²°ê³¼ ìˆ˜ì§‘
                                for i, scoring_method in enumerate(scoring_methods):
                                    if str(i) in data:
                                        best_json = data[str(i)]["best"]
                                        best_data = json.loads(best_json)
                                        reg_results[scoring_method] = best_data.get('models', {})
                                
                                # ëª¨ë“  ëª¨ë¸ëª… ìˆ˜ì§‘
                                all_models = set()
                                for models_dict in reg_results.values():
                                    all_models.update(models_dict.keys())
                                
                                # í†µí•© DataFrame ìƒì„±
                                integrated_data = []
                                for model_name in all_models:
                                    model_scores = {'Model': model_name}
                                    for scoring_method in scoring_methods:
                                        if scoring_method in reg_results:
                                            model_scores[scoring_method] = reg_results[scoring_method].get(model_name, 0.0)
                                        else:
                                            model_scores[scoring_method] = 0.0
                                    integrated_data.append(model_scores)
                                
                                # DataFrame ìƒì„± ë° ì •ë ¬
                                if integrated_data:
                                    sorted_df = pd.DataFrame(integrated_data).set_index('Model')
                                    # neg_mean_squared_error ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìŒìˆ˜ê°’ì´ë¯€ë¡œ ë‚´ë¦¼ì°¨ìˆœ)
                                    if 'neg_mean_squared_error' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='neg_mean_squared_error', ascending=False)
                                    else:
                                        sorted_df = sorted_df.sort_values(by=sorted_df.columns[0], ascending=False)
                                else:
                                    sorted_df = pd.DataFrame({'Score': [0.0]}, index=['No Models'])
                            if option == 'êµ°ì§‘':
                                # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í†µí•©í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                                cluster_results = {}
                                scoring_methods = ['silhouette', 'calinski_harabasz', 'davies_bouldin']
                                
                                # ê° ì§€í‘œë³„ ê²°ê³¼ ìˆ˜ì§‘
                                for i, scoring_method in enumerate(scoring_methods):
                                    if str(i) in data:
                                        best_json = data[str(i)]["best"]
                                        best_data = json.loads(best_json)
                                        cluster_results[scoring_method] = best_data.get('models', {})
                                
                                # ëª¨ë“  ëª¨ë¸ëª… ìˆ˜ì§‘
                                all_models = set()
                                for models_dict in cluster_results.values():
                                    all_models.update(models_dict.keys())
                                
                                # í†µí•© DataFrame ìƒì„±
                                integrated_data = []
                                for model_name in all_models:
                                    model_scores = {'Model': model_name}
                                    for scoring_method in scoring_methods:
                                        if scoring_method in cluster_results:
                                            model_scores[scoring_method] = cluster_results[scoring_method].get(model_name, 0.0)
                                        else:
                                            model_scores[scoring_method] = 0.0
                                    integrated_data.append(model_scores)
                                
                                # DataFrame ìƒì„± ë° ì •ë ¬
                                if integrated_data:
                                    sorted_df = pd.DataFrame(integrated_data).set_index('Model')
                                    # silhouette ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                                    if 'silhouette' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='silhouette', ascending=False)
                                    elif 'calinski_harabasz' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='calinski_harabasz', ascending=False)
                                    elif 'davies_bouldin' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='davies_bouldin', ascending=True)  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                                    else:
                                        sorted_df = sorted_df.sort_values(by=sorted_df.columns[0], ascending=False)
                                else:
                                    sorted_df = pd.DataFrame({'Score': [0.0]}, index=['No Models'])
                                
                                # êµ°ì§‘ ë¶„ì„ ê²°ê³¼ ì •ë ¬ ë¡œì§ ì œê±° (ì´ë¯¸ ì •ë ¬ë¨)
                            # sorted_concat_df = concat_df.sort_values(by='f1_weighted', ascending=False)

                            # ëª¨ë¸ ì¶”ì²œ
                            # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í¬í•¨í•œ ìš°ìƒí–¥ ë¼ì¸ ì°¨íŠ¸
                            chart_df = sorted_df.reset_index()
                            # ìš°ìƒí–¥ ì‹œê°í™”ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ë’¤ì§‘ê¸° (ë‚®ì€ ì„±ëŠ¥ë¶€í„° ë†’ì€ ì„±ëŠ¥ ìˆœì„œ)
                            chart_df = chart_df.iloc[::-1].reset_index(drop=True)
                            
                            # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ long formatìœ¼ë¡œ ë³€í™˜
                            chart_df_long = chart_df.melt(
                                id_vars=['Model'], 
                                var_name='Metric', 
                                value_name='Score'
                            )
                            
                            fig = px.line(
                                chart_df_long,
                                x='Model',
                                y='Score',
                                color='Metric',
                                # title='ëª¨ë“  í‰ê°€ì§€í‘œ ì„±ëŠ¥ ë¹„êµ - ìš°ìƒí–¥ ì„±ëŠ¥ íŠ¸ë Œë“œ',
                                labels={'Model': 'ëª¨ë¸', 'Score': 'ì„±ëŠ¥ ì ìˆ˜', 'Metric': 'í‰ê°€ì§€í‘œ'},
                                markers=True
                            )
                            
                            # xì¶• ìˆœì„œ ê³ ì • (ë‚®ì€ ì„±ëŠ¥ë¶€í„° ë†’ì€ ì„±ëŠ¥ìœ¼ë¡œ ìš°ìƒí–¥)
                            fig.update_xaxes(categoryorder='array', categoryarray=chart_df['Model'].tolist())
                            fig.update_layout(
                                height=500,
                                xaxis_title="ëª¨ë¸",
                                yaxis_title="ì„±ëŠ¥ ì ìˆ˜",
                                legend_title="í‰ê°€ì§€í‘œ"
                            )
                            
                            # ë¼ì¸ ë° ë§ˆì»¤ ìŠ¤íƒ€ì¼ ì„¤ì •
                            fig.update_traces(
                                line=dict(width=3),
                                marker=dict(size=8)
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # í‰ê°€ì§€í‘œ ì„¤ëª… ì¶”ê°€
                            if option == 'ë¶„ë¥˜':
                                st.subheader('ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
                                st.info("""
                                **í‰ê°€ì§€í‘œ ì„¤ëª…:**
                                - **accuracy**: ì •í™•ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                                - **recall**: ì¬í˜„ìœ¨ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                                - **precision**: ì •ë°€ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)  
                                - **f1_weighted**: ê°€ì¤‘ F1 ìŠ¤ì½”ì–´ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                                """)
                            elif option == 'íšŒê·€':
                                st.subheader('ğŸ“Š íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
                                st.info("""
                                **í‰ê°€ì§€í‘œ ì„¤ëª…:**
                                - **neg_mean_squared_error**: ìŒì˜ í‰ê·  ì œê³± ì˜¤ì°¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                                - **neg_mean_absolute_error**: ìŒì˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                                """)
                            elif option == 'êµ°ì§‘':
                                st.subheader('ğŸ“Š êµ°ì§‘ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
                                st.info("""
                                **í‰ê°€ì§€í‘œ ì„¤ëª…:**
                                - **silhouette**: ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, -1~1)
                                - **calinski_harabasz**: ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì§€ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                                - **davies_bouldin**: ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì§€ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                                """)
                            
                            st.subheader('ğŸ¯ ëª¨ë¸ ë¹„êµ')
                            
                            # ì†Œìˆ˜ì  ìë¦¬ìˆ˜ í˜•ì‹ ì§€ì •
                            if not sorted_df.empty and 'Score' not in sorted_df.columns:
                                # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ì†Œìˆ˜ì  4ìë¦¬ë¡œ í‘œì‹œ
                                formatted_df = sorted_df.round(4)
                                st.dataframe(formatted_df, use_container_width=True)
                                
                            else:
                                st.dataframe(sorted_df, use_container_width=True)

                            # ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ
                            data = updated_df
                            
                            # ë°ì´í„° ë¶„ì„ ë° íŠ¹ì„± ì¶”ì¶œ
                            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                            categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
                            
                            # ë°ì´í„° íŠ¹ì„± ë¶„ì„
                            data_characteristics = {}
                            for col in numeric_cols:
                                col_data = data[col].dropna()
                                if len(col_data) > 0:
                                    data_characteristics[col] = {
                                        'mean': col_data.mean(),
                                        'std': col_data.std(),
                                        'skewness': col_data.skew(),
                                        'unique_count': col_data.nunique(),
                                        'missing_ratio': data[col].isnull().sum() / len(data)
                                    }
                            
                            # ì§€ëŠ¥í˜• ì „ì²˜ë¦¬ ì¶”ì²œ
                            preprocessing_recommendations = []
                            
                            for col in numeric_cols:
                                if col in data_characteristics:
                                    char = data_characteristics[col]
                                    recommendations = []
                                    
                                    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                                    if char['missing_ratio'] > 0.1:
                                        recommendations.append('Missing Value Imputation (Mean/Median)')
                                    
                                    # ì´ìƒì¹˜ íƒì§€
                                    if abs(char['skewness']) > 1:
                                        recommendations.append('Outlier Detection')
                                    
                                    # ìŠ¤ì¼€ì¼ë§ ì¶”ì²œ
                                    if char['std'] > char['mean'] * 0.5:
                                        recommendations.append('Standardization (Z-score)')
                                    else:
                                        recommendations.append('Normalization (Min-Max)')
                                    
                                    # ë¶„í¬ íŠ¹ì„±ì— ë”°ë¥¸ ì¶”ì²œ
                                    if abs(char['skewness']) > 2:
                                        recommendations.append('Log/Box-Cox Transformation')
                                    
                                    # ê¸°ë³¸ ì¶”ì²œ
                                    if len(recommendations) < 3:
                                        recommendations.extend(['Feature Engineering', 'Robust Scaling'])
                                    
                                    preprocessing_recommendations.append({
                                        'Column': col,
                                        'Data Type': 'Numeric',
                                        'Primary Recommendation': recommendations[0] if recommendations else 'Standardization',
                                        'Secondary Recommendation': recommendations[1] if len(recommendations) > 1 else 'Normalization',
                                        'Additional Techniques': ', '.join(recommendations[2:]) if len(recommendations) > 2 else 'None'
                                    })
                            
                            for col in categorical_cols:
                                unique_count = data[col].nunique()
                                missing_ratio = data[col].isnull().sum() / len(data)
                                
                                recommendations = []
                                if missing_ratio > 0.1:
                                    recommendations.append('Missing Value Imputation')
                                
                                if unique_count < 10:
                                    recommendations.append('One-Hot Encoding')
                                elif unique_count < 50:
                                    recommendations.append('Label Encoding')
                                else:
                                    recommendations.append('Target Encoding')
                                
                                if unique_count > 100:
                                    recommendations.append('Feature Selection')
                                
                                preprocessing_recommendations.append({
                                    'Column': col,
                                    'Data Type': 'Categorical',
                                    # 'Unique Values': unique_count,
                                    # 'Missing Ratio': f"{missing_ratio:.2%}",
                                    'Primary Recommendation': recommendations[0] if recommendations else 'One-Hot Encoding',
                                    'Secondary Recommendation': recommendations[1] if len(recommendations) > 1 else 'Label Encoding',
                                    'Additional Techniques': ', '.join(recommendations[2:]) if len(recommendations) > 2 else 'None'
                                })
                            
                            # ì‹œê°í™” ì¶”ì²œ
                            visualization_recommendations = []
                            
                            for col in numeric_cols:
                                if col in data_characteristics:
                                    char = data_characteristics[col]
                                    vis_recommendations = []
                                    
                                    # ë¶„í¬ íŠ¹ì„±ì— ë”°ë¥¸ ì‹œê°í™”
                                    if abs(char['skewness']) > 1:
                                        vis_recommendations.extend(['Box Plot', 'Histogram with KDE'])
                                    else:
                                        vis_recommendations.extend(['Histogram', 'Density Plot'])
                                    
                                    # ì´ìƒì¹˜ íƒì§€
                                    if abs(char['skewness']) > 2:
                                        vis_recommendations.append('Outlier Plot')
                                    
                                    # ê¸°ë³¸ ì¶”ì²œ
                                    vis_recommendations.extend(['Distribution Plot', 'Summary Statistics'])
                                    
                                    visualization_recommendations.append({
                                        'Column': col,
                                        'Data Type': 'Numeric',
                                        'Primary Visualization': vis_recommendations[0],
                                        'Secondary Visualization': vis_recommendations[1] if len(vis_recommendations) > 1 else 'Histogram',
                                        'Additional Charts': ', '.join(vis_recommendations[2:]) if len(vis_recommendations) > 2 else 'None'
                                    })
                            
                            for col in categorical_cols:
                                unique_count = data[col].nunique()
                                vis_recommendations = []
                                
                                if unique_count <= 10:
                                    vis_recommendations.extend(['Bar Chart', 'Pie Chart'])
                                else:
                                    vis_recommendations.extend(['Bar Chart', 'Horizontal Bar Chart'])
                                
                                vis_recommendations.extend(['Value Counts', 'Category Distribution'])
                                
                                visualization_recommendations.append({
                                    'Column': col,
                                    'Data Type': 'Categorical',
                                    'Unique Values': unique_count,
                                    'Primary Visualization': vis_recommendations[0],
                                    'Secondary Visualization': vis_recommendations[1] if len(vis_recommendations) > 1 else 'Bar Chart',
                                    'Additional Charts': ', '.join(vis_recommendations[2:]) if len(vis_recommendations) > 2 else 'None'
                                })
                            
                            # ëª¨ë¸ ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ
                            best_model = sorted_df.iloc[0] if len(sorted_df) > 0 else None

                            # ê²°ê³¼ í‘œì‹œ
                            prepro_recommendations_df = pd.DataFrame(preprocessing_recommendations)
                            vis_recommendations_df = pd.DataFrame(visualization_recommendations)
                            
                            
                            st.subheader("ğŸ”§ ì „ì²˜ë¦¬ ì¶”ì²œ ê²°ê³¼")
                            st.dataframe(prepro_recommendations_df, use_container_width=True)
                            
                            st.subheader("ğŸ“ˆ ì‹œê°í™” ì¶”ì²œ ê²°ê³¼")
                            st.dataframe(vis_recommendations_df, use_container_width=True)
                            
                            # í†µí•© ì¶”ì²œ ê²°ê³¼
                            if best_model is not None:
                                st.subheader("ğŸ¯ ìµœì  ëª¨ë¸ ì¶”ì²œ ê²°ê³¼")
                                best_model_name = sorted_df.index[0] # if len(sorted_df) > 0 else "Unknown"
                                
                                recommendation_summary = {
                                    'Best Model': best_model_name,
                                    'Key Preprocessing': prepro_recommendations_df.iloc[0]['Primary Recommendation'] if len(prepro_recommendations_df) > 0 else 'N/A',
                                    'Key Visualization': vis_recommendations_df.iloc[0]['Primary Visualization'] if len(vis_recommendations_df) > 0 else 'N/A'
                                }
                                
                                summary_df = pd.DataFrame([recommendation_summary])
                                st.dataframe(summary_df, use_container_width=True)
                        else:
                            error_data = response.json()
                            st.error(f"API ì˜¤ë¥˜: {error_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                            
                    except requests.exceptions.Timeout:
                        st.error("ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    except requests.exceptions.ConnectionError:
                        st.error("ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    except Exception as e:
                        st.error(f"ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")