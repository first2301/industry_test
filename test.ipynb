{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831165cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv('.\\\\data\\\\Test_01.csv')\n",
    "\n",
    "df2 = pd.read_csv('.\\\\data\\\\Test_02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1921316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIXA_PASTEUR_STATE    3.170699\n",
      "MIXB_PASTEUR_STATE    3.204670\n",
      "MIXA_PASTEUR_TEMP    -4.483568\n",
      "MIXB_PASTEUR_TEMP    -4.155342\n",
      "dtype: float64\n",
      "\n",
      "MIXA_PASTEUR_STATE     8.063625\n",
      "MIXB_PASTEUR_STATE     8.280474\n",
      "MIXA_PASTEUR_TEMP     20.364007\n",
      "MIXB_PASTEUR_TEMP     16.749616\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df1.iloc[:, 1:5].skew())\n",
    "print()\n",
    "print(df1.iloc[:, 1:5].kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b887450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   STD_DT              1567 non-null   object\n",
      " 1   MIXA_PASTEUR_STATE  1567 non-null   int64 \n",
      " 2   MIXB_PASTEUR_STATE  1567 non-null   int64 \n",
      " 3   MIXA_PASTEUR_TEMP   1567 non-null   int64 \n",
      " 4   MIXB_PASTEUR_TEMP   1567 non-null   int64 \n",
      " 5   INSP                1567 non-null   object\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 73.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d254754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1777 entries, 0 to 1776\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   ph               1777 non-null   float64\n",
      " 1   Hardness         1777 non-null   float64\n",
      " 2   Solids           1777 non-null   float64\n",
      " 3   Chloramines      1777 non-null   float64\n",
      " 4   Sulfate          1777 non-null   float64\n",
      " 5   Conductivity     1777 non-null   float64\n",
      " 6   Organic_carbon   1777 non-null   float64\n",
      " 7   Trihalomethanes  1777 non-null   float64\n",
      " 8   Turbidity        1777 non-null   float64\n",
      " 9   Potability       1777 non-null   int64  \n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 139.0 KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86d84d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingRecommender:\n",
    "    \"\"\"\n",
    "    데이터 특성(결측·분포·카디널리티 등)에 근거해 전처리 기법을 추천하는 클래스\n",
    "    \"\"\"\n",
    "\n",
    "    def recommend(self, df: pd.DataFrame) -> dict:\n",
    "        recs = {}\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include='number').columns\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        # ──────────────────────────────\n",
    "        # Numeric Features\n",
    "        # ──────────────────────────────\n",
    "        for col in numeric_cols:\n",
    "            col_data = df[col].dropna()\n",
    "            if col_data.empty:\n",
    "                continue\n",
    "\n",
    "            mean_val = col_data.mean()\n",
    "            std_val = col_data.std()\n",
    "            skew_val = col_data.skew()\n",
    "            missing_ratio = df[col].isna().mean()\n",
    "\n",
    "            col_recs = []\n",
    "\n",
    "            # 결측치 처리\n",
    "            if missing_ratio > 0.10:\n",
    "                col_recs.append('missing value imputation (mean/median)')\n",
    "\n",
    "            # 이상치 탐지(비대칭 분포)\n",
    "            if abs(skew_val) > 1:\n",
    "                col_recs.append('outlier detection')\n",
    "\n",
    "            # 스케일링\n",
    "            if std_val > mean_val * 0.5:\n",
    "                col_recs.append('standardization (z‑score)')\n",
    "            else:\n",
    "                col_recs.append('normalization (min‑max)')\n",
    "\n",
    "            # 심하게 왜도 큰 경우 변환\n",
    "            if abs(skew_val) > 2:\n",
    "                col_recs.append('log/box‑cox transform')\n",
    "\n",
    "            # 기본 보강\n",
    "            if len(col_recs) < 3:\n",
    "                col_recs.extend(['feature engineering', 'robust scaling'])\n",
    "\n",
    "            recs[col] = col_recs\n",
    "\n",
    "        # ──────────────────────────────\n",
    "        # Categorical Features\n",
    "        # ──────────────────────────────\n",
    "        for col in categorical_cols:\n",
    "            unique_cnt = df[col].nunique(dropna=True)\n",
    "            missing_ratio = df[col].isna().mean()\n",
    "\n",
    "            col_recs = []\n",
    "\n",
    "            if missing_ratio > 0.10:\n",
    "                col_recs.append('missing value imputation')\n",
    "\n",
    "            # 인코딩 방식 선택\n",
    "            if unique_cnt < 10:\n",
    "                col_recs.append('one‑hot encoding')\n",
    "            elif unique_cnt < 50:\n",
    "                col_recs.append('label encoding')\n",
    "            else:\n",
    "                col_recs.append('target encoding')\n",
    "\n",
    "            # 고카디널리티 특성 선택 고려\n",
    "            if unique_cnt > 100:\n",
    "                col_recs.append('feature selection')\n",
    "\n",
    "            recs[col] = col_recs\n",
    "\n",
    "        return recs\n",
    "\n",
    "\n",
    "class VisualizationRecommender:\n",
    "    \"\"\"\n",
    "    변수 타입에 따라 기본 탐색용 시각화 유형을 추천하는 클래스\n",
    "    \"\"\"\n",
    "\n",
    "    def recommend(self, df: pd.DataFrame) -> dict:\n",
    "        viz = {}\n",
    "        dtypes = df.dtypes.apply(lambda x: x.name)\n",
    "\n",
    "        for col, dtype in dtypes.items():\n",
    "            if dtype in ['int64', 'float64', 'Int64', 'Float64']:\n",
    "                viz[col] = ['histogram', 'boxplot']\n",
    "            else:\n",
    "                viz[col] = ['bar chart', 'pie chart']\n",
    "\n",
    "        return viz\n",
    "\n",
    "\n",
    "class RecommendationEngine:\n",
    "    \"\"\"\n",
    "    전처리 및 시각화 추천 엔진\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.preprocessor = PreprocessingRecommender()\n",
    "        self.visualizer = VisualizationRecommender()\n",
    "\n",
    "    def run(self, df: pd.DataFrame) -> dict:\n",
    "        return {\n",
    "            'preprocessing': self.preprocessor.recommend(df),\n",
    "            'visualization': self.visualizer.recommend(df)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883e7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = PreprocessingRecommender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30201cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.recommend(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "986dff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ph', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Hardness', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Solids', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Chloramines', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Sulfate', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Conductivity', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Organic_carbon', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Trihalomethanes', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Turbidity', ['normalization (min‑max)', 'feature engineering', 'robust scaling'])\n",
      "('Potability', ['standardization (z‑score)', 'feature engineering', 'robust scaling'])\n"
     ]
    }
   ],
   "source": [
    "for i in test2.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33ef26d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MIXA_PASTEUR_STATE', ['outlier detection', 'standardization (z‑score)', 'log/box‑cox transform'])\n",
      "('MIXB_PASTEUR_STATE', ['outlier detection', 'standardization (z‑score)', 'log/box‑cox transform'])\n",
      "('MIXA_PASTEUR_TEMP', ['outlier detection', 'normalization (min‑max)', 'log/box‑cox transform'])\n",
      "('MIXB_PASTEUR_TEMP', ['outlier detection', 'normalization (min‑max)', 'log/box‑cox transform'])\n",
      "('STD_DT', ['target encoding', 'feature selection'])\n",
      "('INSP', ['one‑hot encoding'])\n"
     ]
    }
   ],
   "source": [
    "for i in test2.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea101db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PreprocessingRecommender:\n",
    "    \"\"\"\n",
    "    풍부한 규칙 집합을 이용해 전처리 기법을 추천하는 클래스\n",
    "    ────────────────────────────────────────────────\n",
    "    ▸ Numeric : 결측·왜도·첨도·분산·상관 등\n",
    "    ▸ Categorical : 결측·카디널리티·희소·길이 등\n",
    "    \"\"\"\n",
    "\n",
    "    def recommend(self, df: pd.DataFrame) -> dict:\n",
    "        recs: dict[str, list[str]] = {}\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include='number').columns\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "        # ──────────────────────────────\n",
    "        # Numeric Features\n",
    "        # ──────────────────────────────\n",
    "        if len(numeric_cols) > 1:\n",
    "            corr = df[numeric_cols].corr().abs()\n",
    "        else:\n",
    "            corr = pd.DataFrame()\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            col_data = df[col].dropna()\n",
    "            if col_data.empty:\n",
    "                continue\n",
    "\n",
    "            mean_val = col_data.mean()\n",
    "            std_val = col_data.std()\n",
    "            skew_val = col_data.skew()\n",
    "            kurt_val = col_data.kurtosis()\n",
    "            missing_ratio = df[col].isna().mean()\n",
    "            cv = std_val / mean_val if mean_val != 0 else np.nan\n",
    "\n",
    "            col_recs: list[str] = []\n",
    "\n",
    "            # (1) 결측치 처리\n",
    "            if missing_ratio > 0.40:\n",
    "                col_recs.append('drop column (excessive missing)')\n",
    "            elif missing_ratio > 0.15:\n",
    "                col_recs.append('advanced imputation (KNN/iterative)')\n",
    "            elif missing_ratio > 0.0:\n",
    "                col_recs.append('missing value imputation (mean/median)')\n",
    "\n",
    "            # (2) 분산 / 정보량\n",
    "            if cv < 0.1:\n",
    "                col_recs.append('low‑variance feature removal')\n",
    "\n",
    "            # (3) 이상치·분포\n",
    "            if abs(skew_val) > 2 or kurt_val > 7:\n",
    "                col_recs.append('winsorization / log‑boxcox transform')\n",
    "            elif abs(skew_val) > 1:\n",
    "                col_recs.append('outlier detection')\n",
    "\n",
    "            # (4) 스케일링 선택\n",
    "            if std_val > mean_val * 0.5:\n",
    "                col_recs.append('standardization (z‑score)')\n",
    "            elif abs(skew_val) > 1:\n",
    "                col_recs.append('robust scaling (IQR)')\n",
    "            else:\n",
    "                col_recs.append('normalization (min‑max)')\n",
    "\n",
    "            # (5) 다중공선성\n",
    "            if not corr.empty and (corr[col].drop(col) > 0.9).any():\n",
    "                col_recs.append('feature selection (high correlation)')\n",
    "\n",
    "            recs[col] = list(dict.fromkeys(col_recs))  # deduplicate preserving order\n",
    "\n",
    "        # ──────────────────────────────\n",
    "        # Categorical Features\n",
    "        # ──────────────────────────────\n",
    "        for col in categorical_cols:\n",
    "            unique_cnt = df[col].nunique(dropna=True)\n",
    "            missing_ratio = df[col].isna().mean()\n",
    "\n",
    "            col_recs: list[str] = []\n",
    "\n",
    "            # (1) 결측치\n",
    "            if missing_ratio > 0.30:\n",
    "                col_recs.append('drop column (excessive missing)')\n",
    "            elif missing_ratio > 0.10:\n",
    "                col_recs.append('missing value imputation (mode)')\n",
    "\n",
    "            # (2) 인코딩 방식\n",
    "            if unique_cnt <= 10:\n",
    "                col_recs.append('one‑hot encoding')\n",
    "            elif unique_cnt <= 50:\n",
    "                col_recs.append('label / ordinal encoding')\n",
    "            elif unique_cnt <= 1000:\n",
    "                col_recs.append('target / frequency encoding')\n",
    "            else:\n",
    "                col_recs.append('hashing encoding')\n",
    "\n",
    "            # (3) 희소·고카디널리티\n",
    "            if unique_cnt > 100:\n",
    "                col_recs.append('feature selection (high cardinality)')\n",
    "\n",
    "            recs[col] = list(dict.fromkeys(col_recs))\n",
    "\n",
    "        return recs\n",
    "\n",
    "\n",
    "class VisualizationRecommender:\n",
    "    \"\"\"\n",
    "    변수 타입 및 관계 기반으로 탐색적 시각화 기법을 추천합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def recommend(self, df: pd.DataFrame) -> dict:\n",
    "        viz: dict[str, list[str]] = {}\n",
    "        dtypes = df.dtypes.apply(lambda x: x.name)\n",
    "\n",
    "        numeric_cols = [c for c, t in dtypes.items() if t in ['int64', 'float64', 'Int64', 'Float64']]\n",
    "        categorical_cols = [c for c in dtypes.index if c not in numeric_cols]\n",
    "\n",
    "        # 단변량 추천\n",
    "        for col in numeric_cols:\n",
    "            viz[col] = ['histogram', 'boxplot']\n",
    "        for col in categorical_cols:\n",
    "            viz[col] = ['bar chart', 'pie chart']\n",
    "\n",
    "        # 간단한 이변량 추천 (높은 상관)\n",
    "        if len(numeric_cols) > 1:\n",
    "            corr = df[numeric_cols].corr().abs()\n",
    "            pairs = [(i, j, corr.at[i, j]) for i in numeric_cols for j in numeric_cols if i < j]\n",
    "            high_pairs = [(i, j) for i, j, v in pairs if v > 0.7]\n",
    "            for x, y in high_pairs[:10]:  # 최대 10쌍 표시\n",
    "                viz[f'{x} vs {y}'] = ['scatter plot', 'regression line']\n",
    "\n",
    "        return viz\n",
    "\n",
    "\n",
    "class RecommendationEngine:\n",
    "    \"\"\"\n",
    "    전처리·시각화 추천 엔진\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.preprocessor = PreprocessingRecommender()\n",
    "        self.visualizer = VisualizationRecommender()\n",
    "\n",
    "    def run(self, df: pd.DataFrame) -> dict:\n",
    "        return {\n",
    "            'preprocessing': self.preprocessor.recommend(df),\n",
    "            'visualization': self.visualizer.recommend(df)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6f25ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = RecommendationEngine().run(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05c10051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MIXA_PASTEUR_STATE', ['winsorization / log‑boxcox transform', 'standardization (z‑score)', 'feature selection (high correlation)'])\n",
      "('MIXB_PASTEUR_STATE', ['winsorization / log‑boxcox transform', 'standardization (z‑score)', 'feature selection (high correlation)'])\n",
      "('MIXA_PASTEUR_TEMP', ['winsorization / log‑boxcox transform', 'robust scaling (IQR)'])\n",
      "('MIXB_PASTEUR_TEMP', ['winsorization / log‑boxcox transform', 'robust scaling (IQR)'])\n",
      "('STD_DT', ['hashing encoding', 'feature selection (high cardinality)'])\n",
      "('INSP', ['one‑hot encoding'])\n"
     ]
    }
   ],
   "source": [
    "for i in test['preprocessing'].items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e05c42cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = RecommendationEngine().run(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7221b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ph', ['normalization (min‑max)'])\n",
      "('Hardness', ['normalization (min‑max)'])\n",
      "('Solids', ['normalization (min‑max)'])\n",
      "('Chloramines', ['normalization (min‑max)'])\n",
      "('Sulfate', ['normalization (min‑max)'])\n",
      "('Conductivity', ['normalization (min‑max)'])\n",
      "('Organic_carbon', ['normalization (min‑max)'])\n",
      "('Trihalomethanes', ['normalization (min‑max)'])\n",
      "('Turbidity', ['normalization (min‑max)'])\n",
      "('Potability', ['standardization (z‑score)'])\n"
     ]
    }
   ],
   "source": [
    "for i in test2['preprocessing'].items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969bc57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataType(Enum):\n",
    "    \"\"\"데이터 타입 분류\"\"\"\n",
    "    NUMERIC_CONTINUOUS = \"numeric_continuous\"\n",
    "    NUMERIC_DISCRETE = \"numeric_discrete\"\n",
    "    CATEGORICAL_NOMINAL = \"categorical_nominal\"\n",
    "    CATEGORICAL_ORDINAL = \"categorical_ordinal\"\n",
    "    DATETIME = \"datetime\"\n",
    "    TEXT = \"text\"\n",
    "    BOOLEAN = \"boolean\"\n",
    "    ID = \"id\"\n",
    "\n",
    "class DataContext(Enum):\n",
    "    \"\"\"데이터 컨텍스트 분류\"\"\"\n",
    "    GENERAL = \"general\"\n",
    "    TIMESERIES = \"timeseries\"\n",
    "    IMBALANCED = \"imbalanced\"\n",
    "    HIGH_DIMENSIONAL = \"high_dimensional\"\n",
    "    SPARSE = \"sparse\"\n",
    "    MULTIMODAL = \"multimodal\"\n",
    "\n",
    "@dataclass\n",
    "class DataProfiler:\n",
    "    \"\"\"데이터 특성 프로파일링\"\"\"\n",
    "    column_name: str\n",
    "    data_type: DataType\n",
    "    missing_ratio: float\n",
    "    unique_count: int\n",
    "    unique_ratio: float\n",
    "    skewness: Optional[float] = None\n",
    "    kurtosis: Optional[float] = None\n",
    "    mean: Optional[float] = None\n",
    "    std: Optional[float] = None\n",
    "    cv: Optional[float] = None  # 변동계수\n",
    "    outlier_ratio: Optional[float] = None\n",
    "    is_constant: bool = False\n",
    "    is_id_like: bool = False\n",
    "    text_length_stats: Optional[Dict] = None\n",
    "    datetime_range: Optional[Tuple] = None\n",
    "\n",
    "class EnhancedPreprocessingRecommender:\n",
    "    \"\"\"\n",
    "    고도화된 전처리 추천 시스템\n",
    "    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "    ▸ 다양한 데이터 타입 지원\n",
    "    ▸ 컨텍스트 기반 추천\n",
    "    ▸ 적응형 임계값\n",
    "    ▸ 우선순위 기반 추천\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.profiles: Dict[str, DataProfiler] = {}\n",
    "        self.context: DataContext = DataContext.GENERAL\n",
    "        self.thresholds = self._get_adaptive_thresholds()\n",
    "    \n",
    "    def _get_adaptive_thresholds(self) -> Dict[str, float]:\n",
    "        \"\"\"적응형 임계값 설정\"\"\"\n",
    "        return {\n",
    "            'missing_low': 0.05,\n",
    "            'missing_medium': 0.15,\n",
    "            'missing_high': 0.40,\n",
    "            'skewness_moderate': 1.0,\n",
    "            'skewness_high': 2.0,\n",
    "            'kurtosis_high': 7.0,\n",
    "            'cv_low': 0.1,\n",
    "            'cv_high': 1.0,\n",
    "            'correlation_high': 0.8,\n",
    "            'correlation_very_high': 0.95,\n",
    "            'unique_ratio_low': 0.01,\n",
    "            'unique_ratio_high': 0.8,\n",
    "            'outlier_ratio_threshold': 0.05\n",
    "        }\n",
    "    \n",
    "    def profile_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"데이터 프로파일링\"\"\"\n",
    "        self.profiles.clear()\n",
    "        \n",
    "        # 전체 데이터 컨텍스트 분석\n",
    "        self.context = self._analyze_data_context(df)\n",
    "        \n",
    "        # 컨텍스트에 따른 임계값 조정\n",
    "        self._adjust_thresholds_by_context()\n",
    "        \n",
    "        for col in df.columns:\n",
    "            profile = self._profile_column(df, col)\n",
    "            self.profiles[col] = profile\n",
    "    \n",
    "    def _analyze_data_context(self, df: pd.DataFrame) -> DataContext:\n",
    "        \"\"\"데이터 컨텍스트 분석\"\"\"\n",
    "        n_rows, n_cols = df.shape\n",
    "        \n",
    "        # 시계열 데이터 검사\n",
    "        datetime_cols = df.select_dtypes(include=['datetime64']).columns\n",
    "        if len(datetime_cols) > 0 or any('date' in col.lower() or 'time' in col.lower() for col in df.columns):\n",
    "            return DataContext.TIMESERIES\n",
    "        \n",
    "        # 고차원 데이터 검사\n",
    "        if n_cols > 100:\n",
    "            return DataContext.HIGH_DIMENSIONAL\n",
    "        \n",
    "        # 희소 데이터 검사\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            zero_ratio = (df[numeric_cols] == 0).sum().sum() / (n_rows * len(numeric_cols))\n",
    "            if zero_ratio > 0.8:\n",
    "                return DataContext.SPARSE\n",
    "        \n",
    "        # 불균형 데이터 검사 (타겟 변수가 있는 경우)\n",
    "        # 여기서는 간단히 범주형 변수의 불균형 정도로 판단\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in categorical_cols:\n",
    "            if df[col].nunique() < 10:\n",
    "                value_counts = df[col].value_counts()\n",
    "                if len(value_counts) > 1:\n",
    "                    imbalance_ratio = value_counts.iloc[0] / value_counts.iloc[1]\n",
    "                    if imbalance_ratio > 10:\n",
    "                        return DataContext.IMBALANCED\n",
    "        \n",
    "        return DataContext.GENERAL\n",
    "    \n",
    "    def _adjust_thresholds_by_context(self) -> None:\n",
    "        \"\"\"컨텍스트에 따른 임계값 조정\"\"\"\n",
    "        if self.context == DataContext.HIGH_DIMENSIONAL:\n",
    "            self.thresholds['missing_high'] = 0.20  # 고차원에서는 더 엄격\n",
    "            self.thresholds['correlation_high'] = 0.7\n",
    "        elif self.context == DataContext.IMBALANCED:\n",
    "            self.thresholds['missing_medium'] = 0.10  # 불균형 데이터에서는 더 보수적\n",
    "        elif self.context == DataContext.SPARSE:\n",
    "            self.thresholds['missing_high'] = 0.60  # 희소 데이터에서는 더 관대\n",
    "    \n",
    "    def _profile_column(self, df: pd.DataFrame, col: str) -> DataProfiler:\n",
    "        \"\"\"개별 컬럼 프로파일링\"\"\"\n",
    "        series = df[col]\n",
    "        missing_ratio = series.isnull().mean()\n",
    "        unique_count = series.nunique()\n",
    "        unique_ratio = unique_count / len(series)\n",
    "        \n",
    "        # 데이터 타입 분류\n",
    "        data_type = self._classify_data_type(series)\n",
    "        \n",
    "        # 기본 프로파일 생성\n",
    "        profile = DataProfiler(\n",
    "            column_name=col,\n",
    "            data_type=data_type,\n",
    "            missing_ratio=missing_ratio,\n",
    "            unique_count=unique_count,\n",
    "            unique_ratio=unique_ratio,\n",
    "            is_constant=unique_count <= 1,\n",
    "            is_id_like=self._is_id_like(series)\n",
    "        )\n",
    "        \n",
    "        # 수치형 데이터 추가 분석\n",
    "        if data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:\n",
    "            numeric_data = pd.to_numeric(series, errors='coerce').dropna()\n",
    "            if len(numeric_data) > 0:\n",
    "                profile.mean = numeric_data.mean()\n",
    "                profile.std = numeric_data.std()\n",
    "                profile.cv = profile.std / profile.mean if profile.mean != 0 else np.inf\n",
    "                profile.skewness = numeric_data.skew()\n",
    "                profile.kurtosis = numeric_data.kurtosis()\n",
    "                profile.outlier_ratio = self._calculate_outlier_ratio(numeric_data)\n",
    "        \n",
    "        # 텍스트 데이터 추가 분석\n",
    "        elif data_type == DataType.TEXT:\n",
    "            text_data = series.dropna().astype(str)\n",
    "            if len(text_data) > 0:\n",
    "                lengths = text_data.str.len()\n",
    "                profile.text_length_stats = {\n",
    "                    'mean_length': lengths.mean(),\n",
    "                    'max_length': lengths.max(),\n",
    "                    'min_length': lengths.min(),\n",
    "                    'std_length': lengths.std()\n",
    "                }\n",
    "        \n",
    "        # 날짜/시간 데이터 추가 분석\n",
    "        elif data_type == DataType.DATETIME:\n",
    "            datetime_data = pd.to_datetime(series, errors='coerce').dropna()\n",
    "            if len(datetime_data) > 0:\n",
    "                profile.datetime_range = (datetime_data.min(), datetime_data.max())\n",
    "        \n",
    "        return profile\n",
    "    \n",
    "    def _classify_data_type(self, series: pd.Series) -> DataType:\n",
    "        \"\"\"데이터 타입 분류\"\"\"\n",
    "        col_name = series.name.lower()\n",
    "        \n",
    "        # ID 컬럼 검사\n",
    "        if self._is_id_like(series):\n",
    "            return DataType.ID\n",
    "        \n",
    "        # 불린 타입 검사\n",
    "        if series.dtype == bool or set(series.dropna().unique()) <= {0, 1, True, False}:\n",
    "            return DataType.BOOLEAN\n",
    "        \n",
    "        # 날짜/시간 타입 검사\n",
    "        if series.dtype.name.startswith('datetime') or 'date' in col_name or 'time' in col_name:\n",
    "            return DataType.DATETIME\n",
    "        \n",
    "        # 수치형 타입 검사\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            # 이산형 vs 연속형 판단\n",
    "            unique_count = series.nunique()\n",
    "            if unique_count <= 20 and series.dtype in ['int64', 'int32']:\n",
    "                return DataType.NUMERIC_DISCRETE\n",
    "            else:\n",
    "                return DataType.NUMERIC_CONTINUOUS\n",
    "        \n",
    "        # 텍스트 길이 기반 분류\n",
    "        if series.dtype == 'object':\n",
    "            text_data = series.dropna().astype(str)\n",
    "            if len(text_data) > 0:\n",
    "                avg_length = text_data.str.len().mean()\n",
    "                if avg_length > 50:  # 평균 길이 50자 이상이면 텍스트\n",
    "                    return DataType.TEXT\n",
    "        \n",
    "        # 범주형 타입 검사\n",
    "        if series.dtype.name == 'category' or series.dtype == 'object':\n",
    "            unique_count = series.nunique()\n",
    "            if unique_count <= 50:\n",
    "                # 순서형 vs 명목형 판단 (간단한 휴리스틱)\n",
    "                unique_vals = series.dropna().astype(str).unique()\n",
    "                if any(val in ['low', 'medium', 'high', 'small', 'large'] for val in unique_vals):\n",
    "                    return DataType.CATEGORICAL_ORDINAL\n",
    "                return DataType.CATEGORICAL_NOMINAL\n",
    "        \n",
    "        return DataType.CATEGORICAL_NOMINAL\n",
    "    \n",
    "    def _is_id_like(self, series: pd.Series) -> bool:\n",
    "        \"\"\"ID 컬럼 여부 판단\"\"\"\n",
    "        col_name = series.name.lower()\n",
    "        \n",
    "        # 컬럼명 기반 판단\n",
    "        if any(keyword in col_name for keyword in ['id', 'key', 'index', 'idx']):\n",
    "            return True\n",
    "        \n",
    "        # 고유값 비율 기반 판단\n",
    "        if series.nunique() / len(series) > 0.95:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _calculate_outlier_ratio(self, data: pd.Series) -> float:\n",
    "        \"\"\"이상치 비율 계산 (IQR 방법)\"\"\"\n",
    "        Q1 = data.quantile(0.25)\n",
    "        Q3 = data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        return len(outliers) / len(data)\n",
    "    \n",
    "    def recommend(self, df: pd.DataFrame) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"전처리 추천\"\"\"\n",
    "        self.profile_data(df)\n",
    "        recommendations = {}\n",
    "        \n",
    "        for col, profile in self.profiles.items():\n",
    "            col_recs = self._generate_column_recommendations(df, profile)\n",
    "            recommendations[col] = {\n",
    "                'data_type': profile.data_type.value,\n",
    "                'recommendations': col_recs,\n",
    "                'priority': self._calculate_priority(profile),\n",
    "                'context': self.context.value\n",
    "            }\n",
    "        \n",
    "        # 전역 추천사항 추가\n",
    "        global_recs = self._generate_global_recommendations(df)\n",
    "        if global_recs:\n",
    "            recommendations['_global'] = {\n",
    "                'data_type': 'global',\n",
    "                'recommendations': global_recs,\n",
    "                'priority': 'high',\n",
    "                'context': self.context.value\n",
    "            }\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _generate_column_recommendations(self, df: pd.DataFrame, profile: DataProfiler) -> List[str]:\n",
    "        \"\"\"컬럼별 추천 생성\"\"\"\n",
    "        recs = []\n",
    "        \n",
    "        # ID 컬럼 처리\n",
    "        if profile.data_type == DataType.ID:\n",
    "            recs.append('consider_dropping_id_column')\n",
    "            return recs\n",
    "        \n",
    "        # 상수 컬럼 처리\n",
    "        if profile.is_constant:\n",
    "            recs.append('drop_constant_column')\n",
    "            return recs\n",
    "        \n",
    "        # 결측치 처리\n",
    "        if profile.missing_ratio > self.thresholds['missing_high']:\n",
    "            recs.append('drop_column_excessive_missing')\n",
    "        elif profile.missing_ratio > self.thresholds['missing_medium']:\n",
    "            recs.append('advanced_imputation_knn_iterative')\n",
    "        elif profile.missing_ratio > self.thresholds['missing_low']:\n",
    "            if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:\n",
    "                recs.append('impute_median_mode')\n",
    "            else:\n",
    "                recs.append('impute_mode_frequent')\n",
    "        \n",
    "        # 수치형 데이터 처리\n",
    "        if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:\n",
    "            recs.extend(self._get_numeric_recommendations(profile))\n",
    "        \n",
    "        # 범주형 데이터 처리\n",
    "        elif profile.data_type in [DataType.CATEGORICAL_NOMINAL, DataType.CATEGORICAL_ORDINAL]:\n",
    "            recs.extend(self._get_categorical_recommendations(profile))\n",
    "        \n",
    "        # 텍스트 데이터 처리\n",
    "        elif profile.data_type == DataType.TEXT:\n",
    "            recs.extend(self._get_text_recommendations(profile))\n",
    "        \n",
    "        # 날짜/시간 데이터 처리\n",
    "        elif profile.data_type == DataType.DATETIME:\n",
    "            recs.extend(self._get_datetime_recommendations(profile))\n",
    "        \n",
    "        return recs\n",
    "    \n",
    "    def _get_numeric_recommendations(self, profile: DataProfiler) -> List[str]:\n",
    "        \"\"\"수치형 데이터 추천\"\"\"\n",
    "        recs = []\n",
    "        \n",
    "        # 분산 검사\n",
    "        if profile.cv and profile.cv < self.thresholds['cv_low']:\n",
    "            recs.append('consider_low_variance_removal')\n",
    "        \n",
    "        # 이상치 처리\n",
    "        if profile.outlier_ratio and profile.outlier_ratio > self.thresholds['outlier_ratio_threshold']:\n",
    "            recs.append('outlier_detection_treatment')\n",
    "        \n",
    "        # 분포 특성 기반 추천\n",
    "        if profile.skewness and profile.kurtosis:\n",
    "            if abs(profile.skewness) > self.thresholds['skewness_high'] or profile.kurtosis > self.thresholds['kurtosis_high']:\n",
    "                recs.append('distribution_transformation_log_boxcox')\n",
    "            elif abs(profile.skewness) > self.thresholds['skewness_moderate']:\n",
    "                recs.append('robust_scaling_iqr')\n",
    "        \n",
    "        # 스케일링 추천\n",
    "        if profile.cv and profile.cv > self.thresholds['cv_high']:\n",
    "            recs.append('standardization_zscore')\n",
    "        else:\n",
    "            recs.append('normalization_minmax')\n",
    "        \n",
    "        # 컨텍스트 기반 추천\n",
    "        if self.context == DataContext.HIGH_DIMENSIONAL:\n",
    "            recs.append('dimensionality_reduction_pca')\n",
    "        elif self.context == DataContext.SPARSE:\n",
    "            recs.append('sparse_feature_selection')\n",
    "        \n",
    "        return recs\n",
    "    \n",
    "    def _get_categorical_recommendations(self, profile: DataProfiler) -> List[str]:\n",
    "        \"\"\"범주형 데이터 추천\"\"\"\n",
    "        recs = []\n",
    "        \n",
    "        # 인코딩 추천\n",
    "        if profile.unique_count <= 10:\n",
    "            recs.append('one_hot_encoding')\n",
    "        elif profile.unique_count <= 50:\n",
    "            if profile.data_type == DataType.CATEGORICAL_ORDINAL:\n",
    "                recs.append('ordinal_encoding')\n",
    "            else:\n",
    "                recs.append('label_encoding')\n",
    "        elif profile.unique_count <= 1000:\n",
    "            recs.append('target_encoding')\n",
    "        else:\n",
    "            recs.append('hashing_encoding')\n",
    "        \n",
    "        # 고카디널리티 처리\n",
    "        if profile.unique_count > 100:\n",
    "            recs.append('high_cardinality_feature_selection')\n",
    "        \n",
    "        # 희소 범주 처리\n",
    "        if profile.unique_ratio < self.thresholds['unique_ratio_low']:\n",
    "            recs.append('rare_category_grouping')\n",
    "        \n",
    "        return recs\n",
    "    \n",
    "    def _get_text_recommendations(self, profile: DataProfiler) -> List[str]:\n",
    "        \"\"\"텍스트 데이터 추천\"\"\"\n",
    "        recs = []\n",
    "        \n",
    "        if profile.text_length_stats:\n",
    "            avg_length = profile.text_length_stats['mean_length']\n",
    "            \n",
    "            if avg_length < 20:\n",
    "                recs.extend(['text_preprocessing_basic', 'bag_of_words'])\n",
    "            elif avg_length < 100:\n",
    "                recs.extend(['text_preprocessing_advanced', 'tfidf_vectorization'])\n",
    "            else:\n",
    "                recs.extend(['text_preprocessing_advanced', 'text_embeddings', 'topic_modeling'])\n",
    "        \n",
    "        recs.append('text_feature_extraction')\n",
    "        return recs\n",
    "    \n",
    "    def _get_datetime_recommendations(self, profile: DataProfiler) -> List[str]:\n",
    "        \"\"\"날짜/시간 데이터 추천\"\"\"\n",
    "        recs = []\n",
    "        \n",
    "        recs.extend([\n",
    "            'datetime_feature_extraction',\n",
    "            'time_based_features',\n",
    "            'cyclical_encoding'\n",
    "        ])\n",
    "        \n",
    "        if self.context == DataContext.TIMESERIES:\n",
    "            recs.extend([\n",
    "                'time_series_decomposition',\n",
    "                'lag_features',\n",
    "                'rolling_statistics'\n",
    "            ])\n",
    "        \n",
    "        return recs\n",
    "    \n",
    "    def _calculate_priority(self, profile: DataProfiler) -> str:\n",
    "        \"\"\"우선순위 계산\"\"\"\n",
    "        if profile.is_constant or profile.missing_ratio > self.thresholds['missing_high']:\n",
    "            return 'high'\n",
    "        elif profile.missing_ratio > self.thresholds['missing_medium']:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "    \n",
    "    def _generate_global_recommendations(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"전역 추천사항\"\"\"\n",
    "        recs = []\n",
    "        \n",
    "        # 다중공선성 검사\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 1:\n",
    "            corr_matrix = df[numeric_cols].corr().abs()\n",
    "            high_corr_pairs = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i+1, len(corr_matrix.columns)):\n",
    "                    if corr_matrix.iloc[i, j] > self.thresholds['correlation_very_high']:\n",
    "                        high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))\n",
    "            \n",
    "            if high_corr_pairs:\n",
    "                recs.append('multicollinearity_detection_vif')\n",
    "        \n",
    "        # 컨텍스트 기반 전역 추천\n",
    "        if self.context == DataContext.IMBALANCED:\n",
    "            recs.extend(['class_balancing_smote', 'stratified_sampling'])\n",
    "        elif self.context == DataContext.HIGH_DIMENSIONAL:\n",
    "            recs.extend(['feature_selection_univariate', 'regularization_techniques'])\n",
    "        elif self.context == DataContext.SPARSE:\n",
    "            recs.extend(['sparse_matrix_optimization', 'feature_hashing'])\n",
    "        \n",
    "        return recs\n",
    "\n",
    "class EnhancedVisualizationRecommender:\n",
    "    \"\"\"\n",
    "    고도화된 시각화 추천 시스템\n",
    "    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "    ▸ 분석 목적 기반 추천\n",
    "    ▸ 다변량 시각화 지원\n",
    "    ▸ 상호작용 시각화\n",
    "    ▸ 데이터 품질 시각화\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, analysis_purpose: str = 'exploratory'):\n",
    "        self.analysis_purpose = analysis_purpose  # 'exploratory', 'confirmatory', 'presentation'\n",
    "        self.data_profiles: Dict[str, DataProfiler] = {}\n",
    "        self.context: DataContext = DataContext.GENERAL\n",
    "    \n",
    "    def recommend(self, df: pd.DataFrame, data_profiles: Dict[str, DataProfiler] = None, \n",
    "                 context: DataContext = DataContext.GENERAL) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"시각화 추천\"\"\"\n",
    "        \n",
    "        if data_profiles:\n",
    "            self.data_profiles = data_profiles\n",
    "        else:\n",
    "            # 간단한 프로파일링\n",
    "            preprocessor = EnhancedPreprocessingRecommender()\n",
    "            preprocessor.profile_data(df)\n",
    "            self.data_profiles = preprocessor.profiles\n",
    "        \n",
    "        self.context = context\n",
    "        \n",
    "        recommendations = {}\n",
    "        \n",
    "        # 단변량 시각화\n",
    "        for col, profile in self.data_profiles.items():\n",
    "            if col.startswith('_'):\n",
    "                continue\n",
    "            \n",
    "            col_viz = self._get_univariate_visualizations(profile)\n",
    "            recommendations[col] = {\n",
    "                'type': 'univariate',\n",
    "                'visualizations': col_viz,\n",
    "                'priority': self._calculate_viz_priority(profile)\n",
    "            }\n",
    "        \n",
    "        # 이변량 시각화\n",
    "        bivariate_viz = self._get_bivariate_visualizations(df)\n",
    "        if bivariate_viz:\n",
    "            recommendations['_bivariate'] = {\n",
    "                'type': 'bivariate',\n",
    "                'visualizations': bivariate_viz,\n",
    "                'priority': 'medium'\n",
    "            }\n",
    "        \n",
    "        # 다변량 시각화\n",
    "        multivariate_viz = self._get_multivariate_visualizations(df)\n",
    "        if multivariate_viz:\n",
    "            recommendations['_multivariate'] = {\n",
    "                'type': 'multivariate',\n",
    "                'visualizations': multivariate_viz,\n",
    "                'priority': 'high'\n",
    "            }\n",
    "        \n",
    "        # 데이터 품질 시각화\n",
    "        quality_viz = self._get_data_quality_visualizations(df)\n",
    "        if quality_viz:\n",
    "            recommendations['_data_quality'] = {\n",
    "                'type': 'data_quality',\n",
    "                'visualizations': quality_viz,\n",
    "                'priority': 'high'\n",
    "            }\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def _get_univariate_visualizations(self, profile: DataProfiler) -> List[str]:\n",
    "        \"\"\"단변량 시각화 추천\"\"\"\n",
    "        viz_list = []\n",
    "        \n",
    "        if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:\n",
    "            viz_list.extend(['histogram', 'box_plot', 'violin_plot'])\n",
    "            \n",
    "            if profile.skewness and abs(profile.skewness) > 1:\n",
    "                viz_list.append('qq_plot')\n",
    "            \n",
    "            if profile.outlier_ratio and profile.outlier_ratio > 0.05:\n",
    "                viz_list.append('outlier_detection_plot')\n",
    "        \n",
    "        elif profile.data_type in [DataType.CATEGORICAL_NOMINAL, DataType.CATEGORICAL_ORDINAL]:\n",
    "            viz_list.extend(['bar_chart', 'horizontal_bar_chart'])\n",
    "            \n",
    "            if profile.unique_count <= 10:\n",
    "                viz_list.append('pie_chart')\n",
    "            \n",
    "            if profile.unique_count > 20:\n",
    "                viz_list.append('pareto_chart')\n",
    "        \n",
    "        elif profile.data_type == DataType.TEXT:\n",
    "            viz_list.extend(['word_cloud', 'text_length_distribution', 'n_gram_frequency'])\n",
    "        \n",
    "        elif profile.data_type == DataType.DATETIME:\n",
    "            viz_list.extend(['time_series_plot', 'temporal_distribution'])\n",
    "            \n",
    "            if self.context == DataContext.TIMESERIES:\n",
    "                viz_list.extend(['seasonal_decomposition', 'autocorrelation_plot'])\n",
    "        \n",
    "        return viz_list\n",
    "    \n",
    "    def _get_bivariate_visualizations(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"이변량 시각화 추천\"\"\"\n",
    "        viz_list = []\n",
    "        \n",
    "        numeric_cols = [col for col, profile in self.data_profiles.items() \n",
    "                       if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]]\n",
    "        \n",
    "        categorical_cols = [col for col, profile in self.data_profiles.items() \n",
    "                          if profile.data_type in [DataType.CATEGORICAL_NOMINAL, DataType.CATEGORICAL_ORDINAL]]\n",
    "        \n",
    "        # 수치형 vs 수치형\n",
    "        if len(numeric_cols) >= 2:\n",
    "            for i in range(len(numeric_cols)):\n",
    "                for j in range(i+1, len(numeric_cols)):\n",
    "                    col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "                    \n",
    "                    # 상관관계 계산\n",
    "                    corr_val = df[col1].corr(df[col2])\n",
    "                    \n",
    "                    viz_recommendations = ['scatter_plot']\n",
    "                    \n",
    "                    if abs(corr_val) > 0.7:\n",
    "                        viz_recommendations.extend(['regression_line', 'correlation_heatmap'])\n",
    "                    \n",
    "                    if abs(corr_val) > 0.3:\n",
    "                        viz_recommendations.append('joint_plot')\n",
    "                    \n",
    "                    viz_list.append({\n",
    "                        'variables': [col1, col2],\n",
    "                        'relationship': 'numeric_numeric',\n",
    "                        'correlation': corr_val,\n",
    "                        'visualizations': viz_recommendations\n",
    "                    })\n",
    "        \n",
    "        # 범주형 vs 수치형\n",
    "        for cat_col in categorical_cols:\n",
    "            for num_col in numeric_cols:\n",
    "                viz_recommendations = ['grouped_box_plot', 'violin_plot_grouped']\n",
    "                \n",
    "                if self.data_profiles[cat_col].unique_count <= 10:\n",
    "                    viz_recommendations.append('strip_plot')\n",
    "                \n",
    "                viz_list.append({\n",
    "                    'variables': [cat_col, num_col],\n",
    "                    'relationship': 'categorical_numeric',\n",
    "                    'visualizations': viz_recommendations\n",
    "                })\n",
    "        \n",
    "        # 범주형 vs 범주형\n",
    "        if len(categorical_cols) >= 2:\n",
    "            for i in range(len(categorical_cols)):\n",
    "                for j in range(i+1, len(categorical_cols)):\n",
    "                    col1, col2 = categorical_cols[i], categorical_cols[j]\n",
    "                    \n",
    "                    viz_recommendations = ['contingency_table_heatmap', 'grouped_bar_chart']\n",
    "                    \n",
    "                    if (self.data_profiles[col1].unique_count <= 10 and \n",
    "                        self.data_profiles[col2].unique_count <= 10):\n",
    "                        viz_recommendations.append('mosaic_plot')\n",
    "                    \n",
    "                    viz_list.append({\n",
    "                        'variables': [col1, col2],\n",
    "                        'relationship': 'categorical_categorical',\n",
    "                        'visualizations': viz_recommendations\n",
    "                    })\n",
    "        \n",
    "        return viz_list[:10]  # 최대 10개 추천\n",
    "    \n",
    "    def _get_multivariate_visualizations(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"다변량 시각화 추천\"\"\"\n",
    "        viz_list = []\n",
    "        \n",
    "        numeric_cols = [col for col, profile in self.data_profiles.items() \n",
    "                       if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]]\n",
    "        \n",
    "        if len(numeric_cols) >= 3:\n",
    "            viz_list.extend(['correlation_matrix', 'pair_plot', 'parallel_coordinates'])\n",
    "            \n",
    "            if len(numeric_cols) >= 4:\n",
    "                viz_list.append('3d_scatter_plot')\n",
    "            \n",
    "            if len(numeric_cols) >= 5:\n",
    "                viz_list.extend(['pca_biplot', 'radar_chart'])\n",
    "        \n",
    "        if self.context == DataContext.HIGH_DIMENSIONAL:\n",
    "            viz_list.extend(['dimensionality_reduction_plot', 'feature_importance_plot'])\n",
    "        \n",
    "        return viz_list\n",
    "    \n",
    "    def _get_data_quality_visualizations(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"데이터 품질 시각화 추천\"\"\"\n",
    "        viz_list = []\n",
    "        \n",
    "        # 결측값 시각화\n",
    "        total_missing = df.isnull().sum().sum()\n",
    "        if total_missing > 0:\n",
    "            viz_list.extend(['missing_value_matrix', 'missing_value_heatmap'])\n",
    "        \n",
    "        # 중복값 시각화\n",
    "        duplicate_count = df.duplicated().sum()\n",
    "        if duplicate_count > 0:\n",
    "            viz_list.append('duplicate_detection_plot')\n",
    "        \n",
    "        # 이상치 시각화\n",
    "        numeric_cols = [col for col, profile in self.data_profiles.items() \n",
    "                       if profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]]\n",
    "        \n",
    "        if numeric_cols:\n",
    "            viz_list.extend(['outlier_detection_plot', 'data_distribution_overview'])\n",
    "        \n",
    "        # 데이터 타입 검증 시각화\n",
    "        viz_list.append('data_type_validation_plot')\n",
    "        \n",
    "        return viz_list\n",
    "    \n",
    "    def _calculate_viz_priority(self, profile: DataProfiler) -> str:\n",
    "        \"\"\"시각화 우선순위 계산\"\"\"\n",
    "        if profile.missing_ratio > 0.3 or profile.is_constant:\n",
    "            return 'high'\n",
    "        elif profile.data_type in [DataType.NUMERIC_CONTINUOUS, DataType.NUMERIC_DISCRETE]:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "\n",
    "class EnhancedRecommendationEngine:\n",
    "    \"\"\"\n",
    "    고도화된 추천 엔진\n",
    "    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "    ▸ 통합 추천 시스템\n",
    "    ▸ 우선순위 기반 정렬\n",
    "    ▸ 실행 가능한 코드 생성\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, analysis_purpose: str = 'exploratory'):\n",
    "        self.preprocessor = EnhancedPreprocessingRecommender()\n",
    "        self.visualizer = EnhancedVisualizationRecommender(analysis_purpose)\n",
    "        self.analysis_purpose = analysis_purpose\n",
    "    \n",
    "    def run(self, df: pd.DataFrame, target_column: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"통합 추천 실행\"\"\"\n",
    "        \n",
    "        # 데이터 검증\n",
    "        if df.empty:\n",
    "            raise ValueError(\"입력 데이터프레임이 비어있습니다.\")\n",
    "        \n",
    "        # 전처리 추천\n",
    "        logger.info(\"전처리 추천 시작...\")\n",
    "        preprocessing_recs = self.preprocessor.recommend(df)\n",
    "        \n",
    "        # 시각화 추천\n",
    "        logger.info(\"시각화 추천 시작...\")\n",
    "        visualization_recs = self.visualizer.recommend(\n",
    "            df, \n",
    "            self.preprocessor.profiles, \n",
    "            self.preprocessor.context\n",
    "        )\n",
    "        \n",
    "        # 결과 통합\n",
    "        result = {\n",
    "            'data_info': {\n",
    "                'shape': df.shape,\n",
    "                'columns': list(df.columns),\n",
    "                'dtypes': df.dtypes.to_dict(),\n",
    "                'missing_values': df.isnull().sum().to_dict(),\n",
    "                'context': self.preprocessor.context.value\n",
    "            },\n",
    "            'preprocessing': preprocessing_recs,\n",
    "            'visualization': visualization_recs,\n",
    "            'summary': self._generate_summary(preprocessing_recs, visualization_recs),\n",
    "            'code_templates': self._generate_code_templates(preprocessing_recs, visualization_recs)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_summary(self, preprocessing_recs: Dict, visualization_recs: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"추천 요약 생성\"\"\"\n",
    "        \n",
    "        # 우선순위별 전처리 추천 집계\n",
    "        priority_counts = {'high': 0, 'medium': 0, 'low': 0}\n",
    "        for col, rec in preprocessing_recs.items():\n",
    "            if col.startswith('_'):\n",
    "                continue\n",
    "            priority = rec.get('priority', 'low')\n",
    "            priority_counts[priority] += 1\n",
    "        \n",
    "        # 주요 추천사항 추출\n",
    "        high_priority_recs = []\n",
    "        for col, rec in preprocessing_recs.items():\n",
    "            if rec.get('priority') == 'high':\n",
    "                high_priority_recs.append(f\"{col}: {rec['recommendations'][:2]}\")\n",
    "        \n",
    "        return {\n",
    "            'total_columns': len([col for col in preprocessing_recs.keys() if not col.startswith('_')]),\n",
    "            'priority_distribution': priority_counts,\n",
    "            'high_priority_recommendations': high_priority_recs[:5],\n",
    "            'recommended_visualizations': len(visualization_recs),\n",
    "            'analysis_context': self.preprocessor.context.value\n",
    "        }\n",
    "    \n",
    "    def _generate_code_templates(self, preprocessing_recs: Dict, visualization_recs: Dict) -> Dict[str, str]:\n",
    "        \"\"\"실행 가능한 코드 템플릿 생성\"\"\"\n",
    "        \n",
    "        templates = {}\n",
    "        \n",
    "        # 전처리 코드 템플릿\n",
    "        preprocessing_code = self._generate_preprocessing_code(preprocessing_recs)\n",
    "        templates['preprocessing'] = preprocessing_code\n",
    "        \n",
    "        # 시각화 코드 템플릿\n",
    "        visualization_code = self._generate_visualization_code(visualization_recs)\n",
    "        templates['visualization'] = visualization_code\n",
    "        \n",
    "        return templates\n",
    "    \n",
    "    def _generate_preprocessing_code(self, recommendations: Dict) -> str:\n",
    "        \"\"\"전처리 코드 생성\"\"\"\n",
    "        code_lines = [\n",
    "            \"import pandas as pd\",\n",
    "            \"import numpy as np\",\n",
    "            \"from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\",\n",
    "            \"from sklearn.impute import SimpleImputer, KNNImputer\",\n",
    "            \"\",\n",
    "            \"# 데이터 전처리 파이프라인\",\n",
    "            \"def preprocess_data(df):\",\n",
    "            \"    df_processed = df.copy()\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        for col, rec in recommendations.items():\n",
    "            if col.startswith('_'):\n",
    "                continue\n",
    "            \n",
    "            for recommendation in rec['recommendations'][:3]:  # 상위 3개 추천만\n",
    "                if 'drop' in recommendation:\n",
    "                    code_lines.append(f\"    # {col}: {recommendation}\")\n",
    "                    code_lines.append(f\"    df_processed = df_processed.drop('{col}', axis=1)\")\n",
    "                elif 'impute' in recommendation:\n",
    "                    code_lines.append(f\"    # {col}: {recommendation}\")\n",
    "                    code_lines.append(f\"    imputer = SimpleImputer(strategy='median')\")\n",
    "                    code_lines.append(f\"    df_processed['{col}'] = imputer.fit_transform(df_processed[['{col}']])\")\n",
    "                elif 'standardization' in recommendation:\n",
    "                    code_lines.append(f\"    # {col}: {recommendation}\")\n",
    "                    code_lines.append(f\"    scaler = StandardScaler()\")\n",
    "                    code_lines.append(f\"    df_processed['{col}'] = scaler.fit_transform(df_processed[['{col}']])\")\n",
    "        \n",
    "        code_lines.extend([\n",
    "            \"\",\n",
    "            \"    return df_processed\",\n",
    "            \"\",\n",
    "            \"# 사용 예시:\",\n",
    "            \"# processed_df = preprocess_data(df)\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(code_lines)\n",
    "    \n",
    "    def _generate_visualization_code(self, recommendations: Dict) -> str:\n",
    "        \"\"\"시각화 코드 생성\"\"\"\n",
    "        code_lines = [\n",
    "            \"import matplotlib.pyplot as plt\",\n",
    "            \"import seaborn as sns\",\n",
    "            \"import plotly.express as px\",\n",
    "            \"\",\n",
    "            \"# 데이터 시각화 함수들\",\n",
    "            \"def create_visualizations(df):\",\n",
    "            \"    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))\",\n",
    "            \"    axes = axes.flatten()\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        plot_idx = 0\n",
    "        for col, rec in recommendations.items():\n",
    "            if col.startswith('_') or plot_idx >= 4:\n",
    "                continue\n",
    "            \n",
    "            viz_type = rec['visualizations'][0] if rec['visualizations'] else 'histogram'\n",
    "            \n",
    "            if viz_type == 'histogram':\n",
    "                code_lines.append(f\"    # {col} 히스토그램\")\n",
    "                code_lines.append(f\"    axes[{plot_idx}].hist(df['{col}'].dropna(), bins=30)\")\n",
    "                code_lines.append(f\"    axes[{plot_idx}].set_title('{col} Distribution')\")\n",
    "            elif viz_type == 'bar_chart':\n",
    "                code_lines.append(f\"    # {col} 막대 차트\")\n",
    "                code_lines.append(f\"    df['{col}'].value_counts().plot(kind='bar', ax=axes[{plot_idx}])\")\n",
    "                code_lines.append(f\"    axes[{plot_idx}].set_title('{col} Value Counts')\")\n",
    "            \n",
    "            plot_idx += 1\n",
    "        \n",
    "        code_lines.extend([\n",
    "            \"\",\n",
    "            \"    plt.tight_layout()\",\n",
    "            \"    plt.show()\",\n",
    "            \"\",\n",
    "            \"# 사용 예시:\",\n",
    "            \"# create_visualizations(df)\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\n\".join(code_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f829f98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:전처리 추천 시작...\n",
      "INFO:__main__:시각화 추천 시작...\n"
     ]
    }
   ],
   "source": [
    "engine = EnhancedRecommendationEngine(analysis_purpose='exploratory')\n",
    "recommendations = engine.run(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "600e4d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📊 고도화된 데이터 분석 추천 시스템\n",
      "============================================================\n",
      "데이터 형태: (1567, 6)\n",
      "분석 컨텍스트: general\n",
      "\n",
      "전처리 추천:\n",
      "  STD_DT: ['consider_dropping_id_column']\n",
      "  MIXA_PASTEUR_STATE: []\n",
      "  MIXB_PASTEUR_STATE: []\n",
      "  MIXA_PASTEUR_TEMP: ['outlier_detection_treatment', 'distribution_transformation_log_boxcox']\n",
      "  MIXB_PASTEUR_TEMP: ['outlier_detection_treatment', 'distribution_transformation_log_boxcox']\n",
      "  INSP: ['one_hot_encoding', 'rare_category_grouping']\n",
      "\n",
      "코드 템플릿이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "print(\"=\" * 60)\n",
    "print(\"📊 고도화된 데이터 분석 추천 시스템\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"데이터 형태: {recommendations['data_info']['shape']}\")\n",
    "print(f\"분석 컨텍스트: {recommendations['data_info']['context']}\")\n",
    "print(\"\\n전처리 추천:\")\n",
    "for col, rec in recommendations['preprocessing'].items():\n",
    "    if not col.startswith('_'):\n",
    "        print(f\"  {col}: {rec['recommendations'][:2]}\")\n",
    "\n",
    "print(f\"\\n코드 템플릿이 생성되었습니다.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534a97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# YAML 기반 시스템 복제 (테스트용)\n",
    "@dataclass\n",
    "class RuleSet:\n",
    "    numeric: Dict[str, Any] = field(default_factory=dict)\n",
    "    categorical: Dict[str, Any] = field(default_factory=dict)\n",
    "    datetime: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str = \"rules.yaml\") -> \"RuleSet\":\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            raw = yaml.safe_load(f)\n",
    "        return cls(**raw)\n",
    "\n",
    "class PreprocessingRecommender:\n",
    "    def __init__(self, rules: RuleSet):\n",
    "        self.rules = rules\n",
    "\n",
    "    def _apply_numeric_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:\n",
    "        rec: List[Tuple[str, str]] = []\n",
    "        r = self.rules.numeric\n",
    "\n",
    "        # ① Missing\n",
    "        miss = s.isna().mean()\n",
    "        for cond in r[\"missing_ratio\"]:\n",
    "            if miss > cond.get(\"gt\", -np.inf):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "\n",
    "        # ② Skew / Kurtosis\n",
    "        skew, kurt = s.skew(), s.kurtosis()\n",
    "        for cond in r[\"skew_kurt\"]:\n",
    "            if abs(skew) > cond.get(\"skew\", 0) or kurt > cond.get(\"kurt\", 1e9):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "\n",
    "        # ③ Coefficient of Variation\n",
    "        mean, std = s.mean(), s.std()\n",
    "        cv = std / mean if mean and not np.isnan(mean) and mean != 0 else np.inf\n",
    "        for cond in r[\"variance\"]:\n",
    "            if cv < cond.get(\"cv_lt\", -np.inf):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "        return rec\n",
    "\n",
    "    def _apply_categorical_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:\n",
    "        rec: List[Tuple[str, str]] = []\n",
    "        r = self.rules.categorical\n",
    "\n",
    "        miss = s.isna().mean()\n",
    "        unique = s.nunique(dropna=True)\n",
    "\n",
    "        for cond in r[\"missing_ratio\"]:\n",
    "            if miss > cond.get(\"gt\", -np.inf):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "\n",
    "        for cond in r[\"cardinality\"]:\n",
    "            if (\n",
    "                (\"lte\" in cond and unique <= cond[\"lte\"])\n",
    "                or (\"gt\" in cond and unique > cond[\"gt\"])\n",
    "            ):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "        return rec\n",
    "\n",
    "    def recommend(self, df: pd.DataFrame) -> Dict[str, List[Dict[str, str]]]:\n",
    "        out: Dict[str, List[Dict[str, str]]] = {}\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\", \"string\"]).columns\n",
    "\n",
    "        # Numeric\n",
    "        for c in numeric_cols:\n",
    "            rec = self._apply_numeric_rules(c, df[c].dropna())\n",
    "            out[c] = [{\"action\": a, \"why\": w} for a, w in rec]\n",
    "\n",
    "        # Categorical\n",
    "        for c in categorical_cols:\n",
    "            rec = self._apply_categorical_rules(c, df[c])\n",
    "            out[c] = [{\"action\": a, \"why\": w} for a, w in rec]\n",
    "\n",
    "        return out\n",
    "\n",
    "class RecommendationEngine:\n",
    "    def __init__(self, rule_path: str = \"rules.yaml\"):\n",
    "        self.rules = RuleSet.load(rule_path)\n",
    "        self.preproc = PreprocessingRecommender(self.rules)\n",
    "\n",
    "    def run(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"preprocessing\": self.preproc.recommend(df),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc5abb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 테스트 데이터 정보:\n",
      "   - 데이터 형태: (1567, 6)\n",
      "   - 수치형 컬럼: 4개\n",
      "   - 범주형 컬럼: 2개\n",
      "   - 결측값 있는 컬럼: 0개\n"
     ]
    }
   ],
   "source": [
    "print(f\"📊 테스트 데이터 정보:\")\n",
    "print(f\"   - 데이터 형태: {df1.shape}\")\n",
    "print(f\"   - 수치형 컬럼: {len(df1.select_dtypes(include='number').columns)}개\")\n",
    "print(f\"   - 범주형 컬럼: {len(df1.select_dtypes(include=['object']).columns)}개\")\n",
    "print(f\"   - 결측값 있는 컬럼: {df1.isnull().any().sum()}개\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6c54f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "import logging\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ResearchRuleSet:\n",
    "    \"\"\"연구 기반 규칙 집합\"\"\"\n",
    "    numeric: Dict[str, Any] = field(default_factory=dict)\n",
    "    categorical: Dict[str, Any] = field(default_factory=dict)\n",
    "    datetime: Dict[str, Any] = field(default_factory=dict)\n",
    "    data_quality: Dict[str, Any] = field(default_factory=dict)\n",
    "    feature_selection: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str = \"research_based_rules.yaml\") -> \"ResearchRuleSet\":\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            raw = yaml.safe_load(f)\n",
    "        return cls(**raw)\n",
    "\n",
    "class ResearchBasedPreprocessingRecommender:\n",
    "    \"\"\"연구 기반 전처리 추천 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, rules: ResearchRuleSet):\n",
    "        self.rules = rules\n",
    "    \n",
    "    def _is_datetime_column(self, s: pd.Series, col_name: str) -> bool:\n",
    "        \"\"\"날짜/시간 컬럼인지 판단하는 함수\"\"\"\n",
    "        \n",
    "        # 1. 컬럼명 기반 판단\n",
    "        col_lower = col_name.lower()\n",
    "        date_keywords = ['date', 'time', 'dt', 'datetime', 'timestamp', 'created', 'updated']\n",
    "        if any(keyword in col_lower for keyword in date_keywords):\n",
    "            return True\n",
    "        \n",
    "        # 2. 데이터 패턴 기반 판단\n",
    "        if s.dtype == 'object':\n",
    "            # 샘플 데이터로 날짜 패턴 테스트\n",
    "            sample_data = s.dropna().head(100)\n",
    "            if len(sample_data) == 0:\n",
    "                return False\n",
    "            \n",
    "            # 다양한 날짜 패턴 정의\n",
    "            date_patterns = [\n",
    "                r'\\d{4}-\\d{2}-\\d{2}',  # YYYY-MM-DD\n",
    "                r'\\d{2}/\\d{2}/\\d{4}',  # MM/DD/YYYY\n",
    "                r'\\d{2}-\\d{2}-\\d{4}',  # MM-DD-YYYY\n",
    "                r'\\d{4}/\\d{2}/\\d{2}',  # YYYY/MM/DD\n",
    "                r'\\d{2}/\\d{2}/\\d{2}',  # MM/DD/YY\n",
    "                r'\\d{8}',              # YYYYMMDD\n",
    "                r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}',  # YYYY-MM-DD HH:MM:SS\n",
    "                r'\\d{2}:\\d{2}:\\d{2}',  # HH:MM:SS\n",
    "            ]\n",
    "            \n",
    "            # 패턴 매칭 테스트\n",
    "            pattern_matches = 0\n",
    "            total_tested = 0\n",
    "            \n",
    "            for value in sample_data:\n",
    "                value_str = str(value).strip()\n",
    "                if value_str and value_str != 'nan':\n",
    "                    total_tested += 1\n",
    "                    for pattern in date_patterns:\n",
    "                        if re.match(pattern, value_str):\n",
    "                            pattern_matches += 1\n",
    "                            break\n",
    "            \n",
    "            # 70% 이상이 날짜 패턴이면 날짜 컬럼으로 판단\n",
    "            if total_tested > 0 and pattern_matches / total_tested > 0.7:\n",
    "                return True\n",
    "            \n",
    "            # 3. 실제 날짜 변환 테스트\n",
    "            try:\n",
    "                pd.to_datetime(sample_data, errors='raise')\n",
    "                return True\n",
    "            except (ValueError, TypeError):\n",
    "                pass\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _extract_datetime_features(self, s: pd.Series) -> Dict[str, Any]:\n",
    "        \"\"\"날짜/시간 특성 추출\"\"\"\n",
    "        try:\n",
    "            # object 타입을 datetime으로 변환\n",
    "            datetime_series = pd.to_datetime(s, errors='coerce')\n",
    "            valid_dates = datetime_series.dropna()\n",
    "            \n",
    "            if len(valid_dates) == 0:\n",
    "                return {}\n",
    "            \n",
    "            # 날짜 범위 계산\n",
    "            date_range = valid_dates.max() - valid_dates.min()\n",
    "            \n",
    "            # 시간 단위별 고유값 개수\n",
    "            unique_years = valid_dates.dt.year.nunique()\n",
    "            unique_months = valid_dates.dt.month.nunique()\n",
    "            unique_days = valid_dates.dt.day.nunique()\n",
    "            unique_hours = valid_dates.dt.hour.nunique() if hasattr(valid_dates.dt, 'hour') else 0\n",
    "            \n",
    "            return {\n",
    "                'date_range_days': date_range.days,\n",
    "                'unique_years': unique_years,\n",
    "                'unique_months': unique_months,\n",
    "                'unique_days': unique_days,\n",
    "                'unique_hours': unique_hours,\n",
    "                'is_timeseries': date_range.days > 30,  # 30일 이상이면 시계열로 판단\n",
    "                'has_time_component': unique_hours > 1,  # 시간 정보가 있는지\n",
    "                'conversion_success_rate': len(valid_dates) / len(s)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"날짜 변환 실패: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _analyze_distribution(self, s: pd.Series) -> Dict[str, float]:\n",
    "        \"\"\"분포 특성 분석 - 연구 기반 메트릭\"\"\"\n",
    "        skew = s.skew()\n",
    "        kurt = s.kurtosis()\n",
    "        mean = s.mean()\n",
    "        std = s.std()\n",
    "        cv = std / mean if mean != 0 else np.inf\n",
    "        \n",
    "        # 이상치 비율 계산 (IQR 방법)\n",
    "        Q1, Q3 = s.quantile([0.25, 0.75])\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_mask = (s < Q1 - 1.5 * IQR) | (s > Q3 + 1.5 * IQR)\n",
    "        outlier_ratio = outlier_mask.mean()\n",
    "        \n",
    "        return {\n",
    "            'skew': skew,\n",
    "            'kurt': kurt,\n",
    "            'cv': cv,\n",
    "            'outlier_ratio': outlier_ratio,\n",
    "            'unique_count': s.nunique(),\n",
    "            'unique_ratio': s.nunique() / len(s)\n",
    "        }\n",
    "    \n",
    "    def _apply_datetime_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:\n",
    "        \"\"\"날짜/시간 데이터 규칙 적용\"\"\"\n",
    "        rec: List[Tuple[str, str]] = []\n",
    "        \n",
    "        # 날짜 특성 추출\n",
    "        datetime_features = self._extract_datetime_features(s)\n",
    "        \n",
    "        if not datetime_features:\n",
    "            return rec\n",
    "        \n",
    "        # 기본 날짜 특성 추출\n",
    "        rec.append((\"datetime_extraction\", \"extract_date_time_features\"))\n",
    "        \n",
    "        # 시계열 데이터인 경우\n",
    "        if datetime_features.get('is_timeseries', False):\n",
    "            rec.extend([\n",
    "                (\"cyclical_encoding\", \"temporal_cyclical_features\"),\n",
    "                (\"lag_features\", \"time_series_lag_features\"),\n",
    "                (\"rolling_statistics\", \"time_series_rolling_features\")\n",
    "            ])\n",
    "        \n",
    "        # 시간 정보가 있는 경우\n",
    "        if datetime_features.get('has_time_component', False):\n",
    "            rec.append((\"time_based_features\", \"extract_time_components\"))\n",
    "        \n",
    "        # 날짜 범위가 넓은 경우\n",
    "        if datetime_features.get('date_range_days', 0) > 365:\n",
    "            rec.append((\"seasonal_decomposition\", \"long_term_temporal_patterns\"))\n",
    "        \n",
    "        # 변환 성공률이 낮은 경우\n",
    "        if datetime_features.get('conversion_success_rate', 1.0) < 0.8:\n",
    "            rec.append((\"datetime_cleaning\", \"inconsistent_date_formats\"))\n",
    "        \n",
    "        return rec\n",
    "    \n",
    "    def _apply_numeric_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:\n",
    "        \"\"\"연구 기반 수치형 규칙 적용\"\"\"\n",
    "        rec: List[Tuple[str, str]] = []\n",
    "        r = self.rules.numeric\n",
    "        \n",
    "        # 기본 통계 계산\n",
    "        miss = s.isna().mean()\n",
    "        stats_dict = self._analyze_distribution(s)\n",
    "        \n",
    "        # 1. 결측치 처리 - 세분화된 임계값\n",
    "        for cond in r[\"missing_ratio\"]:\n",
    "            if miss > cond.get(\"gt\", -np.inf):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "        \n",
    "        # 2. 분포 분석 - 연구 기반 임계값\n",
    "        for cond in r[\"distribution_analysis\"]:\n",
    "            skew_threshold = cond.get(\"skew\", 0)\n",
    "            kurt_threshold = cond.get(\"kurt\", 1e9)\n",
    "            \n",
    "            if (abs(stats_dict['skew']) > skew_threshold or \n",
    "                stats_dict['kurt'] > kurt_threshold):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "        \n",
    "        # 3. 변동성 분석\n",
    "        for cond in r[\"variance_analysis\"]:\n",
    "            if stats_dict['cv'] < cond.get(\"cv_lt\", -np.inf):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "        \n",
    "        # 4. 이상치 탐지 - 다중 방법론\n",
    "        if \"outlier_detection\" in r:\n",
    "            for cond in r[\"outlier_detection\"]:\n",
    "                if \"dimensions_gt\" in cond:\n",
    "                    # 다변량 이상치 (고차원 데이터)\n",
    "                    if len(s) > cond[\"dimensions_gt\"]:\n",
    "                        rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                        break\n",
    "                else:\n",
    "                    # 단변량 이상치\n",
    "                    rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                    break\n",
    "        \n",
    "        # 5. 스케일링 전략 - 복합 조건\n",
    "        if \"scaling_strategy\" in r:\n",
    "            for cond in r[\"scaling_strategy\"]:\n",
    "                cv_condition = cond.get(\"cv_gt\", 0)\n",
    "                outlier_condition = cond.get(\"outlier_ratio_gt\", 0)\n",
    "                \n",
    "                if (stats_dict['cv'] > cv_condition and \n",
    "                    stats_dict['outlier_ratio'] > outlier_condition):\n",
    "                    rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                    break\n",
    "            else:\n",
    "                # 기본 스케일링\n",
    "                if r[\"scaling_strategy\"]:\n",
    "                    default_scaling = r[\"scaling_strategy\"][-1]\n",
    "                    rec.append((default_scaling[\"action\"], default_scaling[\"why\"]))\n",
    "        \n",
    "        # 6. 도메인 특화 규칙\n",
    "        if \"domain_specific\" in r:\n",
    "            for cond in r[\"domain_specific\"]:\n",
    "                if \"name_contains\" in cond:\n",
    "                    if cond[\"name_contains\"].lower() in col.lower():\n",
    "                        # 추가 조건 체크\n",
    "                        if \"skew_lt\" in cond and stats_dict['skew'] < cond[\"skew_lt\"]:\n",
    "                            rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                        elif \"unique_lt\" in cond and stats_dict['unique_count'] < cond[\"unique_lt\"]:\n",
    "                            rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                        elif \"unique_ratio_gt\" in cond and stats_dict['unique_ratio'] > cond[\"unique_ratio_gt\"]:\n",
    "                            rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                        else:\n",
    "                            rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "        \n",
    "        return rec\n",
    "    \n",
    "    def _apply_categorical_rules(self, col: str, s: pd.Series) -> List[Tuple[str, str]]:\n",
    "        \"\"\"연구 기반 범주형 규칙 적용\"\"\"\n",
    "        rec: List[Tuple[str, str]] = []\n",
    "        r = self.rules.categorical\n",
    "        \n",
    "        # 날짜 데이터인지 먼저 확인\n",
    "        if self._is_datetime_column(s, col):\n",
    "            return self._apply_datetime_rules(col, s)\n",
    "        \n",
    "        miss = s.isna().mean()\n",
    "        unique = s.nunique(dropna=True)\n",
    "        \n",
    "        # 1. 결측치 처리\n",
    "        for cond in r[\"missing_ratio\"]:\n",
    "            if miss > cond.get(\"gt\", -np.inf):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "        \n",
    "        # 2. 카디널리티 전략 - 연구 기반 임계값\n",
    "        for cond in r[\"cardinality_strategy\"]:\n",
    "            if (\"lte\" in cond and unique <= cond[\"lte\"]) or (\"gt\" in cond and unique > cond[\"gt\"]):\n",
    "                rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "                break\n",
    "        \n",
    "        # 3. 텍스트 처리 규칙\n",
    "        if \"text_processing\" in r:\n",
    "            for cond in r[\"text_processing\"]:\n",
    "                if \"name_contains\" in cond and cond[\"name_contains\"].lower() in col.lower():\n",
    "                    rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "        \n",
    "        # 4. 순서형 데이터 분석\n",
    "        if \"ordinal_analysis\" in r:\n",
    "            unique_values = set(s.dropna().astype(str).str.lower())\n",
    "            for cond in r[\"ordinal_analysis\"]:\n",
    "                if \"values_contain\" in cond:\n",
    "                    if any(val in unique_values for val in cond[\"values_contain\"]):\n",
    "                        rec.append((cond[\"action\"], cond[\"why\"]))\n",
    "        \n",
    "        return rec\n",
    "    \n",
    "    def _apply_data_quality_rules(self, df: pd.DataFrame) -> List[Tuple[str, str]]:\n",
    "        \"\"\"데이터 품질 규칙 적용\"\"\"\n",
    "        rec: List[Tuple[str, str]] = []\n",
    "        \n",
    "        if \"data_quality\" in self.rules.data_quality:\n",
    "            # 중복 데이터 검사\n",
    "            duplicate_ratio = df.duplicated().mean()\n",
    "            if duplicate_ratio > 0.1:\n",
    "                rec.append((\"remove_duplicates\", \"high_duplicate_ratio\"))\n",
    "            \n",
    "            # 일관성 검사 (간단한 예시)\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':\n",
    "                    # 문자열 길이 일관성 검사\n",
    "                    str_lengths = df[col].astype(str).str.len()\n",
    "                    if str_lengths.std() / str_lengths.mean() > 0.5:\n",
    "                        rec.append((\"data_cleaning\", \"inconsistent_data_patterns\"))\n",
    "        \n",
    "        return rec\n",
    "    \n",
    "    def _apply_feature_selection_rules(self, df: pd.DataFrame) -> List[Tuple[str, str]]:\n",
    "        \"\"\"특성 선택 규칙 적용\"\"\"\n",
    "        rec: List[Tuple[str, str]] = []\n",
    "        \n",
    "        if \"feature_selection\" in self.rules.feature_selection:\n",
    "            numeric_df = df.select_dtypes(include=[np.number])\n",
    "            \n",
    "            if len(numeric_df.columns) > 1:\n",
    "                # 다중공선성 검사\n",
    "                corr_matrix = numeric_df.corr().abs()\n",
    "                high_corr_pairs = []\n",
    "                for i in range(len(corr_matrix.columns)):\n",
    "                    for j in range(i+1, len(corr_matrix.columns)):\n",
    "                        if corr_matrix.iloc[i, j] > 0.95:\n",
    "                            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j]))\n",
    "                \n",
    "                if high_corr_pairs:\n",
    "                    rec.append((\"remove_collinear\", \"high_correlation_collinearity\"))\n",
    "        \n",
    "        return rec\n",
    "    \n",
    "    def recommend(self, df: pd.DataFrame) -> Dict[str, List[Dict[str, str]]]:\n",
    "        \"\"\"연구 기반 추천 시스템\"\"\"\n",
    "        out: Dict[str, List[Dict[str, str]]] = {}\n",
    "        \n",
    "        numeric_cols = df.select_dtypes(include=\"number\").columns\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\", \"string\"]).columns\n",
    "        \n",
    "        # 수치형 컬럼 처리\n",
    "        for c in numeric_cols:\n",
    "            rec = self._apply_numeric_rules(c, df[c].dropna())\n",
    "            out[c] = [{\"action\": a, \"why\": w} for a, w in rec]\n",
    "        \n",
    "        # 범주형 컬럼 처리 (날짜 데이터 포함)\n",
    "        for c in categorical_cols:\n",
    "            rec = self._apply_categorical_rules(c, df[c])\n",
    "            out[c] = [{\"action\": a, \"why\": w} for a, w in rec]\n",
    "        \n",
    "        # 데이터 품질 규칙\n",
    "        quality_rec = self._apply_data_quality_rules(df)\n",
    "        if quality_rec:\n",
    "            out[\"_data_quality\"] = [{\"action\": a, \"why\": w} for a, w in quality_rec]\n",
    "        \n",
    "        # 특성 선택 규칙\n",
    "        selection_rec = self._apply_feature_selection_rules(df)\n",
    "        if selection_rec:\n",
    "            out[\"_feature_selection\"] = [{\"action\": a, \"why\": w} for a, w in selection_rec]\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResearchBasedRecommendationEngine:\n",
    "    \"\"\"연구 기반 추천 엔진\"\"\"\n",
    "    \n",
    "    def __init__(self, rule_path: str = \"research_based_rules.yaml\"):\n",
    "        self.rules = ResearchRuleSet.load(rule_path)\n",
    "        self.preproc = ResearchBasedPreprocessingRecommender(self.rules)\n",
    "    \n",
    "    def run(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        return {\n",
    "            \"preprocessing\": self.preproc.recommend(df),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bccf9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_file = \"./research_based_rules.yaml\"\n",
    "engine = ResearchBasedRecommendationEngine(rules_file)\n",
    "recommendations = engine.run(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13fdd72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MIXA_PASTEUR_STATE': [{'action': 'boxcox_transform', 'why': 'high_skewness_with_kurtosis'}, {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'}, {'action': 'standard_scaler', 'why': 'moderate_variance'}, {'action': 'ordinal_encode', 'why': 'discrete_state_data'}], 'MIXB_PASTEUR_STATE': [{'action': 'boxcox_transform', 'why': 'high_skewness_with_kurtosis'}, {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'}, {'action': 'standard_scaler', 'why': 'moderate_variance'}, {'action': 'ordinal_encode', 'why': 'discrete_state_data'}], 'MIXA_PASTEUR_TEMP': [{'action': 'yeo_johnson_transform', 'why': 'extreme_skewness_kurtosis'}, {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'}, {'action': 'min_max_scaler', 'why': 'bounded_scaling'}, {'action': 'log_transform_shifted', 'why': 'temperature_data_negative_skew'}], 'MIXB_PASTEUR_TEMP': [{'action': 'yeo_johnson_transform', 'why': 'extreme_skewness_kurtosis'}, {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'}, {'action': 'min_max_scaler', 'why': 'bounded_scaling'}, {'action': 'log_transform_shifted', 'why': 'temperature_data_negative_skew'}], 'STD_DT': [{'action': 'datetime_extraction', 'why': 'extract_date_time_features'}, {'action': 'time_based_features', 'why': 'extract_time_components'}], 'INSP': [{'action': 'binary_encode', 'why': 'binary_categorical_variable'}]}\n"
     ]
    }
   ],
   "source": [
    "for i in recommendations.values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61fc6c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preprocessing': {'MIXA_PASTEUR_STATE': [{'action': 'boxcox_transform',\n",
       "    'why': 'high_skewness_with_kurtosis'},\n",
       "   {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'},\n",
       "   {'action': 'standard_scaler', 'why': 'moderate_variance'},\n",
       "   {'action': 'ordinal_encode', 'why': 'discrete_state_data'}],\n",
       "  'MIXB_PASTEUR_STATE': [{'action': 'boxcox_transform',\n",
       "    'why': 'high_skewness_with_kurtosis'},\n",
       "   {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'},\n",
       "   {'action': 'standard_scaler', 'why': 'moderate_variance'},\n",
       "   {'action': 'ordinal_encode', 'why': 'discrete_state_data'}],\n",
       "  'MIXA_PASTEUR_TEMP': [{'action': 'yeo_johnson_transform',\n",
       "    'why': 'extreme_skewness_kurtosis'},\n",
       "   {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'},\n",
       "   {'action': 'min_max_scaler', 'why': 'bounded_scaling'},\n",
       "   {'action': 'log_transform_shifted',\n",
       "    'why': 'temperature_data_negative_skew'}],\n",
       "  'MIXB_PASTEUR_TEMP': [{'action': 'yeo_johnson_transform',\n",
       "    'why': 'extreme_skewness_kurtosis'},\n",
       "   {'action': 'isolation_forest', 'why': 'high_dimensional_outliers'},\n",
       "   {'action': 'min_max_scaler', 'why': 'bounded_scaling'},\n",
       "   {'action': 'log_transform_shifted',\n",
       "    'why': 'temperature_data_negative_skew'}],\n",
       "  'STD_DT': [{'action': 'datetime_extraction',\n",
       "    'why': 'extract_date_time_features'},\n",
       "   {'action': 'time_based_features', 'why': 'extract_time_components'}],\n",
       "  'INSP': [{'action': 'binary_encode', 'why': 'binary_categorical_variable'}]}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd40bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".test_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
